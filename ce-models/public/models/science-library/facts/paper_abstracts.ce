-- --------------------------------------------------------------------------
-- (C) Copyright IBM Corporation 2016
-- All Rights Reserved
-- --------------------------------------------------------------------------

-- This CE has been generated from SL_generator v1.21.xlsm

there is a document named 'doc-1007' that
  has 'Establishing trust in vehicular networks is a critical but also difficult task. In this position paper, we present a new trust architecture and model -situation-aware trust (SAT) - to address several important trust issues in vehicular networks that rely on the current Internet infrastructure. The new model addresses several important trust issues that we believe are essential to overcome the weaknesses of the current vehicular network security and trust models. Our new SAT includes three main components: (a) an attribute based policy control model for highly dynamic communication environments, (b) a proactive trust model to build trust among vehicles and prevent the breakage of the existing trust, and (c) a social network based trust system to enhance trust and to allow the set up of a decentralized trust framework when the vehicular network is under infrastructure failure or under attacks.' as abstract.

there is a document named 'doc-1034' that
  has 'We consider variations of a problem in which data must be delivered to mobile clients en-route, as they travel towards their destinations. The data can only be delivered to the mobile clients as they pass within range of wireless base stations. Example scenarios include the delivery of building maps to firefighters responding to multiple alarms, and the in-transit "illumination" of simultaneous surface-to-air missiles. We cast this scenario as a parallel-machine scheduling problem with the little-studied property that jobs may have different release times and deadlines when assigned to different machines. We present new algorithms and also adapt existing algorithms, for both online and offline settings. We evaluate these algorithms on a variety of problem instance types, using both synthetic and real-world data, and including several geographical scenarios, and show that our algorithms produce schedules achieving nearoptimal throughput.' as abstract.

there is a document named 'doc-1058' that
  has 'Routing protocols for Wireless Sensor Networks (WSN) face three major performance challenges. The first one is an efficient use of bandwidth that minimizes the transfer delay of packets between nodes to ensure the shortest end-to-end delay for packet transmission from source to destination. The second challenge is the ability to maintain data flow around permanent and transient node or link failures ensuring the maximum delivery rate of packets from source to destination. The final challenge is to efficiently use energy while maximizing delivery rate and minimizing end-to-end delay. Protocols that establish a permanent route between source and destination send packets from node to node quickly, but suffer from costly route recalculation in the event of any node or link failures. Protocols that select the next hop at each node on the traversed path suffer from a delay required to make such selection. The way in which a protocol repairs routes determines the number of packets lost by each failure and ultimately affects the energy used for communication. This paper presents a novel family of wireless sensor routing protocols, the Self-Selecting Reliable Path Routing Protocol Family (SSRPF), that address all three of the afore-mentioned challenges.' as abstract.

there is a document named 'doc-1069' that
  has 'Multipath routing can reduce the need for route updates, balance the traffic load and increase the data transfer rate in a wireless sensor network, improving the utilization of the limited energy of sensor nodes. However, previous multiple path routing methods use flooding for route discovery and transmit data with maximum power regardless of need, which results in waste of energy. Moreover, often a serious problem of collisions among multiple paths arises. In this paper, we propose an energy efficient and collision aware (EECA) node-disjoint multipath routing algorithm for wireless sensor networks. With the aid of node position information, the EECA algorithm attempts to find two collision-free routes using constrained and power adjusted flooding and then transmits the data with minimum power needed through power control component of the protocol. Our preliminary simulation results show that ECCA algorithm results in good overall performance, saving energy and transferring data efficiently.' as abstract.

there is a document named 'doc-1070' that
  has 'In wireless networks, the broadcast nature of wireless transmission enables cooperation by sharing the same transmissions with nearby receivers and thus can help improve spatial reuse and boost network throughput along a multi-hop routing. The performance of wireless networks can be further improved if prior information available at the receivers can be utilized to achieve perfect interference subtraction. In this paper, we investigate performance gain on network throughput for wireless cooperative networks by using a simple MUD scheme, called overlapped transmission, in which multiple transmissions are allowed only when the information in the interfering signal is known at the receiver. It is shown that the scheme of cooperative transmission with overlapping increases network throughput by 24% compared to that of direct transmission with overlapping. We then propose a new cooperation scheme called supplementary cooperation, which improves the performance gain of direct transmission with overlapping by 42%. Analytical results are developed to show that in a general network scenario, supplementary cooperation achieves bit error rate (BER) reduction of 34.87%, compared with the conventional cooperative transmission. Furthermore, we proposed a criterion for finding the best cooperative route to achieve maximum network throughput in a general network.' as abstract.

there is a document named 'doc-1071' that
  has 'Most ad hoc mobile devices in wireless networks operate on batteries and power consumption is therefore an important issue for wireless network design. In this paper, we propose and investigate a new distributed cooperative routing algorithm that realizes minimum power transmission for each composed cooperative link, given the link BER (Bit Error Rate) constrained at a certain target level. The key contribution of the proposed scheme is to bring the performance gain of cooperative diversity from the physical layer up to the networking layer. Specifically, the proposed algorithm selects the best relays with minimum power consumption in distributed manner, and then forms cooperative links for establishing a route with appropriate error performance from a source to a destination node. Analytical results are developed to show that our cooperative transmission strategy (MPSDF) achieves average energy saving of 82.43% compared to direct transmission, and of 21.22% compared to the existing minimum power cooperation strategy. Furthermore, the proposed power efficient routing algorithm can also reduce the total power consumption by a couple dB compared to existing cooperative routing algorithms. Monte-Carlo simulation results are also provided for performance evaluation.' as abstract.

there is a document named 'doc-1073' that
  has 'This dissertation presents a Distributed denial-of-service Adaptive ResponsE (DARE) system, capable of executing appropriate detection and mitigation responses automatically and adaptively according to the attacks. It supports easy integration of distributed modules for both signature-based and anomaly-based detection. Additionally, the innovative design of DARE\'s individual components takes into consideration the strengths and weaknesses of existing defence mechanisms, and the characteristics and possible future mutations of DDoS attacks. The distributed components work together interactively to adapt detection and response according to the attack types. Experiments on DARE show that the attack detection and mitigation were successfully completed within seconds, with about 60% to 86% of the attack traffic being dropped, while availability for legitimate and new legitimate requests was maintained. DARE is able to detect and trigger appropriate responses in accordance to the attacks being launched with high accuracy, effectiveness and efficiency. The dissertation is available at http://pubs.doc.ic.ac.uk/VrizlynnThing-PhD-Thesis- 2008/VrizlynnThing-PhD-Thesis-2008.pdf.' as abstract.

there is a document named 'doc-1077' that
  has 'In many situations it is important to deliver information to personnel as they work in the field. We consider such a specialized content distribution application in wireless mesh networks. When a new mission arrives, for example, when an alarm for a fire is reported, data is pushed to storage nodes at the mission site where it may be retrieved locally by responding personnel (e.g., police, firefighters, paramedics, government offi- cials, and the media). It is important that information is available at low latency, when requested or pulled by the personnel. The total latency experienced will be a combination of the push delay (if the personnel arrive at the mission site before all the data can be pushed), and the pull delay. Each delay component will be a function of 1) the hop distance traveled by the data when pushed or pulled and 2) the congestion on the links. In this paper, we define algorithms and protocols that trade-off the push and pull latencies depending on the type of application. Our goal is to choose storage nodes assignment minimizing the total latency-based cost. We start with a simple model in which cost is a function of distance, and then extend the model explicitly taking congestion into account. Since the problem is NP-hard to approximate, our focus is on developing efficient algorithms and distributed protocols that can be easily deployed in wireless mesh networks. In NS2 simulations, we find that our heuristic algorithms achieve on average a cost within at most 15% of the optimum.' as abstract.

there is a document named 'doc-1084' that
  has 'Goal-oriented methods have increasingly been recognised as an effective means for eliciting, elaborating, analysing and specifying software requirements. A key activity in these approaches is the elaboration of a correct and complete set of opertional requirements, in the form of pre- and trigger-conditions, that guarantee the system goals. Few existing approaches provide support for this crucial task and mainly rely on significant effort and expertise of the engineer. In this paper we propose a tool-based framework that combines model checking, inductive learning and scenarios for elaborating operational requirements from goal models. This is an iterative process that requires the engineer to identify positive and negative scenarios from counterexamples to the goals, generated using model checking, and to select operational requirements from suggestions computed by inductive learning.' as abstract.

there is a document named 'doc-1095' that
  has 'Research in pervasive and autonomic computing focuses on supporting services for pervasive applications, but often ignores how such applications can be realised through the federation of autonomous entities. In this paper we propose a methodology for designing collaborations between autonomous components, using the Self-Managed Cell (SMC) framework. We focus on the structural, task-allocation and communication aspects of management interactions between SMCs. We propose a catalogue of architectural styles for SMC interactions, and a model for combining architectural styles in patterns of interactions that can be enforced by different SMCs in large collaborations. This allows us to specify the management of large-scale systems by composing management functions using architectural styles as building block abstractions. A scenario for a health monitoring application involving a number of SMCs is used throughout the paper to illustrate how complex structures can be thus built.' as abstract.

there is a document named 'doc-1100' that
  has 'In sensor networks, it is the Quality of Information (QoI) delivered to the end user that is of primary interest. In general, measurements from different sensor nodes do not contribute equally to the QoI because of differing sensing modalities, node locations, noise levels, sensing channel conditions, fault status, and physical process dynamics. In addition, metrics of QoI are highly application dependent, such as probability of detection of an event or fidelity of reconstruction of a spatio-temporal process. Despite these considerations, traditional data dissemination protocols in sensor networks have been designed with a focus on metrics such as throughput, packet delivery ratio, latency, and fair division of bandwidth, and are thus oblivious to the importance and quality of sensor data and the target application. In this paper, we argue for sensor network protocols that are cognizant of and use feedback from the sensor fusion algorithms to explicitly optimize for application-relevant QoI metrics during network resource allocation decisions. Through analysis and simulation we demonstrate the application-level performance benefits accruing from such a QoI-aware approach to network resource management in the context of a centralized sensor rate selection mechanism for an event detection application scenario.' as abstract.

there is a document named 'doc-1102' that
  has 'The Random Re-Routing (RRR) algorithm has recently been introduced to provide fast adaptive priority routing to rapidly convey packets of important events in sensor networks, while forwarding routine and lower priority traffic along some secondary network paths. This technique can be applied for sensor networks which monitor the environment by a large amount of sensors distributed in different locations. Such a network has to report large volumes of slowly varying routine data and must quickly report the rarer but more significant events that require immediate attention. In this paper, the RRR algorithm and its implementation in real sensors are presented. Experimental results are provided to show the performance of the algorithm in our sensor network testbed.' as abstract.

there is a document named 'doc-1112' that
  has 'We present the first hierarchical identity based encryption (HIBE) system that has full security for more than a constant number of levels. In all prior HIBE systems in the literature, the security reductions suffered from exponential degradation in the depth of the hierarchy, so these systems were only proven fully secure for identity hierarchies of constant depth. (For deep hierarchies, previous work could only prove the weaker notion of selective-ID security.) In contrast, we offer a tight proof of security, regardless of the number of levels; hence our system is secure for polynomially many levels. Our result can very roughly be viewed as an application of Boyen\'s framework for constructing HIBE systems from exponent-inversion IBE systems to a (dramatically souped-up) version of Gentry\'s IBE system, which has a tight reduction. In more detail, we first describe a generic transformation from "identity based broadcast encryption with key randomization" (KR-IBBE) to a HIBE, and then construct KR-IBBE by modifying a recent construction of IBBE of Gentry and Waters, which is itself an extension of Gentry\'s IBE system. Our hardness assumption is similar to that underlying Gentry\'s IBE system.' as abstract.

there is a document named 'doc-1121' that
  has 'Complex communication networks, more particular Mobile Ad Hoc Networks (MANET) and Pocket Switched Networks (PSN), rely on short range radio and device mobility to transfer data across the network. These kind of mobile networks contain duality in nature: they are radio networks at the same time also human networks, and hence knowledge from social networks can be also applicable here. In this paper, we demonstrate how identifying social communities can significantly improve the forwarding efficiencies in term of delivery ratio and delivery cost. We verify our hypothesis using data from five human mobility experiments and test on two application scenarios, asynchronous messaging and publish/subscribe service.' as abstract.

there is a document named 'doc-1171' that
  has 'Multicast streaming is gaining increasing importance in wireless ad hoc networks, in part because ad hoc scenarios often include team activities and the requirement for distribution of audio, video and situation awareness to the members. At the network level, techniques for routing the multimedia streams are quite mature. Much more challenging is the allocation of resources, the fair sharing among streams and the control of congestion. While in rate adaptive UNICAST streams congestion control and fair sharing are accomplished with end-to-end feedback techniques inspired to TCP, the feedback does not scale well in MULTICAST. In fact, it leads to the well knows ACK/NAK "implosion" problem and unfair penalties for heterogeneous receivers. These limitations can be overcome using backpressure from congestion points to the sources - but this approach suffers of latency and cannot rapidly adjust to changes in traffic. Another solution is multilayer adaptive coding. Namely, the encoding adaptation is done locally by dropping layers. It does not require end-to-end feedback nor changes in input rates. Multi-resolution codes are now becoming attractive due to the progress in technology; we expect these to become the prevalent techniques in large scale media distribution. One issue, however, that still remains to be resolved is the fair sharing among competing multicast streams. In this paper we address the congestion control AND fair sharing in a multilayer multicast scenario. We show that lack of proper fairness provisions in the "local adjustments" can lead to serious capture situations, especially in heterogeneous traffic mixes (e.g. voice and video). We then propose a FAIR local adjustment that targets a fair dropping of packets in each interference domain. We show that the scheme can be interpreted as a distributed implementation of a utility function minimization, where the utility is the packet loss subject to fairness bounds across flows. This formulation guarantees stability and convergence of the distributed algorithm. The main contributions of this paper are the low overhead design of the local fairness enforcement algorithm, the utility function framework and the demonstration of convergence via simulation in representative scenarios.' as abstract.

there is a document named 'doc-1183' that
  has 'Much of the traffic carried by Sensor Networks will originate from routine measurements or observations by sensors which monitor a particular situation, such as the temperature and humidity in a room or the infrared observation of the perimeter of a house, so that the volume of routine traffic resulting from such observations may be quite high. When important and unusual events occur, such as a sudden fire breaking out or the arrival of an intruder, it will be necessary to convey this new information very urgently through the network to a designated set of sink nodes where this information can be processed and dealt with. This paper addresses the important challenge of avoiding that the volume of routine background traffic creates delays or bottlenecks that impede the rapid delivery of high priority traffic resulting from the unusual events. Specifically we propose a novel technique, the "Randomized Re-Routing Algorithm (RRR)", which detects the presence of novel events in a distributed manner, and dynamically disperses the background traffic towards secondary paths in the network, while creating a "fast track path" which provides better delay and better QoS for the high priority traffic which is carrying the new information. When the surge of new information has subsided, this is again detected by the nodes and they can progressively revert to best QoS or shortest path routing for all the ongoing traffic. The proposed technique is evaluated using a mathematical model as well as simulations.' as abstract.

there is a document named 'doc-1195a' that
  has 'Random projection (RP) is a common technique for dimensionality reduction under L2 norm for which many significant space embedding results have been demonstrated. However, many similarity search applications often require very low dimension embeddings in order to reduce overhead and boost performance. Inspired by the use of symmetric probability distributions in previous work, we propose a novel RP algorithm, Beta Random Projection, and give its probabilistic analyses based on Beta and Gaussian approximations. We evaluate the algorithm in terms of standard similarity metrics with other RP algorithms as well as the singular value decomposition (SVD). Our experimental results show that BRP preserves both similarity metrics well and, under various dataset types including random point sets, text (TREC5) and images, provides sharper and consistent performance.' as abstract.

there is a document named 'doc-1219a' that
  has 'This paper extends the distributed network utility maximization (NUM) framework to consider the case of resource sharing by multiple competing missions in a military-centric wireless sensor network (WSN) environment. Prior work on NUMbased optimization has considered unicast flows with sender-based utilities in either wireline or wireless networks. We extend the NUM framework to consider three key new features observed in mission-centric WSN environments: i) the definition of an individual mission\'s utility as a joint function of data from multiple sensor sources ii) the consumption of each senders (sensor) data by multiple receivers (missions) and iii) the multicasttree based dissemination of each sensors data flow, using linklayer broadcasts to exploit the "wireless broadcast advantage" in data forwarding. We show how a receiver-centric, pricing-based, decentralized algorithm can ensure optimal and proportionallyfair rate allocation across the multiple missions, without requiring any coordination among independent missions (or sensors). We also discuss techniques to improve the speed of convergence of the protocol, which is essential in an environment as dynamic as the WSN.' as abstract.

there is a document named 'doc-1238' that
  has 'We propose and investigate the notion of aggregate message authentication codes (MACs) which have the property that multiple MAC tags, computed by (possibly) different senders on multiple (possibly different) messages, can be aggregated into a shorter tag that can still be verified by a recipient who shares a distinct key with each sender. We suggest aggregate MACs as an appropriate tool for authenticated communication in mobile ad-hoc networks or other settings where resource-constrained devices share distinct keys with a single entity (such as a base station), and communication is an expensive resource.' as abstract.

there is a document named 'doc-1256' that
  has 'Although several wide-spread internet applications (e.g., job-referral services, dating services) can benefit from online matchmaking, protocols defined over the past two decades fail to address important privacy concerns. In this paper, we enhance traditional privacy requirements (e.g., user anonymity, matching-wish authenticity) with new privacy goals (e.g., resistance to off-line dictionary attacks, and forward privacy of users\' identities and matching wishes), and argue that privacy-enhanced matchmaking cannot be provided by solutions to seemingly related problems such as secret handshakes, set intersection, and trust negotiation. We define an adversary model, which captures the key security properties of privacy-enhanced matchmaking, and show that a simple, practical protocol derived by a two-step transformation of a password-based authenticated key exchange counters adversary attacks in a provable manner (in the standard model of cryptographic security).' as abstract.

there is a document named 'doc-1282' that
  has 'Feature selection is an important pre-processing step for pattern recognition. It can discard irrelevant and redundant information that may not only affect a classifier\'s performance, but also tell against system\'s efficiency. Meanwhile, feature selection can help to identify the factors that most influence the recognition accuracy. The result can provide valuable clues to understand and reason what is the underlying distinctness among human gait-patterns. In this paper, we introduce a computationally-efficient solution to the problem of human gait feature selection. We show that feature selection based on mutual information can provide a realistic solution for high-dimensional human gait data. To assess the performance of the proposed approach, experiments are carried out based on a 73-dimensional model-based gait features set and a 64 by 64 pixels model-free gait symmetry map. The experimental results confirmed the effectiveness of the method, removing about 50% of the model-based features and 95% of the symmetry map\'s pixels without significant accuracy loss, which outperforms correlation and ANOVA based methods.' as abstract.

there is a document named 'doc-1330' that
  has 'Commitment management is a key issue in service-provisioning in the context of virtual organisations (VOs). A service-provider - which may be a single agent acting within an organisation, or the VO acting as a collective whole - manages particular resources, and commits these resources to meet specific goals. Commitments can be modelled as constraints on resources. Such constraints are often soft: they can be broken if necessary. The goal of the work described in this paper is to create an open, reusable commitment management service (CMS) based on Semantic Web standards. The chief requirement is that the CMS should be reusable in different domains, able to manage commitments over services described in a wide range of domain-specific service ontologies. This paper presents open Semantic Web representations for (1) expressing individual commitments as constraints over service descriptions, (2) capturing a set of commitments as a soft constraint satisfaction problem, and (3) representing and communicating the solution to a soft CSP. A reference implementation of a constraint solver able to operate on (1) and (2) to produce (3) is described, and its reuse is demonstrated in two distinct domains: e-commerce and e-response.' as abstract.

there is a document named 'doc-1341' that
  has 'We consider a scenario where multiple pairs of users exchange information within pair, with the help of a dedicated multi-antenna relay. The protocol integrates the idea of analogue network coding in mixing two data streams originating from the same user pair, together with the spatial multiplexing of the data streams originating from different user pairs. The key feature of the protocol is that it enables both the relay and the users to participate in interference cancellation. We propose several beamforming schemes for the multi-antenna relay and evaluate the performance using information theoretical metrics such as ergodic capacity, outage probability and diversity and multiplexing tradeoff. Analytical and simulation results justify that the ergodic capacity, outage probability and diversity and multiplexing tradeoff of the proposed beamforming schemes outperform comparable schemes.' as abstract.

there is a document named 'doc-1342' that
  has 'In this paper, we study the application of physical layer network coding to the joint design of uplink and downlink transmissions, where the base station and the relay have multiple antennas, and all M mobile stations only have a single antenna. A new network coding transmission protocol is proposed, where 2M uplink and downlink transmissions can be accomplished within two time slots. Since each single antenna user has poor receive capability, precoding at the base station and relay has been carefully designed to ensure that co-channel interference can be removed completely. Explicit analytic results have been developed to demonstrate that the multiplexing gain achieved by the proposed transmission protocol is M, much better than existing time sharing schemes. To further increase the achievable diversity gain, two variations of the proposed transmission protocols have also been proposed when there are multiple relays and the number of the antennas at the base station and relay is increased. Monte-Carlo simulation results have also been provided to demonstrate the performance of the proposed network coded transmission protocol.' as abstract.

there is a document named 'doc-1343' that
  has 'Wireless vehicular ad hoc networks are characterized by multi-hop transmission, where a key problem is the design of routing, e.g., how to efficiently direct the information flow from the source to the destination via available intermediate nodes. In this paper, cross-layer routing is studied by applying cooperative transmission and a new strategy of path selection is proposed to achieve a better tradeoff between the transmission power consumption and end-to-end reliability. The influence of cooperative transmission to the wireless link cost is first studied, which shows that the quality of wireless links could be improved significantly. Then we focus on formulating the objective functions for the addressed cross-layer optimization problem. Specifically, according to different quality of service requirements, two different types of routing optimization are investigated to understand the effects of improved link cost to the routing decision. The closed-form expressions of the optimal solutions for two addressed optimization problems are developed and later used as quantitative criteria of the routing decision. Our developed analytical and simulation results show that the criteria using cooperative transmission typically yield more efficient routes than the comparable schemes in terms of end-to-end reliability and total transmission power.' as abstract.

there is a document named 'doc-1344' that
  has 'In this paper, cooperative transmission protocols are proposed for wireless broadcast channels, a fundamental building block of wireless communication networks. The concepts of cognitive radio and precoding have been introduced to broadcast channels in order to improve system performance. Information theoretic metrics, such as outage probability and diversity-multiplexing tradeoff, are developed to facilitate performance evaluation. In the absence of direct S-D links, the proposed protocols can achieve a multiplexing gain close to one, whereas the traditional two-hop scheme only achieves a diversity gain of 1/2. In the presence of direct S-D links, the proposed protocol can still outperform the comparable scheme, particularly at high multiplexing gains. Regarding to the channel state information (CSI) assumptions, in the absence of direct S-D links, the source does not need to know CSI, but it is assumed that the relays have access to their own incoming and outgoing channel information. In the presence of direct S-D links, the use of precoding requires an extra assumption that the global CSI is available at the source.' as abstract.

there is a document named 'doc-1346' that
  has 'The information content of a graph G is defined in [12] as the entropy of a finite probability scheme associated with the vertex partition determined by the automorphism group of G. This provides a quantitative measure of the symmetry structure of a graph that has been applied to problems in such diverse fields as chemistry, biology, sociology and computer science ([16]). The measure extends naturally to directed graphs (digraphs) and can be defined for infinite graphs as well ([13]). This chapter focuses on the information content of digraphs and infinite graphs. In particular, the information content of digraph products and recursively defined infinite graphs is examined.' as abstract.

there is a document named 'doc-1347' that
  has 'This note introduces a quantitative measure (S(G)) of the symmetry structure of a graph G. S(G) is called the index of symmetry of G and is a function of its structural information content and the order of its automorphism group. Properties of S(G) are examined and applications to special classes of graphs are presented. In particular, the index of symmetry of a class of alkanes is shown to be of order k, where k is the number of carbon atoms.' as abstract.

there is a document named 'doc-1348' that
  has 'This paper explores relationships between classical and parametric measures of graph (or network) complexity. Classical measures are based on vertex decompositions induced by equivalence relations. Parametric measures, on the other hand, are constructed by using information functions to assign probabilities to the vertices. The inequalities established in this paper relating classical and parametric measures lay a foundation for systematic classification of entropy-based measures of graph complexity.' as abstract.

there is a document named 'doc-1350' that
  has 'In this paper, we present a novel, multi-period spraying algorithm for routing in Delay Tolerant Networks (DTN). The goal is to minimize the average copy count used per message until the delivery while maintaining the predefined message delivery rate by the given deadline. In each period, some number of additional copies are sprayed into the network, followed by the wait for message delivery. At any time instance, the total number of message copies distributed to the network depends on the urgency of achieving the delivery rate by the given deadline for that message. Waiting for early delivery in the initial periods with small number of copies in existence decreases the average number of copies sprayed in the network till delivery. We first discuss 2- and 3-period variants of our algorithm and then we also give an idea how the presented approach can be extended to more periods. We present an in-depth analysis of the algorithm and validate the analytical results with simulations. The results demonstrate that our multi-period spraying algorithm outperforms the algorithms with single spraying period.' as abstract.

there is a document named 'doc-1354' that
  has 'The natural world is enormous, dynamic, incredibly diverse, and highly complex. Despite the inherent challenges of surviving in such a world, biological organisms evolve, self-organize, self-repair, navigate, and flourish. Generally, they do so with only local knowledge and without any centralized control. Our computer networks are increasingly facing similar challenges as they grow larger in size, but are yet to be able to achieve the same level of robustness and adaptability. Many research efforts have recognized these parallels, and wondered if there are some lessons to be learned from biological systems. As a result, biologically inspired research in computer networking is a quickly growing field. This article begins by exploring why biology and computer network research are such a natural match. We then present a broad overview of biologically inspired research, grouped by topic, and classified in two ways: by the biological field that inspired each topic, and by the area of networking in which the topic lies. In each case, we elucidate how biological concepts have been most successfully applied. In aggregate, we conclude that research efforts are most successful when they separate biological design from biological implementation - that is to say, when they extract the pertinent principles from the former without imposing the limitations of the latter.' as abstract.

there is a document named 'doc-1355' that
  has 'Many organizations depend on critical sensory information to achieve their tasks. As the number of those tasks increase, efficient determination and allocation of required resources in sensor networks become crucial. In this paper, we propose means to describe tasks semantically with their requirements and constraints so that software agents can reason about those tasks and determine what type of sensor resources they may need. Based on the semantic description and reasoning mechanisms, we propose a distributed agent-based approach to efficiently allocate sensor resources to tasks. Our evaluation of the proposed approach shows that not only it enables fully automated determination and allocation of resources for tasks, but also the resulting allocation is efficient and close to optimum.' as abstract.

there is a document named 'doc-1357' that
  has 'This article presents the results of a field research study examining commonalities and differences between American and British operational planners\' mental models of planning. We conducted cultural network analysis interviews with 14 experienced operational planners in the United States and the United Kingdom. Our results demonstrate the existence of fundamental differences between the ways American and British expert planners conceive of a high-quality plan. Our results revealed that the American planners\' model focused on specification of action to achieve synchronization, providing little autonomy at the level of execution, and included the belief that increasing contingencies reduces risk. The British planners\' model stressed the internal coherence of the plan to support shared situational awareness and thereby flexibility at the level of execution. The British model also emphasized the belief that reducing the number of assumptions decreases risk. Overall, the American ideal plan serves a controlling function, whereas the British ideal plan supports an enabling function. Interestingly, both the U.S. and UK planners viewed the other\'s ideal plan as riskier than their own. The implications of cultural models of plans and planning are described for establishing performance measures and designing systems to support multinational planning teams.' as abstract.

there is a document named 'doc-1358' that
  has 'Gupta and Kumar established that the per node throughput of ad hoc networks with multi-pair unicast traffic scales with an increasing number of nodes n as ?(n) = T(1/vn log n), thus indicating that performance does not scale well. However, Gupta and Kumar did not consider network coding and wireless broadcasting, which recent works suggest have the potential to significantly improve throughput. Here, we establish bounds on the improvement provided by such techniques. For random networks of any dimension under either the protocol or physical model that were introduced by Gupta and Kumar, we show that network coding and broadcasting lead to at most a constant factor improvement in per node throughput. For the protocol model, we provide bounds on this factor. We also establish bounds on the throughput benefit of network coding and broadcasting for multiple source multicast in random networks. Finally, for an arbitrary network deployment, we show that the coding benefit ratio is at most O(log n) for both the protocol and physical communication models. These results give guidance on the application space of network coding, and, more generally, indicate the difficulty in improving the scaling behavior of wireless networks without modification of the physical layer.' as abstract.

there is a document named 'doc-1359' that
  has 'Extensive research has demonstrated the potential improvement in physical layer performance when multiple radios transmit concurrently in the same radio channel. We consider how such cooperation affects the requirements for full connectivity and percolation in large wireless ad hoc networks. Both noncoherent and coherent cooperative transmission are considered. For one-dimensional (1-D) extended networks, in contrast to noncooperative networks, for any path loss exponent less than or equal to one, full connectivity occurs under the noncoherent cooperation model with probability one for any node density. Conversely, there is no full connectivity with probability one when the path loss exponent exceeds one, and the network does not percolate for any node density if the path loss exponent exceeds two. In twodimensional (2-D) extended networks with noncoherent cooperation, for any path loss exponent less than or equal to two, full connectivity is achieved for any node density. Conversely, there is no full connectivity when the path loss exponent exceeds two, but the cooperative network percolates for node densities above a threshold which is strictly less than that of the noncooperative network. A less conclusive set of results is presented for the coherent case. Hence, even relatively simple noncoherent cooperation improves the connectivity of large ad hoc networks.' as abstract.

there is a document named 'doc-1361a' that
  has 'New approaches to Quality-of-Service (QoS) Routing in wireless sensor networks which use different forms of learning are the subject of this paper. The Cognitive Packet Network (CPN) algorithm uses smart packets for path discovery, together with reinforcement learning and neural networks, while SelfHealing Routing (SHR) is based on an ant colony paradigm which emulates the pheromone based technique which biological ants use to mark paths and communicate information about paths between different insects of the same colony. In this paper we present first experimental results on a network testbed to evaluate CPN\'s ability to discover paths having the shortest delay, or shortest length. Then, we present small test-bed experiments and large-scale network simulations to evaluate the effectiveness of the SHR algorithm. Finally, the two approaches are compared with respect to their ability to adapt as network conditions change over time.' as abstract.

there is a document named 'doc-1362' that
  has 'An inelastic flow is a flow with inelastic rate: i.e., the rate is fixed, it cannot be dynamically adjusted to traffic and load condition as in elastic flows like TCP. Real time, interactive sessions and video/audio streaming are typical examples of inelastic flows. Reliable support of inelastic flows in wireless ad hoc networks is extremely challenging because flows and routes dynamically change and flows compete for the shared wireless channel. Bandwidth must be reserved for inelastic flows at session set up time. To avoid repeated attempts to set up reservations in a "volatile" network and prevent serious network capacity degradation due to call set up overhead, a Call Admission Control strategy robust to mobility must be developed. In this paper we propose ProbeCast, a probe based call admission control scheme with QoS guarantees for inelastic flows. ProbCast was designed for multicast streams but can also work, by default, for unicast. In ProbeCast, a path (or a tree) is probed for capacity availability. If an intermediate link along the probed path fails to meet the QoS requirement, the flow is "pushed back" via backpressure upstream to an intermediate branch or possibly to the source. The backpressure principle is simple; however its implementation requires some care to avoid unfairness and eventual capture by one of the flows sharing a congested bottleneck. We show that proportional fairness among inelastic contenders will prevent capture. To achieve this, we have developed the Neighborhood Proportional Drop (N-PROD) scheme. N-PROD guarantees fair rejection of unfeasible flows and maintains the same proportional drop rate among surviving flows in the same contention domain. We demonstrate the efficacy and robustness of ProbeCast for unicast as well as multicast scenarios using the Qualnet simulation platform.' as abstract.

there is a document named 'doc-1363' that
  has 'Time synchronization in embedded sensor networks is an important service for correlating data between nodes and communication scheduling. While many different approaches to the problem are possible, one major effect of clock frequency difference between nodes, environmental temperature changes, has often been left out of the solution. The common assumption that the temperature is static over a certain period of time is often used as an excuse to assume constant frequency errors in a clock. This assumption forces synchronization protocols to resynchronize too often. While there exists hardware solutions to this problem, their prohibitive high cost and power consumption make them unsuitable for some applications, such as wireless sensor networks. Temperature Driven Time Synchronization (TDTS) exploits the onboard temperature sensor existing in many sensor network platforms. It uses this temperature sensor to autonomously calibrate the local oscillator and removes effects of environmental temperature changes. This allows a time synchronization protocol to increase its resynchronization period, without loosing synchronization accuracy, and thus saves energy and communication overhead. In addition, TDTS provides a stable clock source when radio communication is impaired. We present the theory behind TDTS, and provide initial results of a simulated comparison of TDTS and the Flooding Time Synchronization Protocol.' as abstract.

there is a document named 'doc-1364a' that
  has 'Studies of worm outbreaks have found that the speed of worm propagation makes manual intervention ineffective. Consequently, many automated containment mechanisms have been proposed to contain worm outbreaks before they grow out of control. These containment systems, however, only provide protection for hosts within networks that implement them. Such a containment strategy requires complete participation to protect all vulnerable hosts. Moreover, collaborative containment systems, where participants share alert data, face a tension between resilience to false alerts and quick reaction to worm outbreaks. This paper suggests an alternative approach where an autonomous system in an internetwork, such as the Internet, protects not only its local hosts, but also all hosts that route traffic through it, which we call internetwork-centric containment. Additionally, we propose a novel reputation-based alerting mechanism to provide fast dissemination of infection information while maintaining the fairness of the system. Through simulation studies, we show that the combination of internetwork-centric containment and reputation-based alerting is able to contain an extremely virulent worm with relatively little participation in the containment system. In comparison to other collaborative containment systems, ours provides better protection against worm outbreaks and resilience to false alerts.' as abstract.

there is a document named 'doc-1365' that
  has 'The vitality and utility of a network is affected significantly by the network management system which is used to administer and monitor the network. However, models that can characterize how good a network management system are generally missing in the literature. In this paper, we introduce the concept of Quality of Monitoring (QoM), provide a mathematical formulation based on stochastic processes that can be used to model a network monitoring system and define QoM metrics based on this formulation. A formal analysis of the proposed framework along various metrics is provided, along with a case study of its application to network monitoring for MANET.' as abstract.

there is a document named 'doc-1367' that
  has 'In this paper, we describe, how agents can support collaborative planning within international coalitions, formed in an ad-hoc fashion as a response to military and humanitarian crises. As these coalitions are formed rapidly and without much lead time or co-training, human planners may be required to observe a plethora of policies that direct their planning effort. In a series of experiments, we show how agents can support human planners and ease their cognitive burden by giving advice on the correct use of policies and catch possible violations.' as abstract.

there is a document named 'doc-1368' that
  has 'Sensor networks face a number of challenges when deployed in unpredictable environments under dynamic, quickly changeable demands, and when shared by many partners, which is often the case in military and security applications. To partially address these challenges, we present a novel target tracking algorithm that can be deployed on various sensor nodes and invoked dynamically when needed by the presence of targets. We also demonstrate that an auction-based mechanism can be used to provide efficient and localized wireless sensor network (WSN) congestion management for bursty traffic of abstract services based just on user-assigned priorities to different services and the quality of information provided by the services. We present results from using this auction mechanism to resolve congestion caused by packets from competing target tracking missions.' as abstract.

there is a document named 'doc-1369' that
  has 'Team decision making is a bundle of interdependent activities that involve gathering, interpreting and exchanging information; creating and identifying alternative courses of action; choosing among alternatives by integrating the often different perspectives of team members; implementing a choice and monitoring its consequences. To accomplish joint tasks, human team members often assume distinctive roles in task completion. We believe that to design and build software agents that can assist human teams, we need develop automated techniques to identify the roles of the human decision-makers. If the supporting agents are insensitive to shifts in the team\'s roles, they cannot effectively monitor the team\'s activities. This article addresses the problem of doing offline role analysis of battle scenarios from multi-player team games. The ability to identify team roles from observations is important for a wide range of applications including automated commentary generation, game coaching, and opponent modeling. We define a role as a preference model over possible actions based on the game state. This article explores two promising approaches for automated role analysis: (1) a model-based system for combining evidence from observed events using Dempster-Shafer theory, and (2) a data-driven discriminative classifier using support vector machines (SVMs).' as abstract.

there is a document named 'doc-1370' that
  has 'Modelling based on probabilistic inference can be used to estimate the quality of information delivered by a military sensor network. Different modelling tools have complementary characteristics that can be leveraged to create an accurate model open to intuitive and efficient querying. In particular, stochastic process models can be used to abstract away from the physical reality by describing it as components that exist in discrete states with probabilistically invoked actions that change the state. The quality of information may be assessed by using the model to compute the probability that reports made by the network to its users are correct. In contrast, dynamic Bayesian network models, which have been used in a variety of military applications, are a more suitable vehicle for understanding the overall network performance and making inferences about the quality of information. Queries can be made simply by instantiating some variables and computing the probability distributions over others. We show that it is possible to combine both modelling tools by constructing a Bayesian network over the state variables of the process algebra model. The sparsity of the resulting Bayesian network allows fast propagation of probabilities, and hence interactive querying for the quality of information.' as abstract.

there is a document named 'doc-1374' that
  has 'In recent years we have witnessed several applications of frequent sequence mining, such as feature selection for protein sequence classification and mining block correlations in storage systems. In typical applications such as clustering, it is not the complete set but only a subset of discriminating frequent subsequences which is of interest. One approach to discovering the subset of useful frequent subsequences is to apply any existing frequent sequence mining algorithm to find the complete set of frequent subsequences. Then, a subset of interesting subsequences can be further identified. Unfortunately, it is very time consuming to mine the complete set of frequent subsequences for large sequence databases. In this paper, we propose a new algorithm, CONTOUR, which efficiently mines a subset of high-quality subsequences directly in order to cluster the input sequences. We mainly focus on how to design some effective search space pruning methods to accelerate the mining process and discuss how to construct an accurate clustering algorithm based on the result of CONTOUR. We conducted an extensive performance study to evaluate the efficiency and scalability of CONTOUR, and the accuracy of the frequent subsequence-based clustering algorithm.' as abstract.

there is a document named 'doc-1375' that
  has 'Mobile peer-to-peer systems have recently got in the limelight of the research community that is striving to build efficient and effective mobile content addressable networks. Along this line of research, we propose a new peer-to-peer (P2P) file sharing protocol suited to mobile ad hoc networks (MANET). The main ingredients of our protocol are network coding and mobility assisted data propagation, i.e., single-hop communication. We argue that network coding in combination with singlehop communication allows P2P file sharing systems in MANET to operate in a more efficient manner and helps the systems to deal with typical MANET issues such as dynamic topology and intermittent connectivity as well as various other issues that have been disregarded in previous MANET P2P researches such as addressing, node/user density, non-cooperativeness, and unreliable channel. Via simulation, we show that our P2P protocol based on network coding and single-hop communication allows shorter file downloading delays compared to an existing MANET P2P protocol.' as abstract.

there is a document named 'doc-1376' that
  has 'Norms (permissions, obligations and prohibitions) offer a useful and powerful abstraction with which to capture social constraints in multi-agent systems. Norms should exclude disruptive or antisocial behaviour without prescribing the design of individual agents or restricting their autonomy. An important challenge, however, in the design and management of systems governed by norms is that norms may, at times, conflict with one another; e.g, an action may be simultaneously prohibited and obliged for a particular agent. In such circumstances, agents no longer have the option of complying with these norms; whatever they do or refrain from doing will lead to a social constraint being broken. In this paper, we present mechanisms for the detection and resolution of normative conflicts. These mechanisms, based on first-order unification and constraint solving techniques, are the building blocks of more sophisticated algorithms we present for the management of normative positions, that is, the adoption and removal of permissions, obligations and prohibitions in societies of agents. We capture both direct and indirect conflicts between norms, formalise a practical concept of authority, and model conflicts that may arise as a result of delegation. We are able to formally define classic ways for resolving conflicts such as lex superior and lex posterior' as abstract.

there is a document named 'doc-1378' that
  has 'In this correspondence, the performance of the network coded amplify-forward cooperative protocol is studied. The use of network coding can suppress the bandwidth resource consumed by relay transmission, and hence increase the spectral efficiency of cooperative diversity. A distributed strategy of relay selection is applied to the cooperative scheme, which can reduce system overhead and also facilitate the development of the explicit expressions of information metrics, such as outage probability and ergodic capacity. Both analytical and numerical results demonstrate that the proposed protocol can achieve large ergodic capacity and full diversity gain simultaneously.' as abstract.

there is a document named 'doc-1381a' that
  has 'Cooperative diversity systems rely on using relay nodes to relay copies of transmitted information to the destination such that each copy experiences different channel fading, hence increasing the diversity of the system. However, without proper processing of the message at the relays, the performance of the cooperative system may not necessarily perform better than direct transmission systems. In this paper, we proposed a distributed beamforming and power allocation algorithm which substantially improves the diversity of the system with only very limited feedback from the destination node. We also derive outage probability as well as study the outage behavior of this scheme.' as abstract.

there is a document named 'doc-1382a' that
  has 'Vehicular sensor networks (VSN) enable brand new and promising sensing applications, such as traffic reporting, relief to environmental monitoring, and distributed surveillance. We have designed and implemented MobEyes, a middleware solution that supports VSN-based proactive urban monitoring applications, where the agents (e.g. police cars) harvest meta-data from regular VSN-enabled vehicles. In a typical urban sensing operation, multiple agents often collaborate in harvesting relevant data, processing them, and searching for the key information. Thus it is critical to design a mechanism to effectively coordinate the operations of multiple agents and guide them to seek most productive fields in a distributed manner. In this paper, we present a novel data harvesting algorithm in an urban sensing environment that has been designed based on biological inspirations such as foraging, stigmergy, and L\'evy flight. The proposed algorithm enables the agents to move to "information patches " where new information concentration is high. Also when the agent encounters some other agents in the same region it moves to another region so that their work is not duplicated. We validate the performance of our proposed data harvesting scheme via extensive simulations. From this study, we show datataxis effectively balances the movement of agents and distributes them, and performs better than other commonly used distributed harvesting strategies. We also report that our algorithm works well without requiring elaborate tuning of the operation parameters.' as abstract.

there is a document named 'doc-1383' that
  has 'On Demand Multicast Routing Protocol (ODMRP) is a multicast routing protocol for mobile ad hoc networks. Its efficiency, simplicity, and robustness to mobility render it one of the most widely used MANET multicast protocols. At the heart of the ODMRP\'s robustness is the periodic route refreshing. ODMRP rebuilds the data forwarding "mesh" on a fixed interval and thus the route refresh interval is a key parameter that has critical impact on the network performance. If the route refresh rate is too high, the network will undergo too much routing overhead wasting valuable resources. If it is too low, ODMRP cannot keep up with network dynamics resulting in packet losses due to route breakages. In this paper, we present an enhancement of ODMRP with the refresh rate dynamically adapted to the environment. Simulation results show that the Enhanced ODMRP (E-ODMRP) reduces the normalized packet overhead by up to a half yet keeping a packet delivery ratio comparable that of original ODMRP. E-ODMRP compares favorably with other published multicast schemes.' as abstract.

there is a document named 'doc-1384a' that
  has 'One of the challenges in a military wireless sensor network is the determination of an information collection infrastructure which minimizes battery power consumption. The problem of determining the right information collection infrastructure can be viewed as a variation of the network design problem, with the additional constraints related to battery power minimization and redundancy. The problem in its generality is NP-hard and various heuristics have been developed over time to address various issues associated with it. In this paper, we propose a heuristic based on the mammalian circulatory system, which results in a better solution to the design problem than the state of the art alternatives.' as abstract.

there is a document named 'doc-1386' that
  has 'Data generated in wireless sensor networks may not all be alike: some data may be more important than others and hence may have different delivery requirements. In this paper, we address differentiated data delivery in the presence of congestion in wireless sensor networks. We propose a class of algorithms that enforce differentiated routing based on the congested areas of a network and data priority. The basic protocol, called Congestion-Aware Routing (CAR), discovers the congested zone of the network that exists between high-priority data sources and the data sink and, using simple forwarding rules, dedicates this portion of the network to forwarding primarily high-priority traffic. Since CAR requires some overhead for establishing the high-priority routing zone, it is unsuitable for highly mobile data sources. To accommodate these, we define MAC-Enhanced CAR (MCAR), which includes MAC-layer enhancements and a protocol for forming high-priority paths on the fly for each burst of data. MCAR effectively handles the mobility of high-priority data sources, at the expense of degrading the performance of low-priority traffic. We present extensive simulation results for CAR and MCAR, and an implementation of MCAR on a 48-node testbed.' as abstract.

there is a document named 'doc-1390' that
  has 'Routing protocols for wireless sensor networks face two challenges. One is an efficient bandwidth usage which requires minimum delay between transfers of packets. Establishing permanent routes from the source to destination addresses this challenge since the received packet can be immediately transmitted to the next node. However, any disruption on the established path either causes packet loss, lowering the delivery rate, or invokes a costly process of creating an alternative path. The second challenge is the ability to tolerate permanent and transient failures of nodes and links, especially since such failures are frequent in sensor networks. Protocols that chose the forwarding  node at each hop of a packet are resilient to such failures, but incur the delay caused by selection of the forwarding node at each hop of the multi-hop path. This paper presents a novel wireless sensor routing protocol, self-selecting reliable path routing (SRP) for wireless sensor network (WSN) routing, that addresses both challenges at once. This protocol evolved from the self-selecting routing (SSR) protocol which is essentially memory-less. In the first generation of SSR protocols each packet selects the forwarding node at each hop on its path from the source to destination. The protocol takes advantage of broadcast communication commonly used in WSNs as a communication primitive. It also uses a prioritized transmission back-off delay to uniquely identify the neighbor of the forwarder that will forward the packet. As a result, the protocol is resistant to node or link failures as long as an alternative path exists from the current forwarder to the destination. The second generation of SSR protocols, called self-healing routing (SHR) added the route repair procedure, invoked when no neighbor of the forwarder closer to the destination is alive. In a series of transmissions, a packet trapped at the current forwarder by failures of its neighbors is capable of backing-off towards the source to find an alternative route, if such exists, to the destination. The main contribution of this paper is the third generation of SSR protocols, termed selfselecting reliable path routing, SRP. It preserves SHRs dynamic path selection in face of failure. Yet it also enables packets to follow established paths without selection delay if failures do not occur. The important change in the protocol is to make it memorize the successfully traversed path and attempt to reuse it for subsequent packets flowing to the same destination. The interesting behavior of SRP arising from this property is that if a path from the source to destination exists on which no transient failures occur, SRP would converge its routing to such a reliable path. In the paper, we describe novel elements of the SRP protocol that resulted in the desired properties. Using simulation, we compare SRP protocol with the representatives of the two other approaches: AODV as the route-based protocol, and GRAB and SHR as the hop-selection protocols.' as abstract.

there is a document named 'doc-1391a' that
  has 'Feature subset selection is an important preprocessing step for pattern recognition, to discard irrelevant and redundant information, as well as to identify the most important attributes. In this paper, we investigate a computationally efficient solution to select the most important features for gait recognition. The specific technique applied is based on mutual information (MI), which evaluates the statistical dependence between two random variables and has an established relation with the Bayes classification error. Extending our earlier research, we show that a sequential selection method based on MI can provide an effective solution for high-dimensional human gait data. To assess the performance of the approach, experiments are carried out based on a 73-dimensional model-based gait features set and on a 64 by 64 pixels model-free gait symmetry map on the Southampton HiD Gait database. The experimental results confirm the effectiveness of the method, removing about 50% of the model-based features and 95% of the symmetry map\'s pixels without significant loss in recognition capability, which outperforms correlation and analysis-of-variance-based methods.' as abstract.

there is a document named 'doc-1392b' that
  has 'Abductive reasoning is a well established field of Artificial Intelligence widely applied to different problem domains not least cognitive robotics and planning. It has been used to abduce high-level descriptions of the world from robot sense data, using rules that tell us what sense data would be generated by certain objects and events of the robots world, subject to certain constraints on their co-occurrence. It has also been used to abduce actions that might result in a desired goal state of the world, using descriptions of the normal effects of these actions, subject to constraints on the action combinations. We can generalise these applications to a multi-agent context. Several robots can collaboratively try to abduce an agreed higher-level description of the state of the world from their separate sense data consistent with their collective constraints on the abduced description. Similarly, multi-agent planning can be accomplished by the abduction of the actions of a collective plan where each agent uses its own description of the effect of its actions within the plan, such that the constraints on the actions of all the participating agents are satisfied. To address this class of problems, we need to generalise the single agent abductive reasoning algorithm to a distributed abductive inference algortihm. In addition, if we want to investigate applications in which the set of collaborating robots/agents is open, we need an algorithm that allows agents to join or leave the collaborating group whilst a particular inference is under way, but which still produces sound abductive inferences. This paper describes such a distributed abductive reasoning system, which we call DARE, and its implementation in the multi-threaded Qu-Prolog variant of Prolog. We prove the soundness of the algorithm it uses and we discuss its completeness in relation to non-distributed abductive reasoning. We illustrate the use of the algorithm with a multi-agent meeting scheduling example. The task is open in that the actual agents who need to attend is not determined in advance. Each individual agent has its own constraints on the possible meeting time and concerning which other agents must or must attend the meeting, if it attends. The algorithm selects the agents to attend and ensures that the constraints of each of the attending agents are satisfied.' as abstract.

there is a document named 'doc-1394' that
  has 'The masquerade attack, where an attacker takes on the identity of a legitimate user to maliciously utilize that user\'s privileges, poses a serious threat to the security of information systems. Such attacks completely undermine traditional security mechanisms due to the trust imparted to user accounts once they have been authenticated. Many attempts have been made at detecting these attacks, yet achieving high levels of accuracy remains an open challenge. In this paper, we discuss the use of a specially tuned sequence alignment algorithm, typically used in bioinformatics, to detect instances of masquerading in sequences of computer audit data. By using the alignment algorithm to align sequences of monitored audit data with sequences known to have been produced by the user, the alignment algorithm can discover areas of similarity and derive a metric that indicates the presence or absence of masquerade attacks. Additionally, we present several scoring systems, methods for accommodating variations in user behavior, and heuristics for decreasing the computational requirements of the algorithm. Our technique is evaluated against the standard masquerade detection dataset provided by Schonlau et al. [14, 13], and the results show that the use of the sequence alignment technique provides, to our knowledge, the best results of all masquerade detection techniques to date.' as abstract.

there is a document named 'doc-1395a' that
  has 'This article advocates a new computing paradigm, called computing with time, that is capable of efficiently performing a certain class of computation, namely, searching in parallel for the closest value to the given parameter. It shares some features with the idea of computing with action potentials proposed by Hopfield, which originated in the field of artificial neuron network. The basic idea of computing with time is captured in a novel distributed algorithm based on broadcast communication called the Lecture Hall Algorithm, which can compute the minimum among n positive numbers using only O(1) messages. When applied to wireless communication network, the Lecture Hall Algorithm leads to an interesting routing protocol having several desirable features that are not acquired by intentional design.' as abstract.

there is a document named 'doc-1397' that
  has 'We investigate a prototypical agent-based model, the Naming Game, on two-dimensional random geometric networks. The Naming Game [A. Baronchelli et al., J. Stat. Mech.: Theory Exp. (2006) P06014.] is a minimal model, employing local communications that captures the emergence of shared communication schemes (languages) in a population of autonomous semiotic agents. Implementing the Naming Games with local broadcasts on random geometric graphs, serves as a model for agreement dynamics in large-scale, autonomously operating wireless sensor networks. Further, it captures essential features of the scaling properties of the agreement process for spatiallyembedded autonomous agents. Among the relevant observables capturing the temporal properties of the agreement process, we investigate the cluster-size distribution and the distribution of the agreement times, both exhibiting dynamic scaling. We also present results for the case when a small density of long-range communication links are added on top of the random geometric graph, resulting in a "small-world"-like network and yielding a significantly reduced time to reach global agreement. We construct a finite-size scaling analysis for the agreement times in this case.' as abstract.

there is a document named 'doc-1398a' that
  has 'The explosive growth of blogs, wikis, social networking sites, and other online communities has transformed the Web in recent years. The mainstream media has taken notice of the so-called Web 2.0 revolution-stories abound about events such as Facebook\'s huge valuation and trends like the growing HuluYouTube rivalry and Flickr\'s role in the current digital camera sales boom. However, a new set of technologies is emerging in the background, and even the Web 2.0 crowd is starting to take notice.' as abstract.

there is a document named 'doc-1400' that
  has 'In May 2006, the US Army Research Laboratory and UK Ministry of Defense created the International Technology Alliance (www.usukita.org). The consortium of 26 partners-including the ARL and MoD-offers an open research environment in which leading US and UK companies and universities can collaborate (see table 1). It will also fuse the best aspects of the US Army\'s Collaborative Technology Alliances and UK MoD\'s Defense Technology Centers on an international scale. The ITA aims to develop flexible, distributed, and secure decision-making procedures to improve networked coalition operations. Network science is a young discipline-we have limited information models and network theories to describe the behavior and scaling of large, complex mobile ad hoc networks.1 Moreover, you can\'t understand a coalition network\'s performance without understanding its cognitive and sociocultural aspects and physical characteristics. A key ITA goal is to perform basic research in network-centric coalition decision making across four technical areas: network theory, security across a system of systems, sensor information processing and delivery, and distributed coalition planning and decision making.2 We focus on the last area because this is where intelligent systems will play the biggest role.' as abstract.

there is a document named 'doc-1401' that
  has 'We consider a wireless network in which packets are forwarded opportunistically from the source towards the destination, without accurate knowledge of the direction that they should take. A Brownian motion model that includes the effect of packet losses, and subsequent retransmission after a time-out, is used to compute the average travel time of the packet. The results indicate that the average travel time is always finite provided that a time-out is used, and that there is an element of randomness in the manner in which successive nodes are being chosen. We show that the average packet travel time can be minimized by a judicious choice of the time-out, and its optimum value in turn depends on other system parameters such as packet-loss probabilities. We present simulations that illustrate the analytical results.' as abstract.

there is a document named 'doc-1403' that
  has 'To address the problem of unsupervised outlier detection in wireless sensor networks, we develop an algorithm that (1) is flexible with respect to the outlier definition, (2) works in-network with a communication load proportional to the outcome, and (3) reveals its outcome to all sensors. We examine the algorithm\'s performance using simulation with real sensor data streams. Our results demonstrate that the algorithm is accurate and imposes a reasonable communication load and level of power consumption.' as abstract.

there is a document named 'doc-1531' that
  has 'Cooperative diversity has been recognized as an effective and low-cost technique to combat multipath fading and enhance transmission reliability. Motivated by the fact that many existing cooperative protocols suffer some loss of ergodic capacity, network coding, a technique well known for its capability to increase system throughput, is proposed in this paper to be combined with cooperative diversity for uplink transmissions. Two kinds of information-theoretic metrics, outage and ergodic capacities, are developed for the proposed transmission scheme to assist performance evaluation. The developed analytical results, shown to be fit well with MonteCarlo simulation, demonstrate that the proposed protocol can achieve better system robustness and larger system throughput simultaneously than comparable schemes. The application of the proposed transmission protocol to scheduling is also studied to achieve better fairness-throughput tradeoff.' as abstract.

there is a document named 'doc-1578e' that
  has 'In this paper, we study network coding in the context of wireless communications. A new form of network coding is developed without the assumptions of precise phase synchronization and high transmission power. Wireless diversity can be efficiently utilized by applying a distributed strategy of relay selection to network coding. To facilitate performance evaluation, two information-theoretic metrics, the outage and ergodic capacity, are studied. Our analytical and simulation results show that the proposed protocol can achieve more robust performance and higher system throughput than the comparable schemes. Furthermore, the proposed network coding is extended to the context of cooperative multiple access channels, which yields a new cooperative protocol with larger outage and ergodic capacity compared with the existing transmission schemes.' as abstract.

there is a document named 'doc-1682' that
  has 'In this paper, we investigate how commonly used in-network aggregation approaches impact the target tracking quality in multi-hop wireless sensor networks. Specifically, we use the expected mean squared error (MSE) of the target location estimate to quantify the target tracking quality, and investigate how in-network aggregation affects the expected MSE. To obtain insights without being obscured by onerous mathematical details, we assume a Brownian motion mobility model for the target, Gaussian measurement noise for the sensors, and independent per-hop delays. Under the above assumptions, we first propose an aggregation scheme that preserves a sufficient statistic for making an optimal estimate from the sensor measurements even with data aggregation at the intermediate nodes and substantial transmission delays. We then analytically study the impact of aggregation in three increasingly more complicated scenarios that have single or multiple tracking tasks and with or not without queuing delay at intermediate sensor nodes. Our results demonstrate that aggregation improves tracking quality in all three scenarios. Furthermore, our analysis provides guidelines on how to choose parameters when using aggregation for target tracking in practice.' as abstract.

there is a document named 'doc-1836' that
  has 'How do I choose whom to delegate a task to? This is an important question for an autonomous agent collaborating with others to solve a problem. Were similar proposals accepted from similar agents in similar circumstances? What arguments were most convincing? What are the costs incurred in putting certain arguments forward? Can I exploit domain knowledge to improve the outcome of delegation decisions? In this paper, we present an agent decision-making mechanism where models of other agents are refined through evidence from past dialogues and domain knowledge, and where these models are used to guide future delegation decisions. Our approach combines ontological reasoning, argumentation and machine learning in a novel way, which exploits decision theory for guiding argumentation strategies. Using our approach, intelligent agents can autonomously reason about the restrictions (e.g., policies/norms) that others are operating with, and make informed decisions about whom to delegate a task to. In a set of experiments, we demonstrate the utility of this novel combination of techniques. Our empirical evaluation shows that decision-theory, machine learning and ontology reasoning techniques can significantly improve dialogical outcomes.' as abstract.

there is a document named 'doc-1840' that
  has 'In a distributed system, the actions of one component may lead to severe failures in the system as a whole. To govern such systems, constraints are placed on the behaviour of components to avoid such undesirable actions. Policies or norms are declarations of soft constraints regulating what is prohibited, permitted or obliged within a distributed system. These constraints provide systems-level means to mitigate against failures. A few machine-processable representations for policies have been proposed, but they tend to be either limited in the types of policies that can be expressed or are limited by the complexity of associated reasoning mechanisms. In this paper, we present a language that suffi- ciently expresses the types of policies essential in practical systems, and which enables both policy-governed decision-making and policy analysis within the bounds of decidability. We then propose an OWL-based representation of policies that meets these criteria and reasoning mechanisms that use a novel combination of ontology consistency checking and query answering. The proposed policy representation and reasoning mechanisms allow development of distributed agent-based systems that operate flexibly and effectively in policy-constrainted environments.' as abstract.

there is a document named 'doc-1873' that
  has 'Information theoretic security has recently emerged as an effective physical layer approach to provide secure communications. The outage performance of such a secrecy communication system is considered in this paper, since it is an important criterion to measure whether users\' predefined quality of service can be met. Provided that the legitimate receiver and eavesdropper have the same noise power, many existing secure schemes cannot achieve outage probability approaching zero, regardless of how large the transmission power is. By introducing cooperative transmission into secrecy communication systems, it will be shown here that outage probability approaching zero can be achieved. In particular, scenarios with single-antenna nodes and multiple-antenna nodes will both be addressed, and the optimal design of beamforming/precoding will be investigated. Explicit expressions of the achievable outage probability and diversity-multiplexing tradeoff will be developed to demonstrate the performance of the proposed cooperative secure transmission schemes, and numerical results are presented.' as abstract.

there is a document named 'doc-1880' that
  has 'The rapid growth and penetration of the Web raises important questions about its effects, not just on our social activities, but also on the nature of our cognitive and epistemic profiles. The extended mind hypothesis may be particularly well-suited to addressing these questions because it encourages us to think about the way in which much of our cognitive success is grounded in processing loops that factor in the contributions of our extra-neural social and technological environments. When applied to the specific socio-technical context of the Web, the extended mind hypothesis gives us the notion of the \'Web-extended mind\', or the idea that the technological and informational elements of the Web can (at least sometimes) serve as part of the mechanistic substrate that realizes human mental states and processes. This paper attempts to explore the notion of the Web-extended mind. It first provides an overview of cognitive extension and the extended mind hypothesis, and it then goes on to discuss the possibility of Web-based forms of cognitive extension involving current or near-future technologies. It is argued that while current forms of the Web may not be particularly suited to the realization of Web-extended minds, new forms of user interaction technology as well as new approaches to information representation on the Web do provide promising new opportunities for Web-based forms of cognitive extension. Extended minds, however, are not solely the product of technological innovation. Cognitivelyempowering forms of bio-technological union sometimes rely on the emergence of social practices and conventions that shape how a technology is used, as well as the specific (bio-)cognitive mechanisms that are available to support its effective exploitation. In particular, it is suggested that Web-extended minds may depend on forms of socio-technical co-evolution in which social forces and factors play just as an important role as do the processes of technology design and development.' as abstract.

there is a document named 'doc-1890' that
  has 'The increasing use of sensor-derived information from planned, ad-hoc, and/or opportunistically deployed sensor networks provides enhanced visibility to everyday activities and processes that enables fast-paced data-to-decision in personal, social, civilian, military, and business contexts. The value that information brings to this visibility and ensuing decisions depends on the quality characteristics of the information gathered. In this paper, we highlight, refine, and extend upon our past work in the areas of quality and value of information (QoI and VoI) for sensor networks. Specifically, we present and elaborate on our two-layer QoI/VoI definition, where the former relates to context-independent aspects and the latter to context-dependent aspects of an information product. Then, we refine our taxonomy of pertinent QoI and VoI attributes anchored around a simple ontological relationship between the two. Finally, we introduce a framework for scoring and ranking information products based on their VoI attributes using the analytic hierarchy multi-criteria decision process, illustrated via a simple example.' as abstract.

there is a document named 'doc-1900' that
  has 'This paper explores the idea of knowledge-based security policies, which are used to decide whether to answer queries over secret data based on an estimation of the querier\'s (possibly increased) knowledge given the results. Limiting knowledge is the goal of existing information release policies that employ mechanisms such as noising, anonymization, and redaction. Knowledge-based policies are more general: they increase flexibility by not fixing the means to restrict information flow. We enforce a knowledge-based policy by explicitly tracking a model of a querier\'s belief about secret data, represented as a probability distribution, and denying any query that could increase knowledge above a given threshold. We implement query analysis and belief tracking via abstract interpretation using a novel probabilistic polyhedral domain, whose design permits trading off precision with performance while ensuring estimates of a querier\'s knowledge are sound. Experiments with our implementation show that several useful queries can be handled efficiently, and performance scales far better than would more standard implementations of probabilistic computation based on sampling.' as abstract.

there is a document named 'doc-1908' that
  has 'To facilitate extensive collaborations, today\'s organizations raise increasing needs for information sharing via on-demand information access. Information Brokering System (IBS) atop a peerto-peer overlay has been proposed to support information sharing among loosely federated data sources. It consists of diverse data servers and brokering components, which help client queries to locate the data servers. However, many existing IBSs adopt server side access control deployment and honest assumptions on brokers, and shed little attention on privacy of data and metadata stored and exchanged within the IBS. In this paper, we study the problem of privacy protection in information brokering process. We first give a formal presentation of the threat models with a focus on two attacks: attribute-correlation attack and inference attack. Then, we propose a broker-coordinator overlay, aa well as two schemes, automaton segmentation scheme and query segment encryption scheme, to share the secure query routing function among a set of brokering servers. With comprehensive analysis on privacy, end-to-end performance, and scalability, we show that the proposed system can integrate security enforcement and query routing while preserving system-wide privacy with reasonable overhead.' as abstract.

there is a document named 'doc-1910' that
  has 'This paper presents a taxonomy and overview of approaches to the measurement 2 of graph and network complexity. The taxonomy distinguishes between deterministic (e.g., 3 Kolmogorov complexity) and probabilistic approaches with a view to placing entropy-based 4 probabilistic measurement in context. Entropy-based measurement is the main focus of the 5 paper. Relationships between the different entropy functions used to measure complexity 6 are examined; and intrinsic (e.g., classical measures) and extrinsic (e.g., Korner entropy)  7 variants of entropy-based models are discussed in some detail.' as abstract.

there is a document named 'doc-1923' that
  has 'Managing the Quality-of-Experience (QoE) of video streaming for wireless clients is becoming increasingly important due to the rapid growth of video traffic on wireless networks. The inherent variability of the wireless channel as well as the Variable Bit Rate (VBR) of the compressed video streams make QoE management a challenging problem. Prior work has studied this problem in the context of transmitting a single video stream. In this paper, we investigate multiplexing schemes to transmit multiple video streams from a base station to mobile clients that use number of playout stalls as a performance metric. In this context, we present an epoch-by-epoch framework to fairly allocate wireless transmission slots to streaming videos. In each epoch our scheme essentially reduces the vulnerability to stalling by allocating slots to videos in a way that maximizes the minimum \'playout lead\' across all videos. Next, we show that the problem of allocating slots fairly is NP-complete even for a constant number of videos. We then present a fast lead-aware greedy algorithm for the problem. Our choice of greedy algorithm is motivated by the fact that this algorithm is optimal when the channel quality of a user remains unchanged within an epoch (but different users may experience different channel quality). Moreover, our experimental results based on public MPEG-4 video traces and wireless channel traces that we collected from a WiMAX test-bed show that the lead-aware greedy approach performs a fair distribution of stalls across the clients when compared to other algorithms, while still maintaining similar or lower average number of stalls per client.' as abstract.

there is a document named 'doc-1996' that
  has 'Location-based routing protocols are stateless since they rely on position information in forwarding decisions. However, their efficiency depends on performance of location services which provide the position information of the desired destination node. Several location service schemes have been proposed, but the most promising among them, hierarchical hashing-based protocols, rely on intuitive design in the published solutions. In this paper, we provide full analysis of the efficiency of routing in hierarchical hashingbased protocols as a function of the placement of the routers. Based on the theoretical analysis of the gain and costs of the query and reply routing, we propose a novel location service protocol that optimizes the distance traveled by the location update and query packets and, thus, reduces the overall energy cost. These gains are further increased in the second presented protocol by the optimal location of servers that we established through analysis of geometrical relationships between nodes and location servers. Simulation results demonstrate that the proposed protocols achieve around 30%-35% energy efficiency while improving or maintaining the query success rate in comparison to the previously proposed algorithms.' as abstract.

there is a document named 'doc-2001' that
  has 'Sensemaking is a key capability for military coalitions, enabling both individuals and teams to make sense of conflicting, ambiguous and uncertain information. Many features of the military coalition environment are likely to exert an effect on sensemaking capabilities; however, at the present time, we have little understanding of the precise nature of these relationships. Computational modelling provides one means of improving our understanding in this area. By developing models that enable us to explore the effect of factors such as network topology, cognitive heterogeneity, miscommunication and interagent trust on the dynamics of both individual and collective sensemaking, we may be able to better understand how specific features of the coalition communication environment affect the dynamics of collective sensemaking. The current paper reviews work in the US/UK International technology Alliance that aims to develop computational models of collective sensemaking in military coalition contexts.' as abstract.

there is a document named 'doc-2006' that
  has 'A key problem in managing intelligence, surveillance and reconnaissance (ISR) operations in a coalition context is assigning available sensing assets - of which there are increasingly many - to mission tasks. High demands for information and relative scarcity of available assets implies that assignments must be made taking into account all possible ways of achieving an ISR task by different kinds of sensing. Moreover, the dynamic nature of most ISR situations means that asset assignment must be done in a highly agile manner. The problem is exacerbated in a coalition context because it is harder for users to have an overview of all suitable assets across multiple coalition partners. In this paper, we describe a knowledge-driven approach to ISR asset assignment using ontologies, allocation algorithms, and a service-oriented architecture. An illustration of the use of the system from a mobile device is presented.' as abstract.

there is a document named 'doc-2029' that
  has 'Routing in delay tolerant networks is a challenging problem due to the intermittent connectivity between nodes resulting in the frequent absence of end-to-end path for any source-destination pair at any given time. Recently, this problem has attracted a great deal of interest and several approaches have been proposed. Since mobile social networks (MSN) are increasingly popular type of DTNs, making accurate analysis of social network properties of these networks is essential for designing efficient routing protocols. In this paper, we introduce a new metric that detects the quality of friendships between nodes accurately. Then, utilizing this metric, each node defines its community as the set of nodes having close friendship relations with itself either directly or indirectly. Then, we present Friendship Based Routing in which periodically differentiated friendship relations are used in forwarding of messages. Extensive simulations on both real and synthetic traces show that the introduced algorithm is more efficient than the existing algorithms.' as abstract.

there is a document named 'doc-2030' that
  has 'This paper describes the use of market mechanisms for resource allocation in pervasive sensor applications to maximize their Value of Information (VoI), which combines the objectively measured Quality of Information (QoI) with the subjective value assigned to it by the users. The unique challenge of pervasive sensor applications that we address is the need for adjusting resource allocation in response to the changing application requirements and evolving sensor network conditions. We use two market mechanisms: auctions at individual sensor nodes to optimize routing, and switch options to optimize dynamic selection of sensor network services as well as switching between modes of operation in pervasive security applications. We also present scenarios of transient congestion management and home security system to motivate the proposed techniques.' as abstract.

there is a document named 'doc-2050' that
  has 'This work presents a study of the complexity of the Blum-Kalai-Wasserman (BKW) algorithm when applied to the Learning with Errors (LWE) problem, by providing refined estimates for the data and computational effort requirements for solving concrete instances of the LWE problem. We apply this refined analysis to suggested parameters for various LWE-based cryptographic schemes from the literature and compare with alternative approaches based on lattice reduction. As a result, we provide new upper bounds for the concrete hardness of these LWE-based schemes. Rather surprisingly, it appears that BKW algorithm outperforms known estimates for lattice reduction algorithms starting in dimension n  250 when LWE is reduced to SIS. However, this assumes access to an unbounded number of LWE samples.' as abstract.

there is a document named 'doc-2058' that
  has 'Neighbor discovery is an important first step in the initialization of a wireless ad hoc network. In this paper, we design and analyze several algorithms for neighbor discovery in wireless networks. Starting with a single-hop wireless network of nodes, we propose a ALOHA-like neighbor discovery algorithm when nodes cannot detect collisions, and an order-optimal receiver feedback-based algorithm when nodes can detect collisions. Our algorithms neither require nodes to have a priori estimates of the number of neighbors nor synchronization between nodes. Our algorithms allow nodes to begin execution at different time instants and to terminate neighbor discovery upon discovering all their neighbors. We finally show that receiver feedback can be used to achieve a running time, even when nodes cannot detect collisions. We then analyze neighbor discovery in a general multihop setting. We establish an upper bound of on the running time of the ALOHA-like algorithm, where denotes the maximum node degree in the network and the total number of nodes. We also establish a lower bound of on the running time of any randomized neighbor discovery algorithm. Our result thus implies that the ALOHA-like algorithm is at most a factor worse than optimal.' as abstract.

there is a document named 'doc-2126' that
  has 'Planning is a core military activity that is undertaken across multiple (human) collaborative agents. In order to collaborate, these humans need to form a shared understanding of various aspects of the plan, mutual goals, the contexts of the other agents, and the rationale for others\' decisions and assumptions. Failure to reach this shared understanding can have serious implications to the success of the resulting plan. This paper describes a framework for representing planning concepts, called the Collaborative Planning Model (CPM), which was designed to support human planners in the management of planning information and to facilitate automated reasoning. The goal was to make plans "alive" by digitizing planning concepts in order to facilitate their dynamic use, modification, dissemination, and re-use; in contrast to the current static documented plans. This framework, however, also provides an effective mechanism for sharing information between different tools and has the potential to improve coalition planning.' as abstract.

there is a document named 'doc-2139' that
  has 'To facilitate extensive collaborations, today\'s organizations raise increasing needs for information sharing via on-demand information access. Information Brokering System (IBS) atop a peer-to-peer overlay has been proposed to support information sharing among loosely federated data sources. It consists of diverse data servers and brokering components, which help client queries to locate the data servers. However, many existing IBSs adopt server side access control deployment and honest assumptions on brokers, and shed little attention on privacy of data and metadata stored and exchanged within the IBS. In this article, we study the problem of privacy protection in information brokering process. We first give a formal presentation of the threat models with a focus on two attacks: attribute-correlation attack and inference attack. Then, we propose a broker-coordinator overlay, as well as two schemes, automaton segmentation scheme and query segment encryption scheme, to share the secure query routing function among a set of brokering servers. With comprehensive analysis on privacy, end-to-end performance, and scalability, we show that the proposed system can integrate security enforcement and query routing while preserving system-wide privacy with reasonable overhead.' as abstract.

there is a document named 'doc-2170' that
  has 'In this paper, we present Ontological Logic Programming (OLP), a novel approach that combines logic programming with ontological reasoning. OLP enables the use of ontological terms (i.e., individuals, classes and properties) directly within logic programmes. The interpretation of these terms is delegated to an ontology reasoner during the interpretation of the programme. Unlike similar approaches, OLP makes use of the full capacity of both ontological reasoning and logic programming. We evaluate the computational properties of OLP in different settings and show that its performance can be significantly improved using caching mechanisms. We then introduce a comprehensive sensor-task selection solution based on OLP and discuss the benefits one can obtain by using OLP. The solution is based on a set of interlinking ontologies that capture the crucial domain knowledge of sensor networks. We then make use of OLP to create and manage complex concepts in the domain as well as to implement effective resource-task assignment algorithms, which compute appropriate resources for tasks such that they sufficiently cover the tasks needs. We compare the advantages of OLP with a knowledge-based set-covering mechanism for resource-task selection.' as abstract.

there is a document named 'doc-2176' that
  has 'ABox abduction is an important reasoning facility in Description Logics (DLs). It finds all minimal sets of ABox axioms, called abductive solutions, which should be added to a background ontology to enforce entailment of an observation which is a specified set of ABox axioms. However, ABox abduction is far from practical by now because there lack feasible methods working in finite time for expressive DLs. To pave a way to practical ABox abduction, this paper proposes a new problem for ABox abduction and a new method for computing abductive solutions accordingly. The proposed problem guarantees finite number of abductive solutions. The proposed method works in finite time for a very expressive DL, SHOIQ , which underpins the W3C standard language OWL 2, and guarantees soundness and conditional completeness of computed results. Experimental results on benchmark ontologies show that the method is feasible and can scale to large ABoxes.' as abstract.

there is a document named 'doc-2181' that
  has 'Energy efficient secure communication in wireless networks in the presence of eavesdroppers is considered. For a secure transmission to the destination, a set of intermediate "jammer" nodes are chosen to generate artificial noise that confuses the eavesdropper. We consider two jamming strategies: beamforming and cooperative diversity. Previous research has focused largely on cooperative beamforming strategies, but we demonstrate a number of scenarios where a cooperative diversity strategy is desirable. This motivates approaches which selectively switch between the two strategies, from which significant energy savings can often be realized. In our simulations, energy savings of up to 40% are observed in the simulated networks.' as abstract.

there is a document named 'doc-2182' that
  has 'Security protocols operating over wireless channels can incur significant communication costs (e.g., energy, delay), especially under adversarial attacks unique to the wireless environment such as signal jamming, fake signal transmission, etc. Since wireless devices are resource constrained, it is important to optimize security protocols for wireless environments by taking into account their communication costs. Towards this goal, we first present a novel application of a signal-flow-based approach to analyze the communication costs of security protocols in the presence of adversaries. Our approach models a protocol run as a dynamic probabilistic system and then utilizes Linear System theory to evaluate the moment generating function of the end-to-end cost. Applying this technique to the problem of secret key exchange over a wireless channel, we quantify the efficiency of existing families of key exchange cryptographic protocols, showing, for example, that an ID-based approach can offer an almost 10-fold improvement in energy consumption when compared to a traditional PKI-based protocol. We then present a new key exchange protocol that combines traditional cryptographic methods with physical-layer techniques, including the use of "ephemeral" spreading codes, cooperative jamming, and role-switching. Utilizing signal flow analysis, we demonstrate that this new protocol offers performance advantages over traditional designs.' as abstract.

there is a document named 'doc-2183' that
  has 'The capability of nodes to broadcast their message to the entire wireless network when nodes employ cooperation is considered. We employ an asymptotic analysis using an extended random network setting under an additive white Gaussian channel model with path loss, and show that the broadcast performance strongly depends on the path loss exponent of the medium. In particular, the probability of broadcast in a onedimensional infinite network is zero for path loss exponents larger than one, and is equal to a nonzero value for path loss exponents less than one. In two-dimensional infinite networks, the same behavior is observed for path loss exponents above and below two, respectively.' as abstract.

there is a document named 'doc-2242' that
  has 'Human users planning for multiple objectives in complex environments are subjected to high levels of cognitive workload, which can severely impair the quality of the plans created. This article describes a software agent that can proactively assist cognitively overloaded users by providing normative reasoning about prohibitions and obligations so that the user can focus on her primary objectives. In order to provide proactive assistance, we develop the notion of prognostic normative reasoning (PNR) that consists of the following steps: 1) recognizing the user\'s planned activities, 2) reasoning about norms to evaluate those predicted activities, and 3) providing necessary assistance so that the user\'s activities are consistent with norms. The idea of PNR integrates various AI techniques-namely, user intention recognition, normative reasoning over a user\'s intention, and planning, execution and replanning for assistive actions. In this article, we describe an agent architecture for PNR and discuss practical applications.' as abstract.

there is a document named 'doc-2248' that
  has 'In the standard setting of broadcast encryption, information about the receivers is transmitted as part of the ciphertext. In several broadcast scenarios, however, the identities of the users authorized to access the content are often as sensitive as the content itself. In this paper, we propose the first broadcast encryption scheme with sublinear ciphertexts to attain meaningful guarantees of receiver anonymity. We formalize the notion of outsider-anonymous broadcast encryption (oABE), and describe generic constructions in the standard model that achieve outsider-anonymity under adaptive corruptions in the chosen-plaintext and chosen-ciphertext settings. We also describe two constructions with enhanced decryption, one under the gap Diffie-Hellman assumption, in the random oracle model, and the other under the decisional Diffie-Hellman assumption, in the standard model.' as abstract.

there is a document named 'doc-2249' that
  has 'We present a square root limit on the information rate with low probability of detection (LPD) over additive white Gaussian noise (AWGN) channels. Specifically, if the transmitter has AWGN channels to a receiver and a warden, both with non-zero noise power, we prove that o( _ n) bits can be sent from the transmitter to the receiver in n channel uses while lower-bounding the warden\'s probability of detection error by 1 _  for any  > 0. Moreover, in most practical scenarios, a lower bound on the noise power on the warden\'s channel to the transmitter is known and O( _ n) bits can be covertly sent in n channel uses. Conversely, attempting to transmit more than O( _ n) bits either results in detection by the warden with probability one or a non-zero probability of decoding error as n _ _.' as abstract.

there is a document named 'doc-2284' that
  has 'This letter establishes the asymptotic optimality of equal power allocation for measurements of a continuous wide-sense stationary (WSS) random process with a squareintegrable autocorrelation function when linear estimation is used on equally-spaced measurements with periodicity meeting the Nyquist criterion and with the variance of the noise on any sample inversely proportional to the power expended by the user to obtain that measurement.' as abstract.

there is a document named 'doc-2295' that
  has 'In this paper, we evaluate the discrimination power of structural superindices. Superindices for graphs represent measures composed of other structural indices. In particular, we compare the discrimination power of the superindices with those of individual graph descriptors. In addition, we perform a statistical analysis to generalize our findings to large graphs.' as abstract.

there is a document named 'doc-2325' that
  has 'Overlay network topology together with peer/data organization and search algorithm are the crucial components of unstructured peer-to-peer (P2P) networks as they directly affect the efficiency of search on such networks. Scale-free (powerlaw) overlay network topologies are among structures that offer high performance for these networks. A key problem for these topologies is the existence of hubs, nodes with high connectivity. Yet, the peers in a typical unstructured P2P network may not be willing or able to cope with such high connectivity and its associated load. Therefore, some hard cutoffs are often imposed on the number of edges that each peer can have, restricting feasible overlays to limited or truncated scale-free networks. In this paper, we analyze the growth of such limited scale-free networks and propose two different algorithms for constructing perfect scale-free overlay network topologies at each instance of such growth. Our algorithms allow the user to define the desired scalefree exponent (_). They also induce low communication overhead when network grows from one size to another. Using extensive simulations, we demonstrate that these algorithms indeed generate perfect scale free networks (at each step of network growth) that provide better search efficiency in various search algorithms than the networks generated by the existing solutions.' as abstract.

there is a document named 'doc-2357' that
  has 'The search for an easily computable, finite, complete set of graph invariants remains a challenging research topic. All measures characterizing the topology of a graph that have been developed thus far exhibit some degree of degeneracy, i.e., an inability to distinguish between non-isomorphic graphs. In this paper, we show that certain graph invariants can be useful in substantially reducing the computational complexity of isomorphism testing. Our findings are underpinned by numerical results based on a large scale statistical analysis.' as abstract.

there is a document named 'doc-2358' that
  has 'We have argued that an information-centric network should natively support publish/subscribe event notification in addition to on-demand content delivery. We have also argued that both primitives could use the same forwarding information base and, furthermore, that both primitives can easily support addresses that are more expressive than simple hierarchical names. In this paper we present a concrete routing scheme that realizes this: "push" as well as "pull" communication; anycast as well as multicast; and descriptor-based (as opposed to name-based) addressing. The scheme is founded on multiple tree covers that can be arranged and composed hierarchically following the structure of network domains. On each tree, the scheme combines addresses so as to reduce forwarding state. We demonstrate the feasibility and scalability of the scheme through simulations on Internetscale workloads in realistic network settings.' as abstract.

there is a document named 'doc-2367' that
  has 'There is an increasing interest in using logic programming to specify and implement distributed algorithms, including a variety of network applications. These are applications where data and computation are distributed among several devices and where, in principle, all the devices can exchange data and share the computational results of the group. In this paper we propose a declarative approach to distributed computing whereby distributed algorithms and communication models can be (i) specified as action theories of fluents and actions; (ii) executed as collections of distributed state machines, where the devices are abstracted as (input/output) automata that can exchange messages; and (iii) analysed using existing results on connecting causal theories and Answer Set Programming (ASP). This work extends our initial results on the use of an A-type language for writing network applications, by showing that it is possible to achieve similar expressiveness and generality but using only two types of non-logical symbols (fluents and actions). Results on the application of our approach to different classes of network protocols are also presented.' as abstract.

there is a document named 'doc-2369' that
  has 'Various canonical forms of general resource allocation problems arise naturally across a broad spectrum of computer systems and communication networks. As the complexities of these systems and networks continue to grow, together with ubiquitous advances in technology, new approaches and methods are required to effectively and efficiently solve these problems. Such environments often consist of different types of resources that are allocated in combination to serve demand whose behavior over time is characterized by different types of uncertainty and variability. Each type of resource has a different reward and cost structure that ranges from the best of a set of primary resource allocation options, having the highest reward, highest cost and highest net-benefit, to a secondary resource allocation option, having the lowest reward, lowest cost and lowest net-benefit. Each type of resource also has different structures for the flexibility and cost of making changes to the allocation capacity. The resource management optimization problem we consider consists of adaptively determining the primary and secondary resource allocation capacities that serve the uncertain demand and that maximize the expected net-benefit over a time horizon of interest based on the foregoing reward, cost and flexibility structural properties of the different types of resources.' as abstract.

there is a document named 'doc-2370' that
  has 'We consider a class of general dynamic resource allocation problems within a stochastic optimal control framework. This class of problems arises in a wide variety of applications, each of which intrinsically involves resources of different types and demand with uncertainty and/or variability. The goal is to determine the allocation capacity for every resource type in order to serve the uncertain/variable demand and maximize the expected net-benefit over a time horizon of interest based on the rewards and costs associated with the different resources. We derive the optimal control policy within a singular control setting, which includes simple expressions for governing the dynamic adjustments to resource allocation capacities over time. Numerical experiments investigate various issues of both theoretical and practical interest, quantifying the significant benefits of our approach over alternative optimization approaches.' as abstract.

there is a document named 'doc-2496' that
  has 'The emergence of large scale, distributed, sensor-enabled, machine-to-machine pervasive applications necessitates engaging with providers of information on demand to collect the information, of varying quality levels, to be used to infer about the state of the world and decide actions in response. In these highly fluid operational environments, involving information providers and consumers of various degrees of trust and intentions, information transformation, such as obfuscation, is used to manage the inferences that could be made to protect providers from misuses of the information they share, while still providing benefits to their information consumers. In this paper, we develop the initial principles for relating to inference management and the role that trust and obfuscation plays in it within the context of this emerging breed of applications. We start by extending the definitions of trust and obfuscation into this emerging application space. We, then, highlight their role as we move from tightly-coupled to loosely-coupled sensory-inference systems and describe how quality, value and risk of information relate in collaborative and adversarial systems. Next we discuss quality distortion illustrated through a human activity recognition sensory system. We then present a system architecture for an inference firewall capability in a publish/subscribe system for sensory information and conclude with a discussion and closing remarks.' as abstract.

there is a document named 'doc-2502a' that
  has 'An authenticated data structure (ADS) is a data structure whose operations can be carried out by an untrusted prover, the results of which a verifier can efficiently check as authentic. This is done by having the prover produce a compact proof that the verifier can check along with each query result. ADSs thus support outsourcing data maintenance and processing tasks to untrusted servers without loss of integrity. Past work on ADSs has focused on particular data structures (or limited classes of data structures), one at a time, often with support only for particular operations. This paper presents a generic method, using a simple extension to a MLlike functional programming language we call __ (lambda-auth), with which one can program authenticated operations over any data structure constructed from standard type constructors, including recursive types, sums, and products. The programmer writes the data structure largely as usual; it can then be compiled to code to be run by the prover and verifier. Using a formalization of __ we prove that all well-typed __ programs result in code that is secure under the standard cryptographic assumption of collision-resistant hash functions. We have implemented our approach as an extension to the OCaml compiler, and have used it to produce authenticated versions of many interesting data structures including binary search trees, red-black trees, skip lists, and more. Performance experiments show that our approach is efficient, giving up little compared to the hand-optimized data structures developed previously.' as abstract.

there is a document named 'doc-2559' that
  has 'Computational trust mechanisms aim to produce trust ratings from both direct and indirect information about agents\' behaviour. Subjective Logic (SL) has been widely adopted as the core of such systems via its fusion and discount operators. In recent research we revisited the semantics of these operators to explore an alternative, geometric interpretation. In this paper we present principled desiderata for discounting and fusion operators in SL. Building upon this we present operators that satisfy these desirable properties, including a family of discount operators. We then show, through a rigorous empirical study, that specific, geometrically interpreted, operators significantly outperform standard SL operators in estimating ground truth. These novel operators offer real advantages for computational models of trust and reputation, in which they may be employed without modifying other aspects of an existing system.' as abstract.

there is a document named 'doc-2560a' that
  has 'Shared information can benefit an agent, allowing others to aid it in its goals. However, such information can also harm, for example when malicious agents are aware of these goals, and can then thereby subvert the goal-maker\'s plans. In this paper we describe a decision process framework allowing an agent to decide what information it should reveal to its neighbours within a communication graph in order to maximise its utility. We assume that these neighbours can pass information onto others within the graph. The inferences made by agents receiving the messages can have a positive or negative impact on the information providing agent, and our decision process seeks to assess how a message should be modified in order to be most beneficial to the information producer. Our decision process is based on the provider\'s subjective beliefs about others in the system, and therefore makes extensive use of the notion of trust with regards to the likelihood that a message will be passed on by the receiver, and the likelihood that an agent will use the information against the provider. Our core contributions are therefore the construction of a model of information propagation; the description of the agent\'s decision procedure; and an analysis of some of its properties.' as abstract.

there is a document named 'doc-2625' that
  has 'Large-scale adoption of MapReduce computations on public clouds is hindered by the lack of trust on the participating virtual machines, because misbehaving worker nodes can compromise the integrity of the computation result. In this paper, we propose a novel MapReduce framework, Cross Cloud MapReduce (CCMR), which overlays the MapReduce computation on top of a hybrid cloud: the master that is in control of the entire computation and guarantees result integrity runs on a private and trusted cloud, while normal workers run on a public cloud. In order to achieve high accuracy, we propose a result integrity check scheme on both the map phase and the reduce phase. On the other hand, we strive to reduce the performance overhead by reducing the cross-cloud communication and merging sub-tasks. We implement CCMR based on Apache Hadoop MapReduce and evaluate it on Amazon EC2. Both theoretical and experimental analysis show that our approach can guarantee high result integrity in a hybrid cloud environment while incurring nonnegligible performance overhead (e.g., when 16.7% workers are malicious, CCMR can guarantee at least 99.52% of accuracy with 33.6% of overhead when replication probability is 0.3 and the credit threshold is 50).' as abstract.

there is a document named 'doc-2630' that
  has 'In domains such as emergency response, environmental monitoring, policing and security, sensor and information networks are deployed to assist human users across multiple agencies to conduct missions at or near the "front line". These domains present challenging problems in terms of human-machine collaboration: human users need to task the network to help them achieve mission objectives, while humans (sometimes the same individuals) are also sources of mission-critical information. We propose a natural language-based conversational approach to supporting humanmachine working in mission-oriented sensor networks. We present a model for human-machine and machine-machine interactions in a realistic mission context, and evaluate the model using an existing surveillance mission scenario. The model supports the flow of conversations from full natural language to a form of Controlled Natural Language (CNL) amenable to machine processing and automated reasoning, including high-level information fusion tasks. We introduce a mechanism for presenting the gist of verbose CNL expressions in a more convenient form for human users. We show how the conversational interactions supported by the model include requests for expansions and explanations of machine-processed information.' as abstract.

there is a document named 'doc-2639' that
  has 'We propose a greedy algorithm, Cluster-MAX-COVERAGE (CMC), to efficiently diagnose large-scale clustered failures. We primarily address the challenge of determining faults with incomplete symptoms. CMC makes novel use of both positive and negative symptoms to output a hypothesis list with a low number of false negatives and false positives quickly. CMC requires reports from about half as many nodes as other existing algorithms to determine failures with 100% accuracy. Moreover, CMC accomplishes this gain significantly faster (sometimes by two orders of magnitude) than an algorithm that matches its accuracy. When there are fewer positive and negative symptoms at a reporting node, CMC performs much better than existing algorithms. We also propose an adaptive algorithm called Adaptive-MAX-COVERAGE (AMC) that performs efficiently during both independent and clustered failures. During a series of failures that include both independent and clustered, AMC results in a reduced number of false negatives and false positives.' as abstract.

there is a document named 'doc-2717' that
  has 'Today, most devices have multiple network interfaces. Coupled with wide-spread replication of popular content at multiple locations, this provides substantial path diversity in the Internet. We propose Multi-source Multipath HTTP, mHTTP, which takes advantage of all existing types of path diversity in the Internet. mHTTP needs only client-side but not server-side or network modifications as it is a receiver-oriented mechanism. Moreover, the modifications are restricted to the socket interface. Thus, no changes are needed to the applications or to the kernel. As mHTTP relies on HTTP range requests, it is specific to HTTP which accounts for more than 60% of the Internet traffic [20]. We implement mHTTP and study its performance by conducting measurements over a testbed and in the wild. Our results show that mHTTP indeed takes advantage of all types of path diversity in the Internet, and that it is a viable alternative to Multipath TCP1 for HTTP traffic. mHTTP decreases download times for large objects up to 50%, whereas it does no harm to small object downloads.' as abstract.

there is a document named 'doc-2723' that
  has 'The increasing interest of researchers in service oriented architecture (SOA) for wireless sensor networks (WSNs) is opening new unexplored venues in the field of WSNs. In service oriented systems, services are configured and composed of various other services and thus perform complex tasks. In such composite services, the geospatial locations of services and their coverage is of vital importance as they signify the geospatial relevance of the service to the area of interest to the user. In this paper, we present a service-oriented system for WSNs that is capable of performing service configuration under geospatial and relevancy constraints. We present and evaluate "Cost Based Model (CBM)" and "Gain Based Model (GBM)" approaches to capture the relevancy of services hosted on WSN nodes in composite service configuration. The system is resilient to failures and can operate in manual or autonomous recovery modes. The system supports three service configuration methods namely, distributed, centralized and hybrid. Furthermore, we present a novel emulation mechanism for testing the performance of our proposed relevancy models and show that our system efficiently configures services.' as abstract.

there is a document named 'doc-2764b' that
  has 'Humans or intelligent software agents are increasingly faced with the challenge of making decisions based on large volumes of streaming information from diverse sources. Decision makers must process the observed information by inferring additional information, estimating its reliability, and orienting it for decision making. In this paper, we propose a comprehensive framework where unstructured reports are streamed from heterogeneous and potentially untrustworthy information sources. These reports are processed to extract valuable uncertain information, which is represented using Controlled Natural Language and Subjective Logic. Additional information is inferred using deduction and abduction operations over subjective opinions derived from the reports. Before fusing extracted and inferred opinions, the framework estimates trustworthiness of these opinions, detects conflicts between them, and resolve these conflicts by analysing evidence about the reliability of their sources. Lastly, we describe an implementation of the framework using International Technology Alliance (ITA) assets (Information Fabric Services and Controlled English Fact Store) and present an experimental evaluation that quantifies the efficiency with respect to accuracy and overhead of the proposed framework.' as abstract.

there is a document named 'doc-2765b' that
  has 'Technology is improving day-by-day and so is the usage of mobile devices. Every activity that would involve manual and paper transactions can now be completed in seconds using your fingertips. On one hand, life has become fairly convenient with the help of mobile devices, whereas on the other hand security of the data and the transactions occurring in the process have been under continuous threat. In this paper, we re-evaluate the different policies and procedures used for preserving privacy including confidential data and location. Policy languages have been very vital in the mobile environments as they can be extended/ used significantly sending/receiving any data. In the mobile environment users always go to service providers to access various services. Hence, communications between the service providers and mobile handsets needs to be secured. Also, the data access control needs to be in place. A section of the paper will review the communication path/ channels and the access criteria. This paper is a contribution to the mobile domain, showing the possible attacks related to privacy and various mechanisms used to preserve end-user privacy. It gives a comparison of the different privacy preserving methods in mobile environments to provide guidance to the readers. This paper also identifies future research challenges in the area of privacy preservation. This paper examines the \'where\' problem and in particular, examines tradeoffs between enforcing location security at a device vs. enforcing location security at an edge location server. This paper also sketches an implementation of location security solutions at both the device and the edge location server and presents detailed experiments using real mobility and user profile data sets collected from various data sources (taxicabs, Smartphones).' as abstract.

there is a document named 'doc-2843' that
  has 'In a federated database system, each independent party exports some of its data for information sharing. The information sharing in such a system is very inflexible, as all peer parties access the same set of data exported by a party, while the party may want to authorize different peer parties to access different portions of its information. We propose a novel query evaluation scheme that supports differentiated access control with decentralized query processing. A new efficient join method, named split-join, along with other safe join methods is adopted in the query planning algorithm. The generated query execution reduces the communication cost by pushing partial query computation to data sources in a safe way. The proofs of the correctness and safety of the algorithm are presented. The evaluation demonstrates that the scheme significantly saves the communication cost in a variety of circumstances and settings while enforcing autonomous and differentiated information sharing effectively.' as abstract.

there is a document named 'doc-2914' that
  has 'Researchers have recently shown that declarative database query languages, such as Datalog, could naturally be used to specify and implement network protocols and services. In this paper we present a declarative framework for the specification, execution, simulation and analysis of distributed applications. Distributed applications, including routing protocols, can be specified using a Declarative Networking language, called D2C, whose semantics captures the notion of a Distributed State Machine (DSM), i.e. a network of computational nodes that communicate with each other through the exchange of data. The D2C specification can be directly executed using the DSM computational infrastructure of our framework. The same specification can be simulated and formally verified. The simulation component integrates the DSM tool within a network simulation environment and allows developers to simulate network dynamics and collect data about the execution in order to evaluate application responses to network changes. The formal analysis component of our framework, instead, complements the empirical testing by supporting the verification of different classes of properties of distributed algorithms, including convergence of network routing protocols. To demonstrate the generality of our framework, we show how it can be used to analyse two classes of network routing protocols, a path vector and a Mobile Ad-Hoc Network (MANET) routing protocol, and execute a distributed algorithm for pattern formation in multi-robot systems.' as abstract.

there is a document named 'doc-2916' that
  has 'Subjective Logic (SL) is a type of probabilistic logic, which is suitable for reasoning about situations with uncertainty and incomplete knowledge. In recent years, SL has drawn a significant amount of attention from the multi-agent systems community as it connects beliefs and uncertainty in propositions to a rigorous statistical characterization via Dirichlet distributions. However, one serious limitation of SL is that the belief updates are done only based on completely observable evidence. This work extends SL to incorporate belief updates from partially observable evidence. Normally, the belief updates in SL presume that the current evidence for a proposition points to only one of its mutually exclusive attribute states. Instead, this work considers that the current attribute state may not be completely observable, and instead, one is only able to obtain a measurement that is statistically related to this state. In other words, the SL belief is updated based upon the likelihood that one of the attributes was observed. The paper then illustrates properties of the partial observable updates as a function of the state likelihood and illustrates the use of these likelihoods for a trust estimation application. Finally, the utility of the partial observable updates is demonstrated via various simulations including the trust estimation case.' as abstract.

there is a document named 'doc-x0006' that
  has 'Mobile Micro-Clouds provide a promising approach to significantly improve tactical network operations by moving computation closer to the edge. A key challenge in such systems is to decide where and when services should be migrated in response to user mobility and demand variation. The objective is to optimize operational costs while providing rigorous performance guarantees. In this work, we model this as a sequential decision making Markov Decision Problem (MDP). However, departing from traditional solution methods (such as dynamic programming) that require extensive statistical knowledge and are computationally prohibitive, we develop a novel alternate methodology. First, we establish an interesting decoupling property of the MDP that reduces it to two independent MDPs on disjoint state spaces. Then, using the technique of Lyapunov optimization over renewals, we design an online control algorithm for the decoupled problem that is provably cost-optimal. This algorithm does not require any statistical knowledge of the system parameters and can be implemented efficiently. We validate the performance of our algorithm using extensive trace-driven simulations. Our overall approach is general and can be applied to other MDPs that possess a similar decoupling property.' as abstract.

there is a document named 'doc-x0009' that
  has 'We investigate the capability of localizing node failures in communication networks from binary states (normal/failed) of end-to-end paths. Given a set of nodes of interest, uniquely localizing failures within this set requires that different observable path states associate with different node failure events. However, this condition is difficult to test on large network s due to the need to enumerate all possible node failures. Our first contribution is a set of sufficient/necessary conditions for identifying a bounded number of failures within an arbitrar y node set that can be tested in polynomial time. In addition to network topology and locations of monitors, our conditions also incorporate constraints imposed by the probing mechanism used. We consider three probing mechanisms that differ according to whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cycle-free, or (iii) uncontrollable (determined by the default routing protocol). Our second contribution i s to quantify the capability of failure localization through (1) the maximum number of failures (anywhere in the network) such that failures within a given node set can be uniquely localized, and (2) the largest node set within which failure s can be uniquely localized under a given bound on the total number of failures. Both measures in (1-2) can be converted into functions of a per-node property, which can be computed efficiently based on the above sufficient/necessary conditions. We demonstrate how measures (1-2) proposed for quantifying failure localization capability can be used to evaluate the impact o f various parameters, including topology, number of monitors, and probing mechanisms.' as abstract.

there is a document named 'doc-x0010' that
  has 'We investigate the problem of identifying individual link metrics in a communication network from end-to-end path measurements, under the assumption that link metrics are additive and constant. To uniquely identify the link metrics, the number of linearly independent measurement paths must equal the number of links. Our contribution is to characterize this condition in terms of the network topology and the number/placement of monitors, under the constraint that measurement paths must be cycle-free. Our main results are: (i) it is generally impossible to identify all the link metrics by using two monitors; (ii) nevertheless, metrics of all the interior links not incident to any monitor are identifiable by two monitors if the topology satisfies a set of necessary and sufficient connectivity conditions; (iii) these conditions naturally extend to a necessary and sufficient condition for identifying all the link metrics using three or more monitors. We show that these conditions not only allow efficient identifiability tests, but also enable an efficient algorithm to place the minimum number of monitors in order to identify all link metrics. Our evaluations on both random and real topologies show that the proposed algorithm achieves identifiability using a much smaller number of monitors than a baseline solution.' as abstract.

there is a document named 'doc-x0022' that
  has 'Truth discovery is an important technique for enabling reliable crowdsourcing applications. It aims to automatically discover the truths from possibly conflicting crowdsourced claims. Most existing truth discovery approaches focus on categorical applications, such as image categorization. They use the accuracy (i.e., rate of exactly correct claims) to capture the reliability of participants. As a consequence, they are not effective for truth discovery in quantitative applications, such as percentage annotation and object counting, where similarity rather than exact matching between crowdsourced claims and latent truths should be considered. In this paper, we propose two Quantitative Truth Finders (QTFs) for truth discovery in quantitative crowdsourcing applications. One QTF explores an additive model and the other explores a multiplicative model to capture different relationships between crowdsourced claims and latent truths in different classes of quantitative tasks. These QTFs naturally incorporate the similarity between variables. Moreover, they use the bias and the confidence instead of the accuracy to capture participants\' abilities in quantity estimation. These QTFs are thus capable of accurately discovering quantitative truths in particular domains. Through experiments, we demonstrate that these QTFs outperform other state-of-the-art approaches for truth discovery in quantitative crowdsourcing applications and they are also quite efficient.' as abstract.

there is a document named 'doc-x0023' that
  has 'Thomas Killion is Deputy Assistant Secretary of the Army for Research and Technology and US Army Chief Scientist; Phil Sutton is Director General Research and Technology in the UK MoD; Michael Frame is the Defence Science and Technology Policy and Plans Officer in the British Embassy in Washington; and Pearl Gendason is a Physical Scientist at the US Army Research Laboratory. They describe the collaborative research being conducted in a UK-US alliance of government, industrial and academic researchers, how that research is organised and what it may mean to soldiers of both nations in the operational front line.' as abstract.

there is a document named 'doc-x0024' that
  has 'We study a problem in which a single sensor is scheduled to observe sites periodically, motivated by applications in which the goal is to maintain up-to-date readings for all the observed sites. In the existing literature, it is typically assumed that the time for a sensor switching from one site to another is negligible. This may not be the case in applications such as camera surveillance of a border, however, in which the camera takes time to pan and tilt to refocus itself to a new geographical location. We formulate a problem with constraints modeling refocusing delays. We prove the problem to be NP-hard and then study a special case in which refocusing is proportional to some Euclidian metric. We give a lower bound on the optimal cost for the scheduling problem, and we derive exact solutions for some special cases of the problem. Finally, we provide and experimentally evaluate several heuristic algorithms, some of which are based on the computed lower bound, for the setting of one sensor and many sites.' as abstract.

there is a document named 'doc-x0032' that
  has 'When a sensor network is deployed in the field it is typically required to support multiple simultaneous missions, which may start and finish at different times. Schemes that match sensor resources to mission demands thus become necessary. In this article, we consider new sensor-assignment problems motivated by frugality, i.e., the conservation of resources, for both static and dynamic settings. In the most general setting, the problems we study are NP-hard even to approximate, and so we focus on heuristic algorithms that perform well in practice. In the static setting, we propose a greedy centralized solution and a more sophisticated solution that uses the Generalized Assignment Problem model and can be implemented in a distributed fashion. In what we call the dynamic setting, missions arrive over time and have different durations. For this setting, we give heuristic algorithms in which available sensors propose to nearby missions as they arrive. We find that the overall performance can be significantly improved if available sensors sometimes refuse to offer utility to missions they could help, making this decision based on the value of the mission, the sensor\'s remaining energy, and (if known) the remaining target lifetime of the network. Finally, we evaluate our solutions through simulations.' as abstract.

there is a document named 'doc-x0033' that
  has 'In this paper, the performance of the network coded amplify-forward cooperative protocol is studied. The use of network coding can suppress the bandwidth resource consumed by relay transmission, and hence increase the spectral efficiency of cooperative diversity. A distributed strategy of relay selection is applied to the cooperative scheme, which can reduce system overhead and also facilitate the development of the explicit expressions of information metrics, such as outage probability and ergodic capacity. Both analytical and numerical results demonstrate that the proposed protocol can achieve large ergodic capacity and full diversity gain simultaneously.' as abstract.

there is a document named 'doc-1000' that
  has 'Mobile wireless networks with intermittent connectivity, often called Delay/Disruption Tolerant Networks (DTNs), have recently received a lot of attention because of their efficiency in various application scenarios where delay is noncritical. It has been shown that DTN routing and transport protocols can benefit from node mobility by letting the nodes carry and forward data to overcome partial connectivity. The scalability of DTN protocols is very important for protocol design and evaluation. In particular, we need models that allow us to predict the performance of DTN as a function of node mobility behavior (e.g., inter-contact times). Yet so far little work has been done to develop a framework that formalizes DTN performance as a function of motion behavior. In this paper, we represent DTNs as a class of wireless mobile networks with intermittent connectivity, where the inter-contact behavior of an arbitrary pair of nodes can be described by a generalized two-phase distribution (i.e., a power-law head with an exponential tail). Recent experiments have confirmed that the two-phase distribution is the most realistic model for vehicular and pedestrian scenarios, where the specific shape of the distribution depends on the degree of correlation among mobile traces. Using this DTN model, we make the following contributions. First, we extend the throughput and delay scaling results of Grossglauser and Tse (derived for exponential intercontact time distributions) to the two-phase distribution with motion correlation. Second, we analyze the impact of various network parameters and routing strategies (such as buffer constraints and replication) on the capacity/delay scaling properties of DTNs, again for different correlation behaviors. Finally, we validate our analytical results with a simulation study.' as abstract.

there is a document named 'doc-1001' that
  has 'The analysis of social and technological networks has attracted a lot of attention as social networking applications and mobile sensing devices have given us a wealth of real data. Classic studies looked at analysing static or aggregated networks, i.e., networks that do not change over time or built as the results of aggregation of information over a certain period of time. Given the soaring collections of measurements related to very large, real network traces, researchers are quickly starting to realise that connections are inherently varying over time and exhibit more dimensionality than static analysis can capture. In this paper we propose new temporal distance metrics to quantify and compare the speed (delay) of information diffusion processes taking into account the evolution of a network from a local and global view. We show how these metrics are able to capture the temporal characteristics of time-varying graphs, such as delay, duration and time order of contacts (interactions), compared to the metrics used in the past on static graphs. As a proof of concept we apply these techniques to two classes of time-varying networks, namely connectivity of mobile devices and e-mail exchanges.' as abstract.

there is a document named 'doc-1004' that
  has 'An individual sensor\'s information output is often insufficient for an application, with ambiguities that require refinement or corroboration by fusion with information from other sensors. Fusion of multiple information sources is performed to create an information product of higher quality of information (QoI) that supports more effective intelligence, surveillance, and reconnaissance (ISR). In this paper we present an approach to determining the QoI attributes (metadata) relevant to tracking and classification of multiple vehicles, and the necessary weighting (qualifying) terms, as information derived from multiple acoustic sensors is fused. Field trial data is used to validate the conclusions.' as abstract.

there is a document named 'doc-1005' that
  has 'In this paper, we employ a stochastic geometry model to analyze transmission capacity in wireless cooperative networks. Assuming that simultaneous transmitters are randomly located in space according to Poisson point process with density _, we develop the bound performances on outage probability and outage capacity for both direct transmission and Decodeand-Forward (DAF) cooperative scheme. Due to the nature of multipath propagation of cooperative transmission, we define regional capacity as the multiplied product of average density of successful simultaneous transmissions, achieved outage capacity and transmission distance. It shows that the regional capacity for cooperative transmission scales as _(__), which is the same as the transport capacity for wireless network. Furthermore, Monte Carlo simulations demonstrate the significant improvement on the transmission capacity by using cooperative transmission.' as abstract.

there is a document named 'doc-1006a' that
  has 'Wireless sensor networks are usually equipped with single radio interfaces where a sensor node can participate in only one transmission at a time. As a result, a node\'s airtime is "shared" by multiple flows that pass through the node. Although it is practically important, how to effectively allocate airtime among flows has not received sufficient research attention in the existing literature. In this paper, after showing the potential gain from adaptive airtime allocation in alleviating network congestions, we formulate a new congestion control problem using network utility maximization with respect to airtime fractions, transmission power and flow rate. We prove that the new congestion control is a concave problem and develop a local algorithm for adaptively tuning airtime fractions in parallel with power and rate. Simulation results show the convergence of the optimal airtime allocation as well as its effectiveness in boosting flow rate and conserving power.' as abstract.

there is a document named 'doc-1008' that
  has 'In this paper, we consider the problem of transmission delay in terms of finite coding length derived from the random coding bound for different cooperative protocols. Specifically, we first study the impact of cooperative transmission on the routing decision for wireless ad-hoc networks, where a routing optimization problem is formulated to minimize the endto-end delay that ensures a satisfactory error performance. The closed-expression of the optimal solution is developed through the optimization problem and later used as quantitative criterion of routing decision. Furthermore, by considering the interference impact on system performance along a multi-hop routing, we then investigate performance gain on transmission delay for wireless cooperative networks by using a simple multi-user detection scheme, called overlapped transmission, in which multiple transmissions are allowed only when the information in the interfering signal is known at the receiver. As a result, both analytical and numerical results demonstrate the significant improvement on the system performance by using cooperative transmission with overlapping as well as the trade-off between the end-to-end delay and network throughput.' as abstract.

there is a document named 'doc-1011' that
  has 'Neighbor discovery is one of the first steps in the initialization of a wireless ad hoc network. In this paper, we design and analyze practical algorithms for neighbor discovery in wireless networks. We first consider an ALOHA-like neighbor discovery algorithm in a synchronous system, proposed in an earlier work. When nodes do not have a collision detection mechanism, we show that this algorithm reduces to the classical Coupon Collector\'s Problem. Consequently, we show that each node discovers all its n neighbors in an expected time equal to ne(ln n c), for some constant c. When nodes have a collision detection mechanism, we propose an algorithm based on receiver status feedback which yields a ln n improvement over the ALOHA-like algorithm. Our algorithms do not require nodes to have any estimate of the number of neighbors. In particular, we show that not knowing n results in no more than a factor of two slowdown in the algorithm performance. In the absence of node synchronization, we develop asynchronous neighbor discovery algorithms that are only a factor of two slower than their synchronous counterparts. We show that our algorithms can achieve neighbor discovery despite allowing nodes to begin execution at different time instants. Furthermore, our algorithms allow each node to detect when to terminate the neighbor discovery phase.' as abstract.

there is a document named 'doc-1022a' that
  has 'In spite of the wide adoption of policy-based approaches for security management, and many existing treatments of policy verification and analysis, relatively little attention has been paid to policy refinement: the problem of deriving lower-level, runnable policies from higher-level policies, policy goals, and specifications. In this paper we present our initial ideas on this task, using and adapting concepts from data integration. We take a view of policies as governing the performance of an action on a target by a subject, possibly with certain conditions. Transformation rules are applied to these components of a policy in a structured way, in order to translate the policy into more refined terms; the transformation rules we use are similar to those of \'global-as-view\' database schema mappings, or to extensions thereof. We illustrate our ideas with an example.' as abstract.

there is a document named 'doc-1025' that
  has 'Wildcard identity-based encryption (IBE) provides an effective means of communicating among groups which do not have a well-defined membership or hierarchy pre-established, as may frequently be the case in dynamic coalition operations. The protection of group communication against compromised nodes is, however, expensive in that it typically requires frequent re-keying in the case of attributebased IBE or voting-based revocation mechanisms, which can be problematic in multi-hop ad-hoc networks. In this paper we investigate the use of asymmetric communication links such as may be provided by unmanned aerial vehicles to provide efficient revocation mechanisms for small ad-hoc networks. Such link characteristics allow the efficient maintenance and propagation of blacklists as proposed by Saxena et al. and also enable the development of probability and plausibility metrics for revocation requests. We therefore propose a scheme for the distribution of group keys that requires nodes of the group to collaborate in order to obtain the group secret key. Consequently, untrusted nodes are hindered from communicating with other groups. This isolation from untrusted nodes allows to avoid or at least to postpone expensive node revocations which require the rekeying of the whole group.' as abstract.

there is a document named 'doc-1026' that
  has 'We show attacks on several cryptographic schemes that have recently been proposed for achieving various security goals in sensor networks. Roughly speaking, these schemes all use "perturbation polynomials" to add "noise" to polynomialbased systems that offer information-theoretic security, in an attempt to increase the resilience threshold while maintaining efficiency. We show that the heuristic security arguments given for these modified schemes do not hold, and that they can be completely broken once we allow even a slight extension of the parameters beyond those achieved by the underlying information-theoretic schemes. Our attacks apply to the key predistribution scheme of Zhang et al. (MobiHoc 2007), the access-control schemes of Subramanian et al. (PerCom 2007), and the authentication schemes of Zhang et al. (INFOCOM 2008). Our results cast doubt on the viability of using "perturbation polynomials" for designing secure cryptographic schemes.' as abstract.

there is a document named 'doc-1029b' that
  has 'We demonstrate an extension of hierarchical identity-based encryption (HIBE) from the domain of a single Trusted Authority (TA) to a coalition of multiple independent Trusted Authorities with their own hierarchies. Coalitions formed under such schemes may be temporary or dynamic in membership without compromising security. In this paper we give an instantiation with formal security model and a proof of security against selective-identity chosen-plaintext attacks in the standard model based upon the difficulty of solving the Bilinear Decisional Diffie-Hellman (BDDH) problem.' as abstract.

there is a document named 'doc-1037' that
  has 'Delay Tolerant Networks (DTNs) are wireless networks in which at any given time instance, the probability of having a complete path from a source to destination is low due to the intermittent connectivity between nodes. Several routing schemes have been proposed for such networks to make the delivery of messages possible despite the intermittent connections. In this paper, in addition to intermittent connectivity which impacts routing most strongly, we also analyze the effects of underlying social structure over the communication network. In a social network, nodes interact in diverse ways so that some nodes meet each other more frequently than others. In the paper, we first propose a new network model to reflect the underlying social structure over the network nodes, then we study the effects of this model on the performance of multi-copy based routing algorithms. We also analyze the performance of routing and validate our analysis with simulations.' as abstract.

there is a document named 'doc-1038' that
  has 'One of the most common and important applications of wireless sensor networks is target tracking. We study it in its most basic form, assuming the binary sensing model in which each sensor can return only information regarding target\'s presence or absence within its sensing range. However, unlike most of traditional approaches to binary sensing, we allow sensors to recognize not only target\'s range but also a sector within the circular range around it. Examples of such sensors include cameras, infrared sensors, ultrasonic sensors, etc. For simplicity, we assume that either a group of sensors are collocated in a single spot providing 360 degree coverage or a sensor has multiple antennas or camera providing such coverage. A novel, real-time and distributed target tracking algorithm with directional binary sensor networks is proposed. It is an extension of our previous work on omni-directional binary sensor networks. Using simulations, we demonstrate that this new algorithm achieves high performance and outperforms other algorithms by yielding accurate estimates of the target\'s location. In addition, we discuss the fundamental performance limits and improvement of the tracking performance resulting from providing direction range in addition to a distance range for the algorithm.' as abstract.

there is a document named 'doc-1039a' that
  has 'This paper studies an auction based allocation of network resources for short-term contracts for heterogeneous network services. The combinatorial winner selection yields the optimal resources allocation in a single-round auction for heterogeneous resources. However, the recurring nature of auction for network services causes least wealthy bidders to exit the auction as they persistently lose under the traditional combinatorial winner selection that focuses only on revenue maximization. Such exits decrease price competition and may cause a collapse of the selling prices and revenues of network service providers. We introduce and evaluate a novel winner selection strategy for auctioning of heterogeneous network services. The proposed strategy prevents collapse of the selling prices and the auctioneer revenues, stabilizes auction market, and enhances social welfare by allowing larger subset of users to become occasional winners of auction rounds than the traditional combinatorial winner selection does.' as abstract.

there is a document named 'doc-1060' that
  has 'In the setting of identity-based encryption with multiple trusted authorities, TA anonymity formally models the inability of an adversary to distinguish two ciphertexts corresponding to the same message and identity, but generated using different TA master public-keys. This security property has applications in the prevention of traffic analysis in coalition networking environments. In this paper, we examine the implications of TA anonymity for key-privacy for normal public-key encryption (PKE) schemes. Key-privacy for PKE captures the requirement that ciphertexts should not leak any information about the publickeys used to perform encryptions. Thus key-privacy guarantees recipient anonymity for a PKE scheme. Canetti, Halevi and Katz (CHK) gave a generic transform which constructs an IND-CCA secure PKE scheme using an identity-based encryption (IBE) scheme that is selective-id IND-CPA secure and a strongly secure one-time signature scheme. Their transform works in the standard model (i.e. does not require the use of random oracles). Here, we prove that if the underlying IBE scheme in the CHK transform is TA anonymous, then the resulting PKE scheme enjoys key-privacy. Whilst IND-CCA secure, key-private PKE schemes are already known in the standard-model, our result gives the first generic method of constructing a key-private PKE scheme in the standard model. We then go on to investigate the TA anonymity of multi-TA versions of well-known standard model secure IBE schemes. In particular, we prove the TA anonymity and selective-id IND-CPA security of a multi-TA version of Gentry\'s IBE scheme. Applying the CHK transform, we obtain a new, efficient key-private, IND-CCA secure PKE scheme in the standard model.' as abstract.

there is a document named 'doc-1061' that
  has 'Modern ISR1/ISTAR2 networks comprise a very diverse and disparate set of asset types and networking technologies, which provide a unique set of challenges in the areas of sensor identification, classification, interoperability and sensor data sharing, dissemination and consumability. This paper presents the ITA Sensor Fabric, developed as part of the International Technology Alliance (ITA) in Network and Information Science, a middleware infrastructure that addresses these challenges by providing unified/universal policy controlled access to, and management of, ISR/ISTAR networks. The ITA Sensor Fabric spans the network from command and control, through forward operating bases, and out to mobile forces and fielded (both mobile and/or fixed) sensors in the area of operations. This paper also presents a use case scenario developed in partnership with the U.S. Army Research Laboratory (ARL) and deployed in ARL\'s Wireless Emulation Laboratory (WEL), that demonstrates the ITA Sensor Fabric applicability to coalition operations.' as abstract.

there is a document named 'doc-1062' that
  has 'Self-Managed Cells (SMCs) define an infrastructure for building ubiquitous computing applications. An SMC consists of an autonomous administrative domain based on a policy-driven feedback control-loop. SMCs are able to interact with each other and compose with other SMCs to form larger autonomous components. In this paper we present a formal specification of an SMC\'s behaviour for the analysis and verification of its operation in collaborations of SMCs. These collaborations typically involve SMCs originated from different administrative authorities, and the definition of a formal model has helped us to verify the correctness of their operation when SMCs are composed or federated.' as abstract.

there is a document named 'doc-1063' that
  has 'Current research on acoustic vehicle classification has been generally aimed at utilizing various feature extraction methods and pattern recognition techniques. Previous research in gait biometrics has shown that domain knowledge or semantic enrichment can assist in improving the classification accuracy. In this paper, we address the problem of semantic enrichment by learning the semantic attributes from the training set, and then formalize the domain knowledge by using ontologies. We first consider a simple data ontology, and discuss how to use it for classification. Next we propose a scheme, which uses a semantic attribute to mediate information fusion for acoustic vehicle classification. To assess the proposed approaches, experiments are carried out based on a data set containing acoustic signals from five types of vehicles. Results indicate that whether the above semantic enrichment can lead to improvement depends on the accuracy of semantic annotation. Among the two enrichment schemes, semantically mediated information fusion achieves less significant improvement, but is insensitive to the annotation error.' as abstract.

there is a document named 'doc-1064' that
  has 'When data from two or more coalition members is used in a joint mission, the flow of any information over the respective networks must be tightly controlled. Data sent over the networks would be routed through nodes that should enforce policies concerning: what information about each coalition member\'s assets will be exchanged with other coalition members; who, with what authentication, and under what conditions can access the information, and in what form it can be provided. We discuss the requirements of such a policy-enabled federated network, in which any node could be responsible for enforcement of all policies applicable to it and the data that flows through it. We describe the high-level architecture of the system currently under development, and the design trade-offs that were made to balance ease of deployability and flexibility in policy enforcement. We also present our initial implementation with performance evaluation results that showcase its applicability to the scenario considered.' as abstract.

there is a document named 'doc-1065' that
  has 'Monitoring multimodal data generated by sensor networks for extracting information is a challenging task for the human observer. To manage the barrage of data, one needs to create mechanisms for identifying only those time intervals which are informative and worthy of further highlevel analysis either by machine or the human observer. We regard a time interval to be informative and contain an event if it is uncommon or distinct from routine background. Different events in general may unfold at different temporal scales. Here, we present a non-parametric distribution based approach for event detection in sensor network data. In this approach we employ multiple sliding windows at different scales to obtain the distribution of the data. We segment the temporal data stream and identify the potential event bearing candidates by comparing the present and past statistical behavior of the data. In the experiments we demonstrate the effect of optimum bin width selection on accuracy and the range of allowable window sizes and therefore time scales. We analyze the computational speed as well as the supporting empirical results on the bin width.' as abstract.

there is a document named 'doc-1066' that
  has 'In this paper, we introduce a shape-based, time-scale invariant feature descriptor for 1-D sensor signals. The time-scale invariance of the feature allows us to use feature from one training event to describe events of the same semantic class which may take place over varying time scales such as walking slow and walking fast. Therefore it requires less training set. The descriptor takes advantage of the invariant location detection in the scale space theory and employs a high level shape encoding scheme to capture invariant local features of events. Based on this descriptor, a scale-invariant classifier with "R" metric (SIC-R) is designed to recognize multi-scale events of human activities. The R metric combines the number of matches of keypoint in scale space with the Dynamic Time Warping score. SICR is tested on various types of 1-D sensors data from passive infrared, accelerometer and seismic sensors with more than 90% classification accuracy.' as abstract.

there is a document named 'doc-1072a' that
  has 'Firewalls remain the main perimeter security protection for corporate networks. However, network size and complexity make firewall configuration and maintenance notoriously difficult. Tools are needed to analyse firewall configurations for errors, to verify that they correctly implement security requirements and to generate configurations from higher-level requirements. In this paper we extend our previous work on the use of formal argumentation and preference reasoning for firewall policy analysis and develop means to automatically generate firewall policies from higher-level requirements. This permits both analysis and generation to be done within the same framework, thus accommodating a wide variety of scenarios for authoring and maintaining firewall configurations. We validate our approach by applying it to both examples from the literature and real firewall configurations of moderate size ( 150 rules).' as abstract.

there is a document named 'doc-1074' that
  has 'Semantic wikis extend the idea of collaborative content editing (made popular by systems such as Wikipedia) to the realm of semantically-enriched representations and formal knowledge models. While a conventional wiki includes structured text and untyped hyperlinks, a semantic wiki is based on the representation of metadata elements. Semantic MediaWiki (SMW) [1] is probably the most popular and mature semantic wiki. It relies on the same wiki engine as Wikipedia and uses elements from the Resource Description Framework (RDF) and Web Ontology Language (OWL) namespaces.' as abstract.

there is a document named 'doc-1075' that
  has 'In both the commercial and defense sectors a compelling need is emerging for highly dynamic, yet risk optimized, sharing of information across traditional organizational boundaries. Risk optimal decisions to disseminate mission critical tactical intelligence information to the pertinent actors in a timely manner is critical for a mission\'s success. In this paper1 , we argue that traditionally decision support mechanisms for information sharing (such as Multi-Level Security (MLS)) besides being rigid and situation agnostic, do not offer explanations and diagnostics for non-shareability. This paper exploits rich security metadata and semantic knowledgebase that captures domain specific concepts and relationships to build a logic for risk optimized information sharing. We show that the proposed approach is: (i) flexible: e.g., sensitivity of tactical information decays with space, time and external events, (ii) situation-aware: e.g., encodes need-to-know based access control policies, and more importantly (iii) supports explanations for non-shareability; these explanations in conjunction with rich security metadata and domain ontology allows a sender to intelligently transform information (e.g., downgrade information, say, by deleting participant list in a meeting) with the goal of making transformed information shareable with the recipient. In this paper, we will describe an architecture for secure information sharing using a publicly available hybrid semantic reasoner and present several illustrative examples that highlight the benefits of our proposal over traditional approaches.' as abstract.

there is a document named 'doc-1078' that
  has 'Today, sensing resources3 are the most valuable assets of critical tasks (e.g., border monitoring). Although, there are various types of assets available, each with different capabilities, only a subset of these assets is useful for a specific task. This is due to the varying information needs of tasks. This gives rise to assigning useful assets to tasks such that the assets fully cover the information requirements of the individual tasks. The importance of this is amplified in the intelligence, surveillance, and reconnaissance (ISR) domain, especially in a coalition context. This is due to a variety of reasons such as the dynamic nature of the environment, scarcity of assets, high demand placed on available assets, sharing of assets among coalition parties, and so on. A significant amount of research been done by different communities to efficiently assign assets to tasks and deliver information to the end user. However, there is little work done to infer sound alternative means to satisfy the information requirements of tasks so that the satisfiable tasks are increased. In this paper, we propose a hybrid reasoning approach (viz., a combination of rule-based and ontology-based reasoning) based on current Semantic Web4 technologies to infer assets types that are necessary and sufficient to satisfy the requirements of tasks in a flexible manner.' as abstract.

there is a document named 'doc-1079' that
  has 'We consider dual classes of geometric coverage problems, in which disks, corresponding to coverage regions of sensors, are used to cover a region or set of points in the plane. The first class of problems involve assigning radii to already-positioned sensors (being cheap). The second class of problems are motivated by the fact that the sensors may, because of practical difficulties, be positioned with only approximate accuracy (being flexible). This changes the character of some coverage problems that solve for optimal disk positions or disk sizes, ordinarily assuming the disks can be placed precisely in their chosen positions, and motivates new problems. Given a set of disk sensor locations, we show for most settings how to assign either (near-)optimal radius values or allowable amounts of placement error. Our primary results are 1) in the 1-d setting we give a faster dynamic programming algorithm for the (linear) sensor radius problem; and 2) we find a max-min fair set of radii for the 2-d continuous problems in polynomial time. We also give results for other settings, including fast approximation algorithms for the 1-d continuous case.' as abstract.

there is a document named 'doc-1080' that
  has 'Sensor networks introduce new resource allocation problems in which sensors need to be assigned to the tasks they best help. Such problems have been previously studied in simplified models in which utility from multiple sensors is assumed to combine additively. In this paper we study more complex utility models, focusing on two particular applications: event detection and target localization. We develop distributed algorithms to assign directional sensors of different types to multiple simultaneous tasks using exact location information. We extend our algorithms by introducing the concept of fuzzy location which may be desirable to reduce computational overhead and/or to preserve location privacy. We show that our schemes perform well using both exact or fuzzy location information.' as abstract.

there is a document named 'doc-1083' that
  has 'Both bandwidth and energy become important resource constraints when multi-hop wireless networks are used to transport relatively high data rate sensor flows. A particularly challenging problem involves the selection of flow data rates that maximize application (or mission) utilities over a time horizon, especially when different missions are active over different time intervals. Prior works on utility driven adaptation of flow data rates typically focus only on instantaneous utility maximization and are unable to address this temporal variation in mission durations. In this work, we derive an optimal control-based Network Utility Maximization (NUM) framework that is able to maximize the system utility over a lifetime that is known either deterministically or statistically. We first consider a static setup in which all the missions are continuously active for a deterministic duration, and show how the rates can be optimally adapted, via a distributed protocol, to maximize the total utility. Next, we develop adaptive protocols for the dynamic cases when we have (i) complete knowledge about the mission utilities and their arrivals and departures, and (ii) a varying amount of statistical information about the missions. Our simulation results indicate that our protocols are robust, efficient and close to the optimal.' as abstract.

there is a document named 'doc-1085' that
  has 'We discuss how agents can support collaborative planning activities within coalitions, where coalition partners plan and act according to given policies. In a set of experiments with human test subjects, we investigate how agents can aid such a collaborative planning effort and how effective they are in the chosen support modes.' as abstract.

there is a document named 'doc-1086' that
  has 'Working collaboratively in a team and formulating a plan of action to achieve a particular goal is often a complex task for humans, especially when the decision-making process is performed in time-stressed situations, in which response teams are assembled in an ad-hoc fashion and operate in dynamic and uncertain environments. We are conducting research towards the development of software agents that will track the activities of human teams and monitor the plan execution in order to offer advice that would enable the humans to avoid problems, resolve them or recognise unexpected options, and in general maintain the synchronization among the different plan components.' as abstract.

there is a document named 'doc-1091' that
  has 'A collaborative planning effort between partners that form coalitions may be complicated by policies that regulate what actions they may deploy in their plans and, in particular, what information they are allowed to exchange during the planning process. We are interested in situations where coalitions have to be formed ad-hoc without much co-training. For this, we investigate how agents can support human planners in producing good plans while observing the normative standards that regulate their planning and communication behavior. Based on an implementation of such norm-processing agents, we conducted a set of experiments, where human test subject were conducting collaborative planning tasks under the guidance of these agents. A summary of experimental results is provided in the paper.' as abstract.

there is a document named 'doc-1092' that
  has 'Different organizations carry out various missions. Some missions may be very crucial for the organizations, and require critical but scarce resources. Scarcity of these critical resources and importance of the missions create an incentive for the organizations to cooperate by sharing their resources (sensors and other assets) with an expectation of carrying out their missions successfully even if the resources in hand are limited. In this paper, we propose a multiagent framework where organizations\' plans to achieve their missions are semantically described, and then a hierarchical multiagent system is organized to represent each mission. Using the semantic description of the mission plans, the agents reason about the sensors and platforms required for their missions and cooperatively decide on the resources that should be shared to carry out those missions. This is achieved in different levels of the agent hierarchy where policies and constraints are used actively during the decision process. Our experiments show that our approach leads to a better utilization of the resources and significantly improves the number of achievable missions when the number of available sensors and platforms is strictly limited.' as abstract.

there is a document named 'doc-1093' that
  has 'Distributed planning has seen a wide range of techniques aiming to address problems such as distributed plan formation and distributed plan execution. Common to these techniques is the notion of Multi-Agent Planning (MAP) as a combination of planning and coordination by de Weerdt et al. [2]. Among the three types of MAP problems summarised by Durfee [3], distributed planning for distributed plans is considered to be the most challenging. The key issue to tackle is the plan merging, which concerns about resolving potential conflicts (negative interactions) between individual plans (sequences of actions) of the agents. A more complex version of the problem that has many real world examples, is that where agents need to maintain confidential information such as private knowledge, and/or do not wish to expose their individual plans. Such confidentiality constraints may make the central/direct analysis of action interactions impossible. This paper focuses on the task of distributed planning for distributed plans with confidentiality.' as abstract.

there is a document named 'doc-1094' that
  has 'A sensor network may be required to support multiple mission to be accomplished simultaneously. Furthermore, the environment may change at any time; i.e. a new mission may arrive at any time. In solving this many-mission, manysensor problem in dynamic environments, conflicts between missions may occur for the use of sensor resources. A mechanism to match sensor resources to mission demands thus becomes necessary. In this paper, motivated by the conservation of resources, we consider the problem of sensor-mission assignment, in which sensors may be shared and reassigned between tasks. To achieve this, sensors are represented by agents, which coordinate to establish virtual organizations to meet mission requirements. The agent coordinating the achievement of a mission utilises a novel multi-round, Knapsack based algorithm, GAP-E, to allocate sensor agents to tasks based on bids received. Through simulations, we empirically demonstrate that this model provides a significant improvement in the number of completed missions as well as execution time.' as abstract.

there is a document named 'doc-1097' that
  has 'Modern military and civilian surveillance applications should provide end users with the high level representation of events observed by sensors rather than with the raw data measurements. Hence, there is a need for a system that can infer higher level meaning from collected sensor data. We demonstrate that probabilistic context free grammars (PCFGs) can be used as a basis for such a system. To recognize events from raw sensor network measurements, we use a PCFG inference method based on Stolcke(1994) and Chen(1996). We present a fast algorithm for deriving a concise probabilistic context free grammar from the given observational data. The algorithm uses an evaluation metric based on Bayesian formula for maximizing grammar a posteriori probability given the training data. We also present a real-world scenario of monitoring a parking lot and the simulation based on this scenario. We described the use of PCFGs to recognize events in the results of such a simulation. We finally demonstrate the deployment details of such an event recognition system.' as abstract.

there is a document named 'doc-1099' that
  has 'Stateless opportunistic forwarding is a simple faulttolerant distributed approach for data delivery and information querying in wireless ad hoc networks, where packets are forwarded to the next available neighbors in a "random walk" fashion, until they reach the destinations or expire. This approach is robust against ad hoc topology changes and is amenable to computation/bandwidth/energy-constrained devices; however, it is generally difficult to predict the end-to-end latency suffered by such a random walk in a given network. In this paper, we make several contributions on this topic. First, by using spectral graph theory we derive a general formula for computing the exact hitting and commute times of weighted random walks on a finite graph with heterogeneous sojourn times at relaying nodes. Such sojourn times can model heterogeneous duty cycling rates in sensor networks, or heterogeneous delivery times in delay tolerant networks. Second, we study a common class of distance-regular networks with varying numbers of geographical neighbors, and obtain simple estimate-formulas of hitting times by numerical analysis. Third, we study the more sophisticated settings of random geographical locations and distance-dependent sojourn times through simulations. Finally, we discuss the implications of this on the optimization of latency-overhead trade-off.' as abstract.

there is a document named 'doc-1101' that
  has 'Recently considerable research has been undertaken into estimating the quality of information (QoI) delivered by military sensor networks. QoI essentially estimates the probability that the information available from the network is correct. Knowledge of the QoI would clearly be of great use to decision makers using a network. An important class of sensors, that provide inputs to networks in real-life, are concerned with target tracking. Assessing the tracking performance of these sensors is an essential component in estimating the QoI of the whole network We have investigated three potential QoI metrics for estimating the dynamic target tracking performance of systems based on some state estimation algorithms. We have tested them on different scenarios with varying degrees of tracking difficulty. We performed experiments on simulated data so that we have a ground truth against which to assess the performance of each metric. Our measure of ground truth is the Euclidean distance between the estimated position and the true position. Recently researchers have suggested using the entropy of the covariance matrix as a metric of QoI [1][2]. Two of our metrics were based on this approach, the first being the entropy of the co-variance matrix relative to an ideal distribution, and the second is the information gain at each update of the covariance matrix. The third metric was calculated by smoothing the residual likelihood value at each new measurement point, similar to the model update likelihood function in an IMM filter. Our experiment results show that reliable QoI metrics cannot be formulated by using solely the covariance matrices. In other words it is possible that a covariance matrix can have high information content, while the position estimate is wrong. On the other hand the smoothed residual likelihood does correlate well with tracking performance, and can be measured without knowledge of the true target position.' as abstract.

there is a document named 'doc-1103' that
  has 'In recent years, the sketch-based technique has been presented as an effective method for counting stream items on processors with limited storage and processing capabilities, such as the network processors. In this paper, we examine the implementation of a sketch-based counting algorithm on the heterogeneous multi-core Cell processor. Like the network processors, the Cell also contains on-chip special processors with limited local memories. These special processors enable parallel processing of stream items using short-vector dataparallel (SIMD) operations. We demonstrate that the inaccuracies of the estimates computed by straightforward adaptations of current sketch-based counting approaches are exacerbated by increased inaccuracies in approximating counts of low frequency items, and by the inherent space limitations of the Cell processor. To address these concerns, we implement a sketch-based counting algorithm, FCM, that is specifically adapted for the Cell processor architecture. FCM incorporates novel capabilities for improving estimation accuracy using limited space by dynamically identifying low- and high-frequency stream items, and using a variable number of hash functions per item as determined by an item\'s current frequency phase. We experimentally demonstrate that with similar space consumption, FCM computes better frequency estimates of both the low- and high-frequency items than a naive parallelization of an existing stream counting algorithm. Using FCM as the kernel, our parallel algorithm is able to scale the overall performance linearly as well as improve the estimate accuracy as the number of processors is increased. Thus, this work demonstrates the importance of adapting the algorithm to the specifics of the underlying architecture.' as abstract.

there is a document named 'doc-1104' that
  has 'In this paper, we will examine the problem of clustering massive domain data streams. Massive-domain data streams are those in which the number of possible domain values for each attribute are very large and cannot be easily tracked for clustering purposes. Some examples of such streams include IP-address streams, credit-card transaction streams, or streams of sales data over large numbers of items. In such cases, it is well known that even simple stream operations such as counting can be extremely difficult because of the difficulty in maintaining summary information over the different discrete values. The task of clustering is significantly more challenging in such cases, since the intermediate statistics for the different clusters cannot be maintained efficiently. In this paper, we propose a method for clustering massive-domain data streams with the use of sketches. We prove probabilistic results which show that a sketch-based clustering method can provide similar results to an infinitespace clustering algorithm with high probability. We present experimental results which validate these theoretical results, and show that it is possible to approximate the behavior of an infinitespace algorithm accurately.' as abstract.

there is a document named 'doc-1105a' that
  has 'The output of a sensor network intended to detect events or objects generally comprises evidentiary reports of features in the environment that may correspond to those phenomena.  Signals from multiple sensors are commonly fused to maximize fidelity of detection through for example synergy between different modes of detection, or simple confirmation.  We have previously demonstrated the ability to calculate the meaning of a location report as a probability distribution over potential ground truths by using a stochastic process algebraic model compiled to a discrete-state, continuous-time Markov chain, and performing a transient analysis which resembles the process of parameterizing a Bayesian network.  We introduce an approach to representing temporal fusion of multiple heterogeneous sensor detections with different modalities and timing characteristics using a stochastic process algebra.  This facilitates analysis of probabilistic properties of the system, and inclusion of those properties into larger models. The formal models are translated into continuous time Markov chains, which provide an important trade-off between the approximation of timing information against complexity of analysis.  This is vital to the investigation of analytic computation in real world problems. We illustrate this with an example detection-oriented sensing service model emphasizing the impact of timing.  Detection probability and confidence is an essential aspect of the quality of information delivered by a sensing service.  The present work is part of an effort to develop a formal event detection calculus that captures the essence of sensor information relating to events, such that features and dependencies can be exploited in re-usable, extendible compositional models.' as abstract.

there is a document named 'doc-1106' that
  has 'Dynamic, mission-focussed intelligence, surveillance and reconnaisance (ISR) requires agile management of informationprovisioning capabilities. This includes rapid assembly of sensing systems, highly efficient resource management, and an ability to configure and reconfigure, task and retask, ISR systems in a robust way [1]. We consider the problem of sensor-mission assignment as that of allocating a collection of ISR assets (including sensors and sensor platforms) to a set of tasks comprising a mission, in an attempt to satisfy the ISR requirements of those tasks. We assume that tasks originate from ad hoc communities of interest (CoIs) within the coalition [1], and that coalition members share ISR assets to some extent.' as abstract.

there is a document named 'doc-1107' that
  has 'The diverse sensor types and networking technologies commonly used in fielded sensor networks provide a unique set of challenges in the areas of sensor identification, interoperability, and sensor data censurability.  The ITA Sensor Fabric is a middleware infrastructure - developed as part of the International Technology Alliance (ITA) in Network and Information Science - that addresses these challenges by providing unified access to, and management of, sensor networks. The Fabric spans the network from command and control through forward porting bases, and out to mobile forces and fielded sensors, maximising the availability and utility of intelligence information to users.' as abstract.

there is a document named 'doc-1108a' that
  has 'This paper addresses the problem of providing congestion-management for a shared wireless sensor networkbased target tracking system. In many large-scale wireless sensor network target tracking scenarios (e.g., a surveillance system for tracking vehicles in urban environments), multiple targets may converge within close proximity of each other. Such scenarios may cause network congestion as nearby sensors attempt to concurrently send updates to a data aggregation point (e.g., base station). We consider the case in which this problem is further complicated by two factors. First, such a large-scale sensor network may very well be deployed to serve multiple target tracking applications with different and dynamic priorities and interests in different (types of) targets. Second, each application will most likely place a different premium on the timeliness of the target information (principally defined by some quality metric) they receive. All the above challenges introduce formidable challenges in providing the expeditious delivery of target information to all prioritized applications. In this paper, we advocate the use of a distributed auction-based approach to locally manage network bandwidth allocation in the described context. We use the Second Price Auction mechanism (to ensure incentive compatibility) in which the congested node acts as the auctioneer and the packets carrying target updates act as bidders. Their bid values are defined by the loss of information utility to the applications associated with the packets. The winning packet receives the current transmission slot of the auctioneer node. We demonstrate through simulation that the resulting auction allocates bandwidth efficiently, maximizing the collective applications\' goals, even when the application priorities change dynamically.' as abstract.

there is a document named 'doc-1109' that
  has 'This paper presents a novel algorithm for enhancing the efficiency and robustness of distributed trust authority protocols for mobile ad hoc networks (MANETs). Our algorithm determines a quorum of trust authority nodes required for a distributed protocol run based upon a set of quality metrics and establishes an efficient routing strategy to contact these nodes. An implementation and efficiency analysis illustrates the viability of our algorithm for small tactical networks consisting of 50 to 150 nodes and shows an approximate 32% reduction in communication overhead over traditional broadcast-based approaches to trust authority computations.' as abstract.

there is a document named 'doc-1110b' that
  has 'Despite several research studies, the effective analysis of policy based systems remains a significant challenge. Policy analysis should at least (i) be expressive (ii) take account of obligations and authorizations, (iii) include a dynamic system model, and (iv) give useful diagnostic information. We present a logic-based policy analysis framework which satisfies these requirements, showing how many significant policy-related properties can be analysed, and we give details of a prototype implementation.' as abstract.

there is a document named 'doc-1110d' that
  has 'Despite several research studies, the effective analysis of policy based systems remains a significant challenge. Policy analysis should at least (i) be expressive (ii) take account of obligations and authorizations, (iii) include a dynamic system model, and (iv) give useful diagnostic information. We present a logic-based policy analysis framework which satisfies these requirements, showing how many significant policy-related properties can be analysed, and we give details of a prototype implementation.' as abstract.

there is a document named 'doc-1113' that
  has 'There has been a growing interest in recent years regarding the relationship between social interaction processes, technological artefacts and the mechanisms that underpin human mental states and processes (Clark, 2003, 2008; Hollan et al., 2000; Hutchins, 1995). According to one view, the external environment in which an agent is embedded constitutes more than just a space for sensory inputs and motor outputs; it is also something that can be flexibly factored into episodes of cognitive processing (Clark, 1997, 2008). Inasmuch as this is true, a perspective that sees all the mechanisms of mind as firmly located \'inside the head\' of a particular agent may no longer be appropriate. Instead, in trying to understand the mechanistic underpinnings of the human mind, we may need to place a much greater emphasis on the representational and computational roles played by a complex web of extra-neural resources, resources that include, on occasion, elements of the wider social and technological matrix in which human thought and reason is so often situated.' as abstract.

there is a document named 'doc-1114' that
  has 'Shared understanding is commonly seen as essential to the success of coalition operations. Anecdotal reports suggest that shared understanding enables coalition forces to coordinate their efforts in respect of mission goals, and shortfalls in shared understanding are frequently cited as the reason for poor coalition performance. In spite of this consensus regarding the importance of shared understanding, however, there are very few empirical studies that attempt to explore shared understanding in a military coalition context. This paper attempts to support future research efforts into shared understanding by proposing a specific definition for shared understanding and identifying a number of research challenges. Shared understanding is defined as the ability of multiple agents to exploit common bodies of causal knowledge for the purposes of accomplishing common (or shared) goals. This definition implies that agents possessing shared understanding will be capable of coordinating their respective behaviours in order to ensure the efficient realization of cognitive and behavioural objectives. We also identify a number of areas for future research into shared understanding. These include the factors that affect shared understanding, the effect of shared understanding on coalition performance, and the development of techniques to reliably measure and assess understanding in coalition environments.' as abstract.

there is a document named 'doc-1115' that
  has 'Military planning is primarily a human activity where plans are generated and interpreted by humans.   Currently plans are represented in static format such as text, diagrams and spreadsheets which do not normally contain any of the reasoning, logic and interdependencies.  As a result, the plans are not easy to update and tend to take a lot more time than is normally available.  There is an ongoing research within the International Technology Alliance program to develop a digitized representation of plans, called Collaborative Planning Model (CPM).   This paper details the results of an evaluation of the CPM, during a distributed, multi_level military planning exercise.  The evaluation explored the representational power of CPM to support multiple planners collaborating to create a plan, and detect resource conflicts as they arose.  In addition the evaluation was able to demonstrate integration possibilities of CPM by allowing multiple planners to import, export, and merge plan on different (and independently developed) planning tools.' as abstract.

there is a document named 'doc-1119' that
  has 'We explore a rule-based formalisation for contracts: the rules capture conditional norms, that is, they describe situations arising during the enactment of a multi-agent system, and norms that arise from these situations. However, such rules may establish conflicting norms, that is, norms which simultaneously prohibit and oblige (or prohibit and permit) agents to perform particular actions. We propose to use a mechanism to detect and resolve normative conflicts in a preemptive fashion: these mechanisms are used to analyse a contract and suggest "amendments" to the clauses of the contract. These amendments narrow down the scope of influence of norms and avoid normative conflicts. Agents propose rules and their amendments, leading to a contract in which no conflicts may arise.' as abstract.

there is a document named 'doc-1122' that
  has 'A large scale, interactive and high quality multimedia distribution system is one of the popular Internet applications. As the bandwidth of the last-mile link grows and the technology of streaming evolves, assorted commercial peer-to-peer applications [1] have shown that current Internet has enough available bandwidth to support TV-quality streaming of 450-600 Kbps. However, none of the above peer-to-peer applications is designed for a more volatile network, such as lossy networks with wireless transmission, mobile networks with mobility churn, delay tolerant networks with intermittent Internet access.' as abstract.

there is a document named 'doc-1123' that
  has 'Shair is a system allowing contracted mobile phone users to benefit from their unused quota minutes and text messages. Unused minutes and texts are shared via opportunistic local connectivity, allowing other Shair users to use sharers\' phones within reach of short range radio such as Bluetooth.' as abstract.

there is a document named 'doc-1125' that
  has 'Providing efficient networking services in MANETs is very challenging due to mobility and unpredictable radio channel: a significant number of packets can be corrupted and/or lost. To increase reliability, various measures have been proposed. A popular approach is to use multiple paths and transmit an identical copy of the packet on each path (i.e., path redundancy). A more efficient way is to use Network Coding on top of path redundancy and send different, encoded packets on each path. Network coding can improve throughput efficiency. However, it increases delays. If channel disruption is intermittent, it behooves us to "turn on" path redundancy and/or Network Coding only when packet loss is severe. In this paper we compare via simulation the performance of multipath routing with and without Network Coding (and with/without dynamic adaptation) for various motion and packet loss scenarios in terms of reliability, efficiency, robustness, and scalability.' as abstract.

there is a document named 'doc-1126a' that
  has 'Network graphs, in general, successfully model a wide variety of interactions and relationships among entities, including both physical and logical connections. In this work, we study the problem of mapping a logical network on to a physical network; such a problem arises in various scenarios, for example, assignment of virtual machines on to physical servers in cloud computing, assignment of services on to physical devices in wireless/wire-line environments and physical resource assignment based on social networks. Specifically, in this paper, a logical network is a set of nodes with edges that denote the communication/bandwidth requirement between them, while a physical network denotes a set of physical nodes with edges that represent the available physical resources. The goal is to map the logical nodes on to the physical nodes and find physical resource allocation to meet the logical network demands, subject to physical network constraints. Towards this end, we propose a two-step approach to the problem, provide a set of novel feasibility checks for node assignment which are proved to be necessary and sufficient, and finally present a simple and fast algorithm that achieves a feasible logical to physical mapping with high probability. Illustrative simulation results are also presented to highlight the efficiency of the proposed algorithms.' as abstract.

there is a document named 'doc-1127a' that
  has 'In this paper, we study the utility of opportunistic communication systems with the co-existence of network infrastructure using three real experimental deployments. We study how some important performance metrics change with varying degrees of infrastructure and mobile nodes willing to participate in the opportunistic forwarding. In doing so, we observe phase transitions in the utility of infrastructure and opportunistic forwarding respectively at different points in the design space. We discuss the implications that this has for the design of future network deployments and how this observation can be used to improve network performance, while keeping cost at a minimum. To the best of our knowledge, this is the largest scale of experimental evaluations of hybrid opportunistic networks. It provides empirical evaluations to the large body of theoretical work that has been done in the past and insights for future research.' as abstract.

there is a document named 'doc-1128' that
  has 'Network Utility Maximization (NUM) techniques, which cast resource sharing problems as one of distributed utility maximization, have been investigated for a variety of optimization problems in wireless and wired networks. Our recent work has extended the NUM framework to consider the case of resource sharing by multiple competing missions in a military-centric wireless sensor network (WSN) environment. Our enhanced NUM-based protocols provide rapid and dynamic mission-based adaptation of tactical wireless networks to support the transport of sensor data streams with very small control overhead. In particular, we focus specifically on mechanisms that capture the joint nature of mission utilities and the presence of prioritized mission demands. We then introduce a new problem, of joint utility and network lifetime maximization, as a representative of a new class of multi-metric optimization problems, and provide early evidence that techniques from optimal control theory can be used to derive distributed adaptation protocols conforming to the basic NUM paradigm. We also enumerate and motivate a list of open cross-layer dynamic adaptation problems of direct relevance to military C4I operations.' as abstract.

there is a document named 'doc-1129a' that
  has 'In recent years, the fast evolution in computer games has moved government organizations to investigate how they can be exploited as virtual environments to simulate scenarios which could be expensive or even dangerous to set up in real life, in particular when collaboration among humans is required in order to carry out a shared plan. The proposed system allows the tracking of progresses achieved by each plan participant within the planning domain, by mapping its steps to states and humans\' actions in the virtual environment. It also permits to render the environment and the planning software loosely coupled and to provide flexible responses to participants\' actions in the form of different alternatives to the same plan, such that goals can be achieved following different courses of action.' as abstract.

there is a document named 'doc-1131a' that
  has 'It is well-recognized that for large catastrophes such as tornadoes, earthquakes, disease outbreaks, or aftermath of a war, coalitions that can coordinate and exploit resources and capabilities of many organizations are required to respond effectively. It is our thesis that in such coalition environments, classical security policies and access-control mechanisms need to be augmented by incorporating the notion of risk and trustworthiness of the parties involved. In this paper, we systematically analyze what trust is, highlight challenges in incorporating the notion of trust in coalition environments, and put forward a proposal to address these challenges.' as abstract.

there is a document named 'doc-1133' that
  has 'To achieve the potential of network centric warfare the Army must securely share information across US operational units and with coalition partners while at the same time denying our enemies access to sensitive information.  The sheer number of configuration parameters necessary to achieve such secure interoperability and optimal data sharing creates the opportunity for human error and slows the deployment process.  Furthermore, the underlying security policies may be dynamic due to changing missions, changing coalition partner relationships and compromise of devices.  Finally, the MANET environment is often bandwidth limited, links are sometimes intermittent and end-to-end connectivity is not always possible.  All of these challenges have severely restricted secure interoperability across coalitions of interest.' as abstract.

there is a document named 'doc-1134a' that
  has 'Large corporations are slowly being transformed from monolithic, vertically integrated entities, into globally disaggregated value networks, where each member focuses on its core competencies and relies on partners and suppliers to develop and deliver goods and services. The ability of multiple partners to come together, share sensitive business information and coordinate activities to rapidly respond to business opportunities, is becoming a key driver for success.' as abstract.

there is a document named 'doc-1135' that
  has 'The scientific community has long envisioned vast networks of sensors each attached to a small battery operated microprocessor replete with a radio communications transceiver. For these scientists, each of the sensor terminals is called a node and the entirety of nodes is termed a Wireless Sensor Network (WSN). WSN\'s have proven effective in a plethora of application domains spanning climate science [2] to military operations [10]. In applications where remote longterm operation, broad geographic coverage, low-cost (e.g. expendable hardware), and rapid deployment are vital, the WSN offers a practical and efficient solution. To achieve remote distributed sensing and computation the WSN must achieve synchrony among its component nodes.' as abstract.

there is a document named 'doc-1137' that
  has 'This article reports work on first steps toward characterizing a negotiation process model for ad hoc and distributed groups or teams, so that automation can more accurately track the states of a negotiation from human discourse. We devised three experimental scenarios and ran human subject experiments that involved group decision-making and consensus building. Our experiments showed that the communication patterns of successful distributed ad hoc teams differed in two significantly different conditions. We describe our motivations, experimental design and results.' as abstract.

there is a document named 'doc-1138' that
  has 'In this work, we propose a new technique that makes use of Angle-of-Arrival (AoA) information in conjunction with local interferometry to improve target location estimation. We call this process Angle-of-arrival-assisted Radio Interferometry (ARI). It has a number of desirable attributes including the ability to reduce the synchronization, network, and hardware requirements when operating as the sole RADAR modality and its ability to augment existing pulsed and CW RADAR algorithms. The proposed approach is described and analyzed.' as abstract.

there is a document named 'doc-1139' that
  has 'The problem of detecting transient signals in noise under missing signal observations (samples) is considered. Specifically, a fusion center tries to detect the presence of a decaying signal in additive white Gaussian noise (AWGN) by collecting samples from distributed sensors through erasure channels, during which some of the samples may be lost. Under Neyman-Pearson detection, it is shown that missing samples cause performance degradation by reducing the signal energy received at the fusion center. Based on the assumption that the fusion center can control the sampling procedure through a feedback channel, an adaptive sampling policy is proposed with the goal of achieving accurate and timely detection with the minimum communication cost. The proposed policy is efficient and flexible in that it can be configured to yield a range of performance-cost combinations, where approximated closed-form solutions are derived for the configuration. Simulations show that compared with fixed-rate sampling, the proposed policy achieves significantly better tradeoff between detection performance and communication cost.' as abstract.

there is a document named 'doc-1141' that
  has 'Recently cooperative diversity has received a lot of attention as an effective and low-cost technique to combat multipath fading and enhance transmission reliability. However, many existing cooperative protocols suffer some loss of ergodic capacity due to the extra bandwidth resource consumed by relay transmission. Motivated by such a fact, network coding, a technique well known for its capability to increase system throughput, is proposed in this paper to be combined with cooperative diversity for uplink transmissions. The dynamic nature of multipath propagation has been efficiently utilized by applying a distributed strategy of relay selection. Two kinds of information-theoretic metrics, outage and ergodic capacities, are developed for the proposed transmission scheme to assist performance evaluation. The developed analytical results, shown to fit well with Monte-Carlo simulation, demonstrate that the proposed protocol can achieve better system robustness and larger system throughput simultaneously than comparable schemes.' as abstract.

there is a document named 'doc-1143a' that
  has 'Received power measurements at spatially distributed, passive, monitors contains valuable information on the active wireless transmitters, which can be usefully exploited to make various inferences. In this paper, we study blind estimation of the number of active wireless sources, their transmission powers and their locations in the network based on received power measurements at multiple monitoring nodes, without assuming any prior knowledge or statistical characterization of these parameters. Utilizing geometrical analysis and algorithmic approach we present estimation algorithms for these parameters under signal combination from various transmitters. We present useful non-trivial insights regarding such inferences, and also present simulation results verifying the analysis and quantifying the performance of the estimation algorithms.' as abstract.

there is a document named 'doc-1144a' that
  has 'Risk-based information trading systems have recently emerged as a new paradigm for enabling information sharing in dynamic environments. Such systems build an information trading market whose commodity is information (quantized into objects) and whose currency is monetized evaluated risk. In these trading systems, risk is calculated by the information seller (and consequently charged to the information buyer) as a function of the value of the object and an information buyer\'s propensity to divulge shared information (based on observed past behavior). Whilst standard techniques exist for evaluating the value of an object, determining the propensity of a buyer to leak information is somewhat more problematic. Ostensibly, a seller could rely on static pre-assigned credentials of the buyer, however, such credentials only provide a clue as to the buyer\'s "trustworthiness" at the time of credential issuance and gives no indication of post-issuance behavior. In this paper, we propose the use of a information leakage monitoring subsystem as part of a larger risk trading system to detect information leakage. We propose a framework for the design of such a subsystem and identify the fundamental tradeoffs between maximum information leakage rates, delays in leakage detection, buyer budgetary constraints and inherent errors in the monitoring subsystem.' as abstract.

there is a document named 'doc-1145a' that
  has 'Tactical networks in urban environments are constrained by limited line-of-sight communication and frequent network partitioning. Moreover, energy efficiency for both communication and computation is a major concern as such networks are typically limited to battery-powered devices. Additional resources such as autonomous unmanned vehicles (UVs) or unmanned aerial vehicles (UAVs) are available in today missions. In this paper we investigate the connectivity augmentation of platoon size networks by said network resources, especially UAVs. We propose an authentication and negotiation protocol to securely request information and services from said resources. Services potentially provided in considered scenarios can incorporate the forwarding of data with a certain bandwidth, information about the UV\'s primary mission or missions, or a communication back-link to an infrastructure network. We furthermore investigate a more sophisticated support by motional adjustment of the UAVs. The efficiency and efficacy of the proposed protocols is evaluated in an simulated small-unit tactical network operating in an urban environment.' as abstract.

there is a document named 'doc-1146' that
  has 'Wireless sensor networks fuse data from a multiplicity of sensors of different modalities and spatiotemporal scales to provide information for reconnaissance, surveillance, and situational awareness in many defense applications. For decisions to be based on information returned by sensor networks it is crucial that such information be of sustained high quality. While the Quality of Information (QoI) depends on many factors, perhaps the most crucial is the integrity of the sensor data sources themselves. Even ignoring malicious subversion, sensor data quality may be compromised by nonmalicious causes such as noise, drifts, calibration, and faults. On-line detection and isolation of such misbehaviors is crucial not only for assuring QoI delivered to the end-user, but also for efficient operation and management by avoiding wasted energy and bandwidth in carrying poor quality data and enabling timely repair of sensors. We describe a two-tiered system for on-line detection of sensor faults. A local tier running at resource-constrained nodes uses an embedded model of the physical world together with a hypothesis-testing detector to identify potential faults in sensor measurements and notifies a global tier. In turn, the global tier uses these notifications on the one hand during fusion for more robust estimation of physical world events of interest to the user, and on the other hand for consistency checking among notifications from various sensors and generating feedback to update the embedded physical world model at the local nodes. Our system eliminates the undesirable attributes of purely centralized and purely distributed approaches that respectively suffer from high resource consumption from sending all data to a sink, and high false alarms due to lack of global knowledge. We demonstrate the performance of our system on diverse real-life sensor faults by using a modeling framework that permits injection of sensor faults to study their impact on the application QoI.' as abstract.

there is a document named 'doc-1147a' that
  has 'When a sensor network is deployed in the field, it is typically required to support multiple simultaneous missions, which may start and finish at different times. Schemes that match sensor resources to mission demands thus become necessary. In this paper, we propose centralized and distributed schemes to assign sensors to missions. We also adapt our distributed scheme to make it energy-aware to extend network lifetime. Finally, we show simulation results comparing these solutions. We find that our greedy algorithm frequently performs near-optimally and that the distributed schemes usually perform nearly as well.' as abstract.

there is a document named 'doc-1148' that
  has 'Intermittently connected mobile networks, also called Delay Tolerant Networks (DTNs), are wireless networks in which at any given time instance, the probability of having a complete path from a source to destination is low. Several routing algorithms have been proposed for such networks based on control flooding in which there is a fixed number of copies for each message. Although a DTN is delay tolerant by definition, often there is an upper bound imposed on message delivery delay. In this paper, we propose a novel spraying algorithm in which the number of message copies in the network depends on the urgency of meeting the expected delivery delay for that message. The main objective of this algorithm is to give a chance to early delivery with small number of copies in existence, consequently decreasing the average number of copies sprayed in the network. We derive the formula for the optimum borders of periods for spraying for two-period and three-period variants of our algorithm. We also present simulations of the method and compare their results with the analytical ones and observe the good match between them. Furthermore, we demonstrate that time dependent spraying algorithm provides a significant decrease in average copy count per message while preserving the percentage of the messages delivered before the upper bound of the acceptable delay expires.' as abstract.

there is a document named 'doc-1149' that
  has 'We study target tracking with wireless sensor networks in its most basic form, assuming a binary sensing model in which each sensor can return only 1-bit information regarding target\'s presence or absence in its sensing range. A novel, realtime and distributed target tracking algorithm for imperfect binary sensing models is proposed, which is an extension of our previous work on the ideal binary sensing model. The algorithm estimates target\'s location, velocity and trajectory in a distributed and asynchronous manner. Extensive simulations show that our algorithm achieves high performance and outperforms other algorithms in terms of accuracy of the estimation of target\'s location, velocity and trajectory.' as abstract.

there is a document named 'doc-1150a' that
  has 'In this paper we consider the problem of how entities operating under distinct roots of trust in a coalition environment can flexibly and securely communicate with one another. We consider the identity-based setting, with each entity being pre-configured with a private key from a particular Trusted Authority (TA), but where multiple, independent TAs are involved in the coalition. Our solution to the problem adapts the Boneh-Franklin identity-based encryption (IBE) scheme. It allows any entity to securely communicate with any other entity, even without knowing the TA with which the intended recipient is associated. To enable this, we assume that the TAs co-operate to distribute certain additional public information to all entities which allows entities to decrypt a ciphertext that was composed using the public parameters of one TA, using a private key issued by another. We include a security analysis of our new approach.' as abstract.

there is a document named 'doc-1151' that
  has 'Diverse security applications often require monitoring of a narrow passage, such as an indoor corridor, a tunnel, a bridge; either to protect critical assets at the end of such a passage or to control the passage over it, or both. Often, sensors are arranged in a vector along such a passage and are capable of registering the crossing of a target but not its identity. In this paper, we consider coordinated tracking by such a vector of sensors where each sensor learns timings of objects passing at monitored spots from its predecessors. Hence, the problem we are solving could be formulated as an assignment or matching of target identities to arrival times of the targets at the subsequent sensors in a vector. We introduce a cost function that is the sum of squares of the difference between each target\'s predicted and observed arrival time at each sensor. We use target\'s speed as computed from the arrival times of the target at previous two sensors to predict the time of arrival at the current sensor. In such a scheme, the simple matching algorithm that sorts predicted and actual arrival times in two separated lists and then matches both lists by positions minimizes this and similar cost functions for each sensor locally in O(n log n) time. We have also developed more costly algorithms that yield higher quality global solution at the cost of communication, computation and memory. In the paper, we evaluate analytically, and by simulations, different variants of this matching algorithm and their complexity and performance.' as abstract.

there is a document named 'doc-1152a' that
  has 'In wireless ad hoc and sensor networks, each node is capable of functioning using only its local information about the environment. However, such a node can reach only locally optimal decisions that may prevent the network from ever reaching the global optimum performance for the given application. To avoid this problem, each node needs to cooperate with others to gain knowledge about the overall network and environment properties so that its decision contributes to the network\'s global objectives. This paper models the cooperation between nodes by measuring the level of information sharing between the neighbors. If h-cooperation is applied, each node shares its information with all nodes which are at most h hop away from it. As the cooperation level raises, knowledge of each individual node about its environment increases, thus, it can make better decisions in meeting the main objective of the network application. On the other hand, it also brings extra communication cost and increases the network operation complexity. Therefore, these two contradicting aspects of cooperation cause a cost-quality tradeoff. In this paper, we investigate the effects of this tradeoff in three different types of sensor network applications: (i) finding an efficient sleep schedule based on sensing coverage redundancy (ii) routing in a network with failureprone nodes (iii) routing in a network with a mobile sink node. In all of these applications, we simulated different levels of cooperation and showed significant improvements in the overall system quality when the optimal level of cooperation between the network\'s nodes is chosen.' as abstract.

there is a document named 'doc-1153' that
  has 'We consider the problem of sensor-mission assignment as that of allocating a collection of intelligence, surveillance and reconnaisance (ISR) assets (including sensors and sensor platforms) to a set of mission tasks in an attempt to satisfy the ISR requirements of those tasks. This problem is exacerbated in a coalition context because the full range of possible ISR solutions is not easy to obtain at-a-glance. Moreover, the operational environment is highly dynamic, with frequent changes in ISR requirements and availability of assets. In this paper we describe a solution for the sensor-mission assignment problem that aims to maximize agility in sensor-mission assignment, while preserving robustness. The search space of potential solutions is reduced by employing a semantic reasoner to work out the types of sensor and platform bundles suitable for a given set of ISR tasks. Then, an efficient resource allocation algorithm is used to assign bundles of sensor/platform instances to satisfy each task, within the search space determined by the reasoner. The availability of instances takes into account access rights on those instances across the coalition\'s inventory. We describe a proofof-concept implementation of this approach, in the form of a decision support tool for ISR planning. We illustrate the approach in the context of a coalition peace support operation scenario.' as abstract.

there is a document named 'doc-1154' that
  has 'An application planner\'s point of view of sensorenabled detection systems is considered and a hypothesistesting-based computational framework for evaluating the quality of information (QoI) supported by a sensor network deployment is explored. Through a common, modular analysis framework, that decomposes the computational burden of QoI analysis, the QoI properties of various decision architectures are investigated and trade-offs explored at the sensor, cluster, and system-level. Both finite and infinite-sized networks are considered and extensions of the analysis framework to faulty sensor and the impact of calibration are also investigated.' as abstract.

there is a document named 'doc-1155' that
  has 'In Geographic Routing protocols (e.g. GPSR), a node makes packet forwarding decisions based on the coordinates of its neighbors and the packet\'s destination. Geo-routing uses greedy forwarding as a default; if this fails (e.g. the packet is trapped in a dead end caused by holes and/or obstacles), a recovery scheme based on perimeter routing is invoked. This however often leads to degraded performance. In this paper, we present a hybrid routing scheme called Geographic Direction Forwarding Routing (Geo-DFR), which features efficient recovery from dead ends. Geo-DFR integrates on demand, table driven routing with geo-routing. During the data transfer, periodic routing advertisements from the destination help to track node motion and to update/maintain a feasible direction to the destination. Direction Forwarding in Geo-DFR is designed to complement and even replace perimeter routing in dead end recovery. With the help of a local coordinate system (e.g., GPS or virtual coordinate system), a node derives the direction of the arrival of the advertisements. A packet is first forwarded to the neighbor which yields the most progress towards the destination, i.e., greedy forwarding. If greedy forwarding fails, the packet is "directionally" forwarded to the "most promising" node along the advertised direction, i.e., direction forwarding. Moreover, the direction can be used proactively for "early dead end detection" to decide which forwarding scheme should be used to avoid getting stuck, which is opposed to GPSR in which perimeter routing is applied only after greedy forwarding fails. Through simulation experiments we show that Geo-DFR substantially improves the performance in large, mobile network scenarios.' as abstract.

there is a document named 'doc-1156' that
  has 'Military commanders require precise command, control, and planning information available for a given mission, information that must be tailored for a particular area of operation, for a specific level of command, and for a specific time period. The problem of developing information of this kind is further complicated in a multi-national coalition setting where different components of a coalition plan are developed in semiindependent fashion, but then aggregated and composed to form an overall operational plan that is sufficiently flexible to support change as circumstances evolve. This paper will provide a foundation for context-aware and collaborative planning that will enable customized agents to traverse a diverse, distributed, frequently changing information space to identify relevant data. Once aware of the data, visual interfaces should provide the new information and facilitate understanding of changes among geographically distributed planners. As a first steps toward this vision we have developed a framework called Graphical Plan Authoring Language (G-PAL) that enables multiple distributed planners to collaboratively build plan components that can be composed later on to provide a global view of the plan.' as abstract.

there is a document named 'doc-1157' that
  has 'We compare the reliability performance gain of Random Linear Network Coding (RLNC) with Automatic Repeat Request (ARQ) for a wireless relay network taking into account overhead and complexity of feedback mechanism as well as overhead due to encoding vector embedded in packet header under RLNC. Our goal is not to propose a new ARQ or RLNC error control protocol, but rather to study the fundamental properties of ARQ and RLNC under condition of finite block sizes. We consider an Enhanced ARQ (ARQ-E) scheme that exploits sender side path diversity between the sender and the relays as well as a Single Path Routing (ARQ-SPR) scheme that uses a hop-by-hop ARQ protocol. The performance metric of interest is reliability gain, the expected number of channel uses per data bit received at the receiver. In the case of AWGN channels, we compare the reliability performance of these protocols with each other and observe the fact that RLNC provides limited performance gains.' as abstract.

there is a document named 'doc-1159' that
  has 'Vehicular sensor networks (VSNs) provide a collaborative sensing environment where mobile vehicles equipped with sensors of different nature (from chemical detectors to still/video cameras) inter-work to implement monitoring applications such as traffic reporting, environment monitoring, and distributed surveillance. In particular, there is an increasing interest in proactive urban monitoring where vehicles continuously sense events from streets, autonomously process sensed data (e.g., recognizing license plates), and possibly route messages to vehicles in their vicinity to achieve a common goal (e.g., to permit police agents to track the movements of specified cars). MobEyes is a middleware solution to support VSN-based proactive urban monitoring applications, where the agents (e.g., police cars) harvest metadata from regular VSNenabled vehicles. Since multiple agents collaborate in a typical urban sensing operation, it is critical to design a mechanism to effectively coordinate their operations to the area where new information is rich in a completely decentralized and lightweight way. We present a novel agent coordination algorithm for urban sensing environments that has been designed based on biological inspirations such as foraging, stigmergy, and L\'evy flight. The reported simulation results show that the proposed algorithm enables the agents to move to "information patches" where new information concentration is high, and yet limits duplication of work due to simultaneous presence of agents in the same region.' as abstract.

there is a document named 'doc-1160a' that
  has 'Key agreement is a fundamental security functionality by which pairs of nodes agree on shared keys to be used for protecting their pairwise communications. In this work we study key-agreement schemes that are well-suited for the mobile network environment. Specifically, we describe schemes with the following characteristics: _ Non-interactive: any two nodes can compute a unique shared secret key without interaction; _ Identity-based: to compute the shared secret key, each node only needs its own secret key and the identity of its peer; _ Hierarchical: the scheme is decentralized through a hierarchy where intermediate nodes in the hierarchy can derive the secret keys for each of its children without any limitations or prior knowledge on the number of such children or their identities; _ Resilient: the scheme is fully resilient against compromise of any number of leaves in the hierarchy, and of a threshold number of nodes in each of the upper levels of the hierarchy. Several schemes in the literature have three of these four properties, but the schemes in this work are the first to possess all four. This makes them well-suited for environments such as MANETs and tactical networks which are very dynamic, have significant bandwidth and energy constraints, and where many nodes are vulnerable to compromise. We provide rigorous analysis of the proposed schemes and discuss implementations aspects.' as abstract.

there is a document named 'doc-1163a' that
  has 'In both the commercial and defence sectors a compelling need is emerging for the rapid, yet secure, dissemination of information across traditional organisational boundaries. In this paper we present a novel trust management paradigm for securing pan-organisational information flows that aims to address the threat of information leakage. Our trust management system is built around an economic model and a trust-based encryption primitive wherein: (i) entities purchase a key from a Trust Authority (TA) which is bound to a voluntarily reported trust score r, (ii) information flows are encrypted such that a flow tagged with a recipient trust score R can be decrypted by the recipient only if it possesses the key corresponding to a voluntarily reported score r _ R, (iii) the economic model (the price of keys) is set such that a dishonest entity wishing to maximise information leakage is incentivised to report an honest trust score r to the TA. This paper makes two important contributions. First, we quantify fundamental tradeoffs on information flow rate, information leakage rate and error in estimating recipient trust score R. Second, we present a suite of key encryption schemes that realise our trust-based encryption primitive and identify computation and communication tradeoffs between them.' as abstract.

there is a document named 'doc-1164' that
  has 'Discovering the topology of a network and detecting link state changes (e.g.: link failures) is an essential element for various network management and monitoring tasks. In this paper, we investigate scalable mechanisms to monitor the topology and link states of networks based on information available in network nodes\' routing tables. We first present an algorithm that infers the network topology based on the full or partial information about network distances between nodes, based on which we obtain a scalable network topology discovery solution via a novel use of random walk in graphs. We then present scalable algorithms to detect the state changes of remote links by monitoring the routing tables of a small fraction of the routers, where the routers to be monitored are selected by a greedy approach to an NP-complete Tree Cover problem. We show the efficacy and scalability of our topology monitoring algorithms through experimental evaluation performed both on synthetic topologies and on a large topology data-set from a real enterprise network.' as abstract.

there is a document named 'doc-1165' that
  has 'In this paper we examine the Quality of Information (QoI) at the output of a wireless sensor network by considering the difference between the monitored environment and the interpreted data produced by the network. Using practical examples in an experimental setting, we hope to shed light on the concept of QoI and on the manner of estimating and evaluating it. We use a real wireless network in combination with simulated events, to help us formulate and understand the concept of QoI and its associated technical questions. Using algorithms such as trilateration and clustering to interpret the outputs of the sensor network, we explore several definitions of QoI, including the peak signal to noise ratio. Furthermore we investigate the impact that different packet transmission approaches have on the QoI. We show that QoI is time-varying, and that in-network processing allows QoI levels to be maintained while reducing network load.' as abstract.

there is a document named 'doc-1166a' that
  has 'Reliable support of inelastic flows, e.g., video and audio streaming, in wireless ad hoc networks is extremely challenging since flows and routes dynamically change and the flow interactions in the shared wireless channel are difficult to evaluate. Under these circumstances, the traditional Call Acceptance Control (CAC) strategy based on a per flow resource reservation and allocation may cause excessive overhead since nodes must frequently update network available resource information (and flows must refresh their reservations). Instead of resource reservation before flow acceptance, we propose ProbeCast, a probe based CAC scheme for inelastic flows where a new flow first probes the availability of resources. If an intermediate link fails to meet the QoS requirement, the flow is "pushed back" by sending a backpressure message upstream to the source. The backpressure principle is simple, but it works only if the congested link is shared with proportional fairness among inelastic contenders, else a big newcomer can capture the channel. To achieve this, we have developed a distributed fairness scheme, Neighborhood Proportional Drop (N-PROD) which guarantees the same proportional drop rate among all flows competing in the same contention domain. In ProbeCast, each flow has a drop probability threshold. The incoming flow has by design a consistently lower drop probability threshold than the preexisting flows. If during probing the drop probability within a flow increases beyond the threshold, the flow is backpressured. If the bottleneck becomes over allocated and there is no available detour, ProbeCast will reject the incoming flow. This achieves the same effect as traditional CAC without incurring the reservation and allocation overhead. In this paper we demonstrate the efficacy and robustness of ProbeCast for unicast as well as multicast inelastic flows via representative Qualnet simulation case studies.' as abstract.

there is a document named 'doc-1167a' that
  has 'Mobile wireless networks with intermittent connectivity, often called Delay/Disruption Tolerant Networks (DTNs), have recently received a lot of attention because of their applicability in various applications, including multicasting. To overcome intermittent connectivity, DTN routing protocols utilize mobilityassist routing by letting the nodes carry and forward the data. In this paper, we study the scalability of DTN multicast routing. As Gupta and Kumar showed that unicast routing is not scalable, recent reports on multicast routing also showed that the use of a multicast tree results in a poor scaling behavior. However, Grossglauser and Tse showed that in delay tolerant applications, the unicast routing overhead can be relaxed using the two-hop relay routing where a source forwards packets to relay nodes and the relay nodes in turn deliver packets to the destination via "mobility," thus achieving a perfect scaling behavior of _(1). Inspired by this result, we seek to improve the throughput bound of wireless multicast in a delay tolerant setting using mobility-assist routing. To this end, we propose RelayCast, a routing scheme that extends the two-hop relay algorithm in the multicast scenario. Given that there are ns sources each of which is associated with nd random destinations, our results show that RelayCast can achieve the throughput upper bound of _(min(1, n nsnd )). We also analyze the impact of various network parameters and routing strategies (such as buffer size, multi-user diversity among multicast receivers, and delay constraints) on the throughput and delay scaling properties of RelayCast. Finally, we validate our analytical results with a simulation study.' as abstract.

there is a document named 'doc-1170' that
  has 'Task-centric wireless sensor network environments are often characterized by the simultaneous operation of multiple tasks. Individual tasks compete for constrained resources and thus need resource mediation algorithms at two levels. First, different sensors must be allocated to different tasks based on the combination of sensor attributes and task requirements. Subsequently, sensor data rates on various data routes must be dynamically adapted to share the available wireless bandwidth, especially when links experience traffic congestion. In this paper we investigate heuristics for incrementally modifying the sensortask matching process to incorporate changes in the transport capacity constraints or feasible task utility values.' as abstract.

there is a document named 'doc-1172a' that
  has 'We extend the existing network utility maximization (NUM) framework for wired networks to wireless sensor networks by formulating it in order to take into account interference among radio links. We study the conditions under which the formulated problem is a convex optimization problem with a feasible solution. Under such conditions, a distributed algorithm is proposed to solve the problem optimally. Finally, we provide numerical results, based on computer simulations, to show the performance of the proposed algorithm and the rate of convergence of its solution.' as abstract.

there is a document named 'doc-1173' that
  has 'Current and future coalition operations increasingly involve collaboration on operations beyond the traditional battlespace. The challenge is to communicate effectively among multinational teams and to understand each nation\'s developed communication culture. During multinational collaboration, communications are often via electronic networks. This, as a result, removes physical presence and rich context information with the important verbal, behavioural and cultural cues that are often vital to appropriately interpreting the content of the information. In addition, communication preferences, customs, variations in language use and other linguistic and cultural characteristics may create barriers between nations, even without electronic mediation. In this paper, we propose a multidimensional approach, which would capture major aspects of cross-cultural communication and provide a systematic and a comprehensive method for studying communication preferences and peculiarities in the light of cultural differences. More specifically, we propose to analyze data from cross-cultural, cognitive, and linguistic perspectives. Our approach will identify crucial elements involved in cross-cultural communication. Our approach will also discuss overall and individual strategies in collaborating, which can serve as a basis for training to improve multinational communication effectiveness.' as abstract.

there is a document named 'doc-1174' that
  has 'Research efforts to investigate culture in military command and control, or indeed in any form of headquarters, are of crucial importance now that both peacekeeping and warfighting are carried out on a multinational basis. One aspect of working in a coalition headquarters is doing collaborative planning, where the group needs to understand what they as a group have been told to do (i.e., the commander\'s intent) and what their part in the task is. This requires understanding the meaning of the task, and forming enough common ground to be able to coordinate group efforts. Meaning cannot be understood independently from communication, and is also reliant on coordination between both parties. We propose a theory of social sensemaking; that behaviours to create common ground are based on sensemaking strategies, and that specific strategies are used to uncover the knowledge necessary for finding sufficient and necessary common ground.' as abstract.

there is a document named 'doc-1175' that
  has 'In this paper we present an architecture for direction and dissemination in a sensor network.  Information is pushed to recipients that require it in advance of its need.  The decision to push information is made based on mission scripts and descriptions of the roles of the recipients in the missions.  A variety of mechanisms are used to deliver the information within its required time constraints.  We illustrate the use of the system through a previously published vignette.' as abstract.

there is a document named 'doc-1176a' that
  has 'A sensor network in the field is usually required to support multiple sensing tasks or missions to be accomplished simultaneously. Since missions might compete for the exclusive usage of the same sensing resource we need to assign individual sensors to missions. Missions are usually characterized by an uncertain demand for sensing resource capabilities. In this paper we model this assignment problem by introducing the Sensor Utility Maximization (SUM) model, where each sensor-mission pair is associated with a utility offer. Moreover each mission is associated with a priority and with an uncertain utility demand. We also define the benefit or profit that a sensor can bring to a mission as the fraction of mission\'s demand that the sensor is able to satisfy, scaled by the priority of the mission. The goal is to find a sensor assignment that maximizes the total profit, while ensuring that the total utility cumulated by each mission does not exceed its uncertain demand. SUM is NP-Complete and is a special case of the well known Generalized Assignment Problem (GAP), which groups many knapsack-style problems. We compare four algorithms: two previous algorithms for problems related to SUM, an improved implementation of a state-of-the-art pre-existing approximation algorithm for GAP, and a new greedy algorithm. Simulation results show that our greedy algorithm offers the best trade-off between quality of solution and computation cost.' as abstract.

there is a document named 'doc-1177' that
  has 'This paper examines the practical challenges in the application of the distributed network utility maximization (NUM) framework to the problem of resource allocation and sensor device adaptation in a mission-centric wireless sensor network (WSN) environment. By providing rich (multi-modal), real-time information about a variety of (often inaccessible or hostile) operating environments, sensors such as video, acoustic and short-aperture radar enhance the situational awareness of many battlefield missions. Prior work on the applicability of the NUM framework to mission-centric WSNs has focused on tackling the challenges introduced by i) the definition of an individual mission\'s utility as a collective function of multiple sensor flows and ii) the dissemination of an individual sensor\'s data via a multicast tree to multiple consuming missions. However, the practical application and performance of this framework is influenced by several parameters internal to the framework and also by implementation-specific decisions. This is made further complex due to mobile nodes. In this paper, we use discrete-event simulations to study the effects of these parameters on the performance of the protocol in terms of speed of convergence, packet loss, and signaling overhead thereby addressing the challenges posed by wireless interference and node mobility in ad-hoc battlefield scenarios. This study provides better understanding of the issues involved in the practical adaptation of the NUM framework. It also helps identify potential avenues of improvement within the framework and protocol.' as abstract.

there is a document named 'doc-1178' that
  has 'This paper describes initial research in addressing the challenges of managing quality of information for wireless sensor network target tracking with multiple missions of various priorities tracking multiple targets. We address the use of a distributed market-based mechanism to equalize the information value loss (that itself is a function of Quality of Information (QoI)) of tracked targets across multiple tracking missions while managing network congestion arising as a result of tracking. This includes considering missions\' priorities and starvation of missions\' data updates. In support of this approach, we define QoI as a function of the precision of position prediction of a tracked target and the loss of information value as the product of QoI and the priority of the mission tracking the target.' as abstract.

there is a document named 'doc-1179' that
  has 'We consider the security of Identity-Based Encryption (IBE) in the setting of multiple Trusted Authorities (TAs). In this multi-TA setting, we envisage multiple TAs sharing some common parameters, but each TA generating its own master secrets and master public keys. We provide security notions and security models for the multi-TA setting which can be seen as natural extensions of existing notions and models for the single-TA setting. In addition, we study the concept of TA anonymity, which formally models the inability of an adversary to distinguish two ciphertexts corresponding to the same message and identity but generated using different TA master public keys. We argue that this anonymity property is a natural one of importance in enhancing privacy and limiting traffic analysis in multi-TA environments. We study a modified version of a Fujisaki-Okamoto conversion in the multi-TA setting, proving that our modification lifts security and anonymity properties from the CPA to the CCA setting. Finally, we apply these results to study the security of the Boneh-Franklin and Sakai-Kasahara IBE schemes in the multi-TA setting.' as abstract.

there is a document named 'doc-1180' that
  has 'Sensor-mission assignment involves the allocation of sensor and other information-providing resources to missions in order to cover the information needs of the individual tasks in each mission. This is an important problem in the intelligence, surveillance, and reconnaissance (ISR) domain, where sensors are typically over-subscribed, and task requirements change dynamically. This paper approaches the sensor-mission assignment problem from a Semantic Web perspective: the core of the approach is a set of ontologies describing mission tasks, sensors, and deployment platforms. Semantic reasoning is used to recommend collections of types of sensors and platforms that are known to be "fitfor-purpose" for a particular task, during the mission planning process. These recommended solutions are used to constrain a search for available instances of sensors and platforms that can be allocated at mission execution-time to the relevant tasks. An interface to the physical sensor environment allows the instances to be configured to operate as a coherent whole and deliver the necessary data to users. Feedback loops exist throughout, allowing re-planning of the sensor-task fitness, reallocation of instances, and reconfiguration of the sensor network.' as abstract.

there is a document named 'doc-1181a' that
  has 'We propose a firewall architecture that treats port numbers as part of the IP address. Hosts permit connectivity to a service by advertising the IPaddr:port/48 address; they block connectivity by ensuring that there is no route to it. This design, which is especially well-suited to MANETs, provides greater protection against insider attacks than do conventional firewalls, but drops unwanted traffic far earlier than distributed firewalls do.' as abstract.

there is a document named 'doc-1182' that
  has 'Opportunistic forwarding, by which data is randomly relayed to a neighbor based on local network information, is a fault-tolerant distributed algorithm particularly useful for challenged ad hoc and sensor networks where it is difficult to obtain global topology information because of frequent disruptions. Also, duty cycling is a common technique that constrains the RF operations of wireless devices for saving the battery energy and thus extending the longevity of the network. The combination of opportunistic forwarding and duty cycling is a useful approach for wireless ad hoc and sensor networks that are plagued with energy constraints and poor connectivity. However, such a design is hampered by the difficulty of analyzing and controlling its performance, particularly, the end-to-end latency. This paper presents analytical results that shed light on the latency of opportunistic forwarding in wireless networks with duty cycling. In particular, we give approximation formulas and bounds for the expected latency of opportunistic forwarding in presence of duty cycling for general finite network topologies, and an exact formula for a specific regular network topology that captures some common sensor network deployment scenarios. Moreover, our results concern finite-sized networks, and hence, are practically more useful than other asymptotic analyses in the literature.' as abstract.

there is a document named 'doc-1184' that
  has 'Networks composed of mobile nodes inherently suffer from intermittent connections and high delays. Performance can be improved by adding supporting infrastructure, including base stations, meshes, and relays, but the cost-performance trade-offs of different designs is poorly understood. To examine these trade-offs, we have physically deployed a large-scale vehicular network and three infrastructure enhancement alternatives. The results of these deployments demonstrate some of the advantages of each kind of infrastructure; however, these conclusions can be applied only to other networks of similar characteristics, including scale, wireless technologies, and mobility patterns. Thus we complement our deployment with a demonstrably accurate, analytical model of large-scale networks in the presence of infrastructure. Based on our deployment and analysis, we make several fundamental observations about infrastructure-enhanced mobile networks. First, if the average packet delivery delay in a vehicular deployment can be reduced by a factor of two by adding x base stations, the same reduction requires 2x mesh nodes or 5x relays. Given the high cost of deploying base stations, relays or mesh nodes can be a more cost-effective enhancement. Second, we observe that adding small amount of infrastructure is vastly superior to even a large number of mobile nodes capable of routing to one another, obviating the need for mobile-to-mobile disruption tolerant routing schemes.' as abstract.

there is a document named 'doc-1185a' that
  has 'In this paper, we study the impact of cooperative transmission on the routing decision for wireless ad-hoc networks. The influence of cooperative transmission to the wireless link cost is first studied at the physical layer. Then the problem of routing optimization is investigated to understand the effects of improved link cost on the routing decision, where the closed-form solution of the optimization problem is developed and later used as a quantitative criterion of the route selection. Our developed analytical and simulation results show that the criteria using cooperative transmission typically yield more efficient routes compared with the non-cooperative schemes.' as abstract.

there is a document named 'doc-1186' that
  has 'Wireless sensor networks (WSN) can report large volumes of slowly varying routine data, while important or significant events can be relatively rare. An important challenge is then to offer the significant or unusual data an adequate routing policy that will allow it to rapidly reach the sink nodes, despite the large volume of routine packets in the network. In this paper we introduce Randomized Re-Routing (RRR), to detect the unusual events in a distributed manner, and dynamically transfer routine data packets to secondary paths in the network, while offering a fast track path with better QoS for the packets carrying unusual data. In this paper we describe the RRR algorithm and evaluate it with extensive simulations.' as abstract.

there is a document named 'doc-1188a' that
  has 'Recently, risk-based information trading has emerged as a new paradigm for securely sharing information across traditional organizational boundaries. In this paradigm, the risk of sharing information between organizations is characterized using expected losses (due, for example, to (un)intended information disclosure) and billed to a recipient. However, within risk-based information trading systems, quantifying the risks associated with sharing information is a non-trivial task, particularly when risk calculations depend on a number of factors. In this paper we introduce a data-centric metadata framework that extends risk-based information trading approaches by allowing one or more domains to exchange sensitive information based on metadata evaluated against internal risk assessments of the domains. We present a use case of our metadata framework using a coalition military scenario, wherein information flows can be controlled and regulated by our framework whilst allowing sufficiently high-quality tactical information to be disseminated.' as abstract.

there is a document named 'doc-1189a' that
  has 'We consider the problem of energy-efficient data fusion for optimal detection in sensor networks. The spatial correlation structure of sensor measurements is modeled by a Markov random field, where the neighborhood system is defined by means of a nearest-neighbor dependency graph. The data fusion scheme that achieves the best detection accuracy and minimizes the total energy consumption is desired. Two techniques that enhance performance on energy efficiency of an existing data fusion scheme DFMRF are proposed. Simulation results are provided to verify the superiority of the proposed techniques over the original DFMRF in terms of energy efficiency. It is also shown that applying both techniques together is not always the optimal solution.' as abstract.

there is a document named 'doc-1190a' that
  has 'The problem of interest is the detection of transient signals in additive white Gaussian noise (AWGN) in the presence of missing signal observations (samples). Specifically, a fusion center aims at detecting the presence of transient signals by collecting measurements from individual sensors through erasure channels. Under the assumption that the fusion center can control the sampling procedure through a feedback channel, a strategy is proposed to adapt the sampling rate in response to sample missing with the goal of achieving accurate and timely decisions with the minimum communication cost measured by sampling rate. The proposed strategy is flexible in that it can be configured to suit different performance requirements. Compared with fixedrate sampling, the proposed strategy achieves better tradeoff between Quality of Detection (QoD) and communication cost through dynamic adaptation.' as abstract.

there is a document named 'doc-1192a' that
  has 'We propose a representation of imperatives in computational systems, and a multi-agent dialogue protocol to argue over these. Our representation treats a command as a presumptive argument for an action to be executed by a designated agent, together with a set of associated critical questions whose answers may defeat the presumption. The critical questions enable the identification of attacks on the uttered command, and so can be used to specify a dialogue game protocol for participants to argue over the command. We present a formal syntax for part of the protocol, called CDP, and outline denotational semantics for both commands and for the protocol.' as abstract.

there is a document named 'doc-1193a' that
  has 'Inter-domain routing is an important component to allow interoperation among heterogeneous network domains operated by different organizations. Although inter-domain routing has been well supported in the Internet, there has been relatively little support to the Mobile Ad Hoc Networks (MANETs) space. In MANETs, the inter-domain routing problem is challenged by: (1) dynamic network topology due to mobility, and (2) diverse intra-domain ad hoc routing protocols. In this paper, we discuss how to enable inter-domain routing among MANETs, and to handle the dynamic nature of MANETs. We first present the design challenges for inter-domain routing in MANETs, and then propose a framework for inter-domain routing in MANETs.' as abstract.

there is a document named 'doc-1194' that
  has 'Time keeping and synchronization are important services for networked and embedded systems. High quality timing information allows embedded network nodes to provide accurate time-stamping, fast localization, efficient duty cycling schedules, and other basic but essential functions - all of which are required for low power operation. In this paper we present a new type of local clock source called Crystal Compensated Crystal based Timer (XCXT) and a number of novel algorithms that effectively utilize it to achieve low power consumption in wireless sensor networks. The XCXT has timing accuracies similar to timers based on temperature compensated crystal oscillators (TCXO) but has a lower implementation cost and requires less power. Our initial 8MHz prototype unit, using the simplest algorithm, achieves an effective frequency stability of +/-1.2ppm and consumes only 1.27mW. On the other hand, commercially available TCXOs with similar stability can cost over 10 times as much and consume over 20mW. In addition to the prototype, we will present algorithms that will improve the XCXT\'s power consumption by at least 48%, depending on application and environmental conditions. We will also show how XCXT\'s power efficiency can be improved even further by employing clocks at different frequency when different time granularities are required by an application.' as abstract.

there is a document named 'doc-1196a' that
  has 'In opportunistic forwarding, a node randomly relays packets to one of its neighbors based on local information, without the knowledge of global topology. Each intermediate node continues this process until the packet arrives at its destination. This is particularly attractive in certain types of wireless ad hoc and sensor networks where obtaining accurate knowledge of global topology may be infeasible. However, opportunistic forwarding is hampered by the difficulty to control its performance, particularly, the end-to-end latency. This paper presents new analytical results that shed light on the latency of "random walk", the simplest type of opportunistic forwarding, for several useful regular network topologies, such as r-nearest cycle that can model variable wireless transmission distance in one dimensional scenario, and a 2D regular torus-type graph that can approximate grid-like deployments. We give new exact and approximation formulas for the maximum expected hitting time of random walk on such topologies.' as abstract.

there is a document named 'doc-1197' that
  has 'In this paper, we study the utility of opportunistic communication systems with the co-existence of network infrastructure. We study how some important performance metrics change with varying degrees of infrastructure and mobile nodes willing to participate in the opportunistic forwarding. In doing so, we observe phase transitions in the utility of infrastructure and opportunistic forwarding respectively at different points in the design space. We discuss the implications that this has for the design of future network deployments and how this observation can be used to improve network performance, while keeping cost at a minimum.' as abstract.

there is a document named 'doc-1198' that
  has 'Pocket Switched Networks (PSNs) are a kind of Delay Tolerant Network (DTN), which focus of human-to-human communication. In this kind of networks, human mobility determines data delivery opportunity. In this paper, we show that human mobility is well predictable in a day-to-day base and make use of it can much improve forwarding efficiency over oblivious schemes in terms of delivery ratio and delivery cost.' as abstract.

there is a document named 'doc-1199a' that
  has 'In the early days a policy was a set of simple rules with a clear intuitive motivation that could be formalised to good effect. However the world is becoming much more complex. Subtle risk decisions may often need to be made and people are not always adept at expressing rationale for what they do. In this paper we investigate how policies can be inferred automatically using Genetic Programming (GP) from examples of decisions made. This allows us to discover a policy that may not formally have been documented, or else extract an underlying set of requirements by interpreting user decisions to posed "what if" scenarios. Three proof of concept experiments on Bell-LaPadula [1] and Fuzzy MLS policies [2, 3] have been carried out. The results show this approach is promising.' as abstract.

there is a document named 'doc-1201' that
  has 'This paper addresses the problem of specifying and establishing secure collaborations between autonomous entities that need to interact and depend on each other in order to accomplish their goals. We call such collaborations mission-oriented dynamic communities. We propose an abstract model for policy-based collaboration that relies on a set of task-oriented roles. Nodes are discovered dynamically and assigned to one or more roles, and then enforce the policies associated with these roles according to the description of the community. In this paper we describe a basic set of management roles that are needed to provide management and security functions for dynamic communities. We focus on collaborations between nodes in the context of mobile ad-hoc networks.' as abstract.

there is a document named 'doc-1202' that
  has 'Sensor and actuator networks (SANETs) are a growing class of distributed systems combining the reactive functionalities of sensors for environmental sensing and monitoring with the proactive functionalities of actuators for reacting to environmental changes and attempting to control its dynamic processes. As SANETs evolve to accommodate complexity and size of large-scale deployments, each sensor or actuator needs to consider with increasing probability that its attempted task may impact the actions of other sensors and actuators; for example by collectively exceeding the capacity of available resources. Hence, there is a need for new tools that can autonomously coordinate the execution of tasks in the network for the benefit of the entire distributed system. This is especially important for actuators since they affect the environment as opposed to simply monitoring it. In this paper, we advocate the use of market-based methods as the basis for a distributed actuator coordination tool in the embodiment of the Sentire SANET middleware framework. We then demonstrate that the use of auctions for distributed actuator coordination in an HVAC system leads to an efficient, time-dependent, and fair allocation of energy even when the desired temperature cannot be supported in all locations simultaneously. We also discuss how our approach can be generalized to similar SANET problems, including resource allocation. Finally, we outline future research directions and challenges which auction mechanism design faces when applied to SANETs.' as abstract.

there is a document named 'doc-1203a' that
  has 'Many acoustic factors can contribute to the classification accuracy of ground vehicles. Classification based on a single feature set may lose some useful information. To obtain more complete knowledge regarding vehicles\' acoustic characteristics, we propose a fusion approach to combine two sets of features, in which various aspects of an acoustic signature are emphasized individually. The first set of features consists of a number of harmonic components, mainly characterizing engine noise. The second set of features is a group of key frequency components, designated to reflect other minor but also important acoustic factors, such as tire friction noise. To find these features, we apply a harmonic extraction and a mutual information based method that have been shown effective in our previous research. Fusing these two sets of features provides a more complete description of vehicles\' acoustic signatures, and reduces the limitation of relying one particular feature set. Further to a feature level fusion method, we propose a modified Bayesian based fusion method to take advantage of matching each specific feature set with its favored classifier. To assess the proposed fusion method, experiments are carried out based on a multi-category vehicles acoustic data set. Results indicate that the fusion approach can effectively increase the classification accuracy compared to those using each individual set of features. Bayesian based decision level fusion is found to be significantly better than the feature level fusion approach.' as abstract.

there is a document named 'doc-1205' that
  has 'The International Technology Alliance in Network and Information Services <http://usukita.org> is a research consortium of academic, industrial and government researchers jointly funded by the UK Ministry of Defense and US Army Research Laboratory. The ITA is working to expand the technological capabilities of the US Army and UK Armed Forces to give them an information advantage in challenging urban warfare situations that may include humanitarian relief or full combat operations. A fundamental issue in pursuit of this goal is the definition, prediction and measurement of the quality of information available in a tactical sensor networks. The methods developed must support the design process, and effective running of such a network. We have used PEPA to perform a demonstration of the ability to predict information quality as a timedependent probabilistic measure. We are investigating further abstractions to mission-specific utility and value to enable construction of cost and benefit functions to balance command requirements against available resources.' as abstract.

there is a document named 'doc-1206' that
  has 'Position-based routing has proven to be well suited for highly dynamic environment such as Vehicular Ad Hoc Networks (VANET) due to its simplicity. Greedy Perimeter Stateless Routing (GPSR) and Greedy Perimeter Coordinator Routing (GPCR) both use greedy algorithms to forward packet and try to find a route by the right-hand rule in perimeter mode when it encounters a local maximum. These protocols could forward packets efficiently given that the underlying network is fully connected. However, the dynamic nature of vehicular network, such as vehicle density, traffic pattern, and radio obstacles could create unconnected networks partitions. To this end, we propose a hybrid geographic routing solution GeoDTN Nav that exploits the vehicular mobility and on-board vehicular navigation systems to efficiently deliver packets even in partitioned networks. GeoDTN Nav outperforms standard geographic routing protocols such as GPSR and GPCR because it is able to estimate network partitions and then improves partitions reachability by using a storecarry-forward procedure when necessary. We propose a virtual navigation interface (VNI) to provide generalized route information for the delay tolerant forwarding. We finally evaluate the benefit of our approach first analytically and then with simulations. By using delay tolerant forwarding, GeoDTN Nav greatly increases the packet delivery ratio in a sparse network.' as abstract.

there is a document named 'doc-1207' that
  has 'The recent work on COPE by Katti et al. demonstrates a practical application of network coding to wireless multihop networks. We note, however, that the opportunistic nature of COPE leaves it at the mercy of higher and lower layer protocols to create coding opportunities spontaneously. In this paper, we go one step beyond COPE\'s opportunism and study how to create coding opportunities in a more deterministic, yet still practical way. We start from the insight that in two-way traffic the existence of coding opportunities can be guaranteed through carefully co-ordinated packet scheduling, and establish general properties of protocols that are able to achieve this. We then propose Near-Optimal Coordinated Coding (noCoCo), a cross-layer scheme that integrates per-hop packet scheduling, network coding, and congestion control in a novel way. Extensive simulations show that noCoCo significantly outperforms standard non-coding approaches as well as COPE in terms of network throughput, delay and transmission overhead.' as abstract.

there is a document named 'doc-1209' that
  has 'In this paper, we propose a novel duty cycling algorithm for a large-scale dense wireless sensor networks. The proposed algorithm is based on a social behavior of nodes in the sense that individual node\'s sleep/wakeup decision is influenced by the state of its neighbors. We analyze the behavior of the proposed duty cycling algorithm using a stochastic spatial process. In particular, we consider a geometric form of neighborhood dependence and a reversible Markov chain, and apply this model to analyze the behavior of the duty cycling network. We then identify a set of parameters for the reversible spatial process model, and study the steady state of the network with respect to these parameters. We report that our algorithm is scalable to a large network, and can effectively control the active node density while achieving a small variance. We also report that the social behavior of nodes has interesting and non-obvious impacts on the performance of duty cycling. Finally, we present how to set the parameters of the algorithm to obtain a desirable duty cycling behavior.' as abstract.

there is a document named 'doc-1210' that
  has 'We study the following problem. Given a weighted planar graph G, assign labels L(v) to vertices so that given L(u), L(v) and L(x) for x _ X for any X _ V (G), compute the distance dG\\X(u, v). We show how to construct in polynomial time such a labeling with labels of Oe(k) bits1 for every n-vertex planar graph of treewidth k, which is Oe(n 1/2 ) for general planar graphs. Our scheme also gives a compact routing scheme using labels of the same size. This improves the previous Oe(k 2 ) bound for treewidth-k graphs [7]. Surprisingly, this matches the best-known bound for static (X = _) distance labeling in planar graphs, and is optimal to within polylogarithmic factors.' as abstract.

there is a document named 'doc-1211a' that
  has 'In the early days a policy was a set of simple rules with a clear intuitive motivation that could be formalised to good effect. However the world is now much more complex. Subtle risk decisions may often need to be made and people are not always adept at expressing rationale for what they do. Previous research has demonstrated that Genetic Programming can be used to infer statements of policies from examples of decisions made [1]. This allows a policy that may not formally have been documented to be discovered automatically, or an underlying set of requirements to be extracted by interpreting user decisions to posed "what if" scenarios. This study compares the performance of three different approaches in using Genetic Programming to infer security policies from decision examples made, namely symbolic regression, IF-THEN rules inference and fuzzy membership functions inference. The fuzzy membership functions inference approach is found to have the best performance in terms of accuracy. Also, the fuzzification and de-fuzzification methods are found to be strongly correlated; incompatibility between them can have strong negative impact to the performance.' as abstract.

there is a document named 'doc-1212' that
  has 'Traditional policies often focus on access control requirement and there have been several proposals to define access control policy algebras to handle their compositions. Recently, obligations are increasingly being expressed as part of security policies. However, the compositions and interactions between these two have not yet been studied adequately. In this paper, we propose an algebra capturing both authorization and obligation policies. The algebra consists of two policy constants and six basic operations. It provides language independent mechanisms to manage policies. As a concrete example, we instantiate the algebra for the Ponder2 policy language.' as abstract.

there is a document named 'doc-1213a' that
  has 'This paper addresses the problem of plan recognition for multi-agent teams. Complex multi-agent tasks typically require dynamic teams where the team membership changes over time. Teams split into subteams to work in parallel, merge with other teams to tackle more demanding tasks, and disband when plans are completed. We introduce a new multi-agent plan representation that explicitly encodes dynamic team membership and demonstrate the suitability of this formalism for plan recognition. From our multi-agent plan representation, we extract local temporal dependencies that dramatically prune the hypothesis set of potentially-valid team plans. The reduced plan library can be efficiently processed to obtain the team state history. Naive pruning can be inadvisable when low-level observations are unreliable due to sensor noise and classification errors. In such conditions, we eschew pruning in favor of prioritization and show how our scheme can be extended to rank-order the hypotheses. Experiments show that this robust pre-processing approach ranks the correct plan within the top 10%, even under conditions of severe noise.' as abstract.

there is a document named 'doc-1215' that
  has 'A sensor network in the field is usually required to support multiple sensing tasks or missions to be accomplished simultaneously. Since missions might compete for the exclusive usage of the same sensing resource we need to assign individual sensors to missions. Missions are usually characterized by an uncertain demand for sensing resource capabilities. Consider for example a mission that requires video sensors to identify a target but the weather conditions and visibility range in the field are not exactly known: in this case the required number of sensors and the resolution of their cameras cannot be precisely determined. We can for example specify only the highest resolution of the cameras needed by the mission or the maximum number of video sensors required. If instead two missions require to identify two different targets that are located in nearby regions on the map, then these missions might compete for the exclusive control of a particular video sensor. Indeed the mission to which the video sensor will be assigned might decide to point the camera in a direction that could be completely opposite to where the other mission would require it.' as abstract.

there is a document named 'doc-1216' that
  has 'Besides safe navigation (e.g., warning of approaching vehicles), car to car communications will enable a host of new applications, ranging from office-on-the-wheel support to entertainment. One of the most promising applications is content distribution among drivers such as multi-media files and software updates. Content distribution in vehicular networks is a challenge due to network dynamics and high mobility, yet network coding was shown to efficiently handle such dynamics and to considerably enhance performance. This paper provides an in-depth analysis of implementation issues of network coding in vehicular networks. To this end, we consider general resource constraints (e.g., CPU, disk, memory) besides bandwidth, that are likely to impact the encoding and storage management operations required by network coding. We develop an abstract model of the network coding procedures and implement it in the wireless network simulator to evaluate the impact of limited resources. We then propose schemes that considerably improve the use of such resources. Our model and extensive simulation results show that network coding parameters must be carefully configured by taking resource constraints into account.' as abstract.

there is a document named 'doc-1217a' that
  has 'Policy is a key component in the interoperations among multiple heterogeneous domains of networks with different routing metrics and preferences. IDRM is a recent protocol that enables policy-based inter-domain routing over mobile ad hoc networks (MANETs), supporting dynamic network topology and diverse intra-domain routing protocols. Notwithstanding a protocol to interoperate multidomain MANETs, there are fundamental challenges to support dynamic networks - how can network administrators formulate practical inter-domain routing policies considering MANET-specific characteristics. This paper studies this issue with illustrating examples, and discusses potential solutions.' as abstract.

there is a document named 'doc-1218a' that
  has 'Research into the retrieval and dissemination of mission-specific information across sensor networks is leading to the development of many novel new algorithms and the definition of new paradigms for configuring and enforcing communication flow policies amongst the various components. Testing the relative merits of such algorithms and policies, and exploring interoperability issues between them, is difficult unless they share a common test and validation framework. The wide variety of algorithms being developed (including ontological, mission scripting, resource allocation, network routing, data fusion algorithms, and policies) presents unique challenges to the development of such a framework and its subsequent instrumentation to provide experimental results. This work demonstrates a prototype of such framework (or "Fabric"), built on top of commercial off-the-shelf (COTS) components, used in a real-world sensor network deployment to experiment with the deployment of algorithms and gathering of live sensor data.' as abstract.

there is a document named 'doc-1220a' that
  has 'In sensor networks applied to monitoring applications, individual sensors may perform preassigned or on-demand tasks, or missions. Data updates (info-pages) may be sent to sensors from a command center, via a time-division broadcast channel. Sensors are normally put in sleep mode when not actively listening, in order to conserve energy in their batteries. Hence, a schedule is required that specifies when sensors should listen for updates and when they should sleep. The performance of such a schedule is evaluated based on data-related costs and sensor-related costs. Data-related costs reflect the obsoleteness of current sensor data, or the delay while sensors wait for updated instructions. Sensor-related costs reflect the energy that sensors consume while accessing the broadcast channel and while switching between the active and sleeping modes (rebooting). Our goal is a schedule with the minimum total cost. Previous related work has explored data-related costs, but listening cost has been addressed only under the assumption that the rebooting operation is free. This paper formulates a new cost model, which recognizes the cost of sensor rebooting. We derive an optimal schedule for the single-sensor setting. We proceed to consider schedules of multiple sensors, and formulate a mathematical program to find an optimal fractional schedule for this setting. Several heuristics for scheduling multiple sensors are introduced and analyzed, and various tradeoffs among the cost factors are demonstrated.' as abstract.

there is a document named 'doc-1221' that
  has 'When a sensor network is deployed in the field, such as in monitoring applications, it is typically required to support multiple simultaneous missions, which may start and finish at different times. Schemes that match sensor resources to mission demands thus become necessary. In this paper, we consider new sensor-assignment problems motivated by frugality, i.e., the conservation of resources, for both static and dynamic settings. In general, the problems we study are NP-hard even to approximate, and so we focus on heuristic algorithms that perform well in practice. For some interesting constrained problems settings, however, we give optimal or guaranteed approximation algorithms. Some of our algorithms can be run in a distributed fashion. For the NP-hard problem settings, we implement our approximation algorithms and test them on randomly generated data. In the online setting, available sensors propose to nearby missions, which then decide which proposals to accept. We find that overall performance can be significantly improved if available sensors sometimes refuse to offer utility to missions they could help. The decision of whether to make the offer is based on the value of the mission, the sensor\'s remaining battery level, and (if known) the remaining lifetime of the sensor network.' as abstract.

there is a document named 'doc-1222' that
  has 'Development of innovative and effective technologies to support complex collaborative human activities depends both on observation and analysis of these activities and on iterative development and evaluation of the technologies in the deployment context. Applying these principles to military mission planning and execution is challenging because actual warfare is dangerous for both participants and observers. We use the simulation engine of a computer-based game to simulate warfare, to recognize opportunities for technological intervention, and for preliminary testing of potential technologies. We developed a simulation environment based on the commercial warfare game Battlefield 2, and we use this environment to observe and analyse military behaviour and to evaluate how it is affected by technological interventions such as software agents. In this paper we describe the environment and illustrate how it can enhance our understanding of military activities and support our investigations of adaptive mission planning as a means of achieving rapid adaptation to changing circumstances in a battlefield. Our research explores methods for interpreting the collaborative behaviour of simulation participants and constructing dynamic models of the mission context. We investigate how a planning agent can actively support participants by comparing the current context to the mission plans and objectives and suggesting actions that will facilitate participants\' progress. We also describe how the environment can be extended to simulate the effects of both sensor and communication networks in military operations. Finally, the use of computer-based games and community-based development of additions to this environment provides a sharp contrast to more traditional methods used by military organizations for developing systems, sensors and agents to aid their war fighters. We compare the two approaches and indicate where the new methods have a place in military thinking.' as abstract.

there is a document named 'doc-1224a' that
  has 'Connectivity and capacity are two measures for the performance of mobile ad hoc networks that have been studied extensively under standard point-to-point physical layer assumptions. However, extensive recent research at the physical layer has demonstrated the improvement in performance possible when multiple radios concurrently transmit in the same radio channel. In this paper, we consider how such physical layer cooperation improves the connectivity in wireless ad hoc networks. In particular, with noncoherent cooperation at the physical layer, we consider conditions on the node density _ (or, equivalently, the transmit power) for full connectivity and percolation for large networks in various dimensions and with various path loss exponents _. For one-dimensional (1-D) extended networks, in sharp contrast to noncooperative networks, we demonstrate that full connectivity can be realized under certain conditions. In particular, for any node density with _ < 1, or for node density _ > 2 when _ = 1, full connectivity occurs with probability one. Conversely, we demonstrate that, under noncoherent cooperation, there is no full connectivity with probability one when _ > 1. In two-dimensional (2-D) extended networks with noncoherent cooperation, for any node density with _ < 2, or for node density _ > 5 when _ = 2, full connectivity is achieved. Conversely, there is no full connectivity with probability one when _ > 2, but we prove that, for _ 6 4, the percolation threshold of the noncoherent cooperative network is strictly less than that of the noncooperative network. Analogous results are presented for dense networks. Hence, the main conclusion is that even relatively simple physical layer cooperation in the form of noncoherent power summing can substantially improve the connectivity of large ad hoc networks.' as abstract.

there is a document named 'doc-1226a' that
  has 'In this work we consider the problem of routing in networks with time-varying links. We focus on wireless and mobile ad-hoc networks where link changes occur at the timescale of seconds to minutes. Since links are changing, to maintain network connectivity paths must be periodically recomputed, incurring increased control overhead. Suppose, however, we can perform routing in such a way that paths are "robust" to link changes in the network: i.e., link changes may occur, but paths still perform well enough. Then paths can be recomputed less frequently and the control overhead decreased. In this work we specifically examine the problem of how to perform such "robust" routing.' as abstract.

there is a document named 'doc-1227' that
  has 'Wireless sensor networks consist of a large number of sensor nodes, each of which senses, computes and communicates with other nodes to collect and process data about the environment. Those networks are emerging as one of the new paradigms in networking with great impact on industry, government and military applications. A sensor network attempts to collect sensing data from the entire domain of its deployment, to process this data to understand phenomena and activities going on in this domain, and finally to communicate the results to the outside world to enable actuators to execute the necessary reactions. However, a sensor node is only capable of sensing events within its limited sensing range, so it has only a localized information about its environment. Hence, to provide the coverage of the entire domain, sensors need to collaborate and share their information with each other. Such sharing increases the knowledge of each sensor about the environment, but it also brings extra communication cost and increases the network operation complexity. In other words, cooperation and data sharing invokes a cost-quality tradeoff in the network. In this paper, we study two different sensor network applications: (i) finding an efficient sleep schedule based on sensing coverage redundancy, and (ii) adjusting traffic light periods to optimize traffic flow. In both applications the cost-quality tradeoff arises. In the paper, we study how fast network functionality increases when the level of cooperation raises and how much this increased functionality is offset by the raising cooperation costs. We simulated both applications with different level of cooperation and without it and demonstrated significant improvements in the overall system quality resulting from the properly selected levels of cooperation between the network\'s nodes.' as abstract.

there is a document named 'doc-1228a' that
  has 'This paper addresses the problem of plan recognition for multiagent teams. Complex multi-agent tasks typically require dynamic teams where the team membership changes over time. Teams split into subteams to work in parallel, merge with other teams to tackle more demanding tasks, and disband when plans are completed. We introduce a new multi-agent plan representation that explicitly encodes dynamic team membership and demonstrate the suitability of this formalism for plan recognition. From our multi-agent plan representation, we extract local temporal dependencies that dramatically prune the hypothesis set of potentially-valid team plans. The reduced plan library can be efficiently processed to obtain the team state history. Naive pruning can be inadvisable when low-level observations are unreliable due to sensor noise and classification errors. In such conditions, we eschew pruning in favor of prioritization and show how our scheme can be extended to rank-order the hypotheses. Experiments show that this robust pre-processing approach ranks the correct plan within the top 10%, even under conditions of severe noise.' as abstract.

there is a document named 'doc-1229' that
  has 'In this paper, a quality-of-service driven routing protocol is proposed for wireless cooperative networks. The key contribution of the proposed protocol is to bring the performance gain of cooperative diversity from the physical layer up to the networking layer. Specifically, the proposed protocol uses a distributed algorithm to select the best relays based on link quality to form cooperative links for establishing a route with appropriate error performance from a source to a destination node. Furthermore, analytical results are developed to show that the proposed distributed routing protocol can perform close to the optimal in terms of error performance, especially for linear network topologies. Monte-Carlo simulation results are also provided for performance evaluation.' as abstract.

there is a document named 'doc-1230' that
  has 'We define a vertex labelling for every planar 3-connected graph with n vertices from which one can answer connectivity queries. A connectivity query asks whether there exists in the given graph a path linking u and v that avoids a set F of edges and a set X of vertices. The vertices u,v and those of X are given by their labels. The edges of F are given by the labels of their ends. Each label has a size of O(log(n)) bits. Our construction makes an essential use of straight-line embeddings on n _ n grids of simple loop-free planar graphs. Such embeddings can be constructed in linear time by Schnyder\'s algorithm [7].' as abstract.

there is a document named 'doc-1232' that
  has 'We study blind estimation of transmission power of a node based on received power measurements obtained under wireless fading. Specifically, the setup consists of a set of monitors that measure the signal power received from the transmitter, and the goal is to utilize these measurements to estimate the transmission power in the absence of any prior knowledge of the transmitter\'s location or any statistical distribution of its power. Towards this end, we exploit spatial diversity in received-power measurements and cooperation among the multiple monitoring nodes; based on theoretical analysis we obtain the Maximum Likelihood (ML) estimate, derive fundamental geometrical insights and show that this estimate is asymptotically optimal. Finally, we provide numerical results comparing the performance of the estimators through simulations and on a data-set of field measurements.' as abstract.

there is a document named 'doc-1233a' that
  has 'We are investigating computing platform-independent policy frameworks to specify, analyze, and deploy security and networking policies. The goal is to provide easy to use mechanisms for refining high-level user-specified goals into low-level controls. This scenario-based demo of a Coalition Policy Management Portal prototype uses the context of a hostage rescue situation to demonstrate usable and effective policy authoring through either natural language or structured lists that create natural language policy rules; policy visualization; analysis of policies for conflict, dominance, and coverage, and methods to resolve the issues identified; policy transformation from natural language to XML or ACPL SPL for automated enforcement, and deployment of policies onto mission equipment. The prototype builds on the SPARCLE and PONDER2 research projects.' as abstract.

there is a document named 'doc-1235' that
  has 'Target tracking is a typical and important cooperative sensing application of wireless sensor networks. We study it in its most basic form, assuming the binary sensing model in which each sensor can return only 1-bit information regarding target\'s presence or absence within its sensing range. A novel, real-time and distributed target tracking algorithm is proposed. The algorithm reduces the uncertainty of the target location from a two-dimensional area into a one-dimensional arc and estimates the target velocity and trajectory in a distributed and asynchronous manner. Extensive simulations show that our algorithm achieves good performance by yielding highly accurate estimates of the target\'s location, velocity and trajectory.' as abstract.

there is a document named 'doc-1237b' that
  has 'A method to determine entry points and paths of DDoS attack traffic flows into network domains is proposed. We determine valid source addresses seen by routers from sampled traffic under non-attack conditions. Under attack conditions, we detect route anomalies by determining which routers have been used for unknown source addresses to construct the attack paths. We show results from simulations to detect the routers carrying attack traffic in the victim\'s network domain. Our approach is non-intrusive, not requiring any changes to the Internet routers and data packets. Precise information regarding the attack is not required allowing a wide variety of DDoS attack detection techniques to be used. The victim is also relieved from the traceback task during an attack. Our algorithm is simple and efficient, allowing for a fast traceback and the method is scalable due to the distribution of processing workload.' as abstract.

there is a document named 'doc-1239' that
  has 'As described by Major General Scales (Marine Corp., Ret.), the U.S. military\'s Achilles heel is knowledge of the enemy, and technology is no replacement for understanding the enemy\'s mind. This extends to understanding civilians, as well, since winning their "hearts and minds" or influencing their attitudes and psychological states will play an important role in the wars we can expect over the coming decades.  Influence is a key aspect of information operations (IO), where the goal is to affect enemy and other decision makers to achieve specific objectives.  Also, the purpose of PSYOP as a component of IO is to influence the attitudes and behavior of foreign governments, organizations, groups, and individuals.  Furthermore, past research has identified the characterization of target audiences as one of the most critical and challenging aspects of IO (Sieck, Stevens, & Shafer, 2004).' as abstract.

there is a document named 'doc-1240a' that
  has 'Query formulation is a key aspect of information retrieval, contributing to both the efficiency and usability of many semantic applications. A number of query languages, such as SPARQL, have been developed for the Semantic Web; however, there are, as yet, few tools to support end users with respect to the creation and editing of semantic queries. In this paper we introduce a graphical tool for semantic query construction (NITELIGHT) that is based on the SPARQL query language specification. The tool supports end users by providing a set of graphical notations that represent semantic query language constructs. This language provides a visual query language counterpart to SPARQL that we call vSPARQL. NITELIGHT also provides an interactive graphical editing environment that combines ontology navigation capabilities with graphical query visualization techniques. This paper describes the functionality and user interaction features of the NITELIGHT tool based on our work to date. We also present details of the vSPARQL constructs used to support the graphical representation of SPARQL queries.' as abstract.

there is a document named 'doc-1241b' that
  has 'This paper presents the results of a research study applying a new cultural analysis method to capture commonalities and differences between US and UK mental models of operational planning. The results demonstrate the existence of fundamental differences between the way US and UK planners think about what it means to have a high quality plan. Specifically, the present study captures differences in how US and UK planners conceptualize plan quality. Explicit models of cultural differences in conceptions of plan quality are useful for establishing performance metrics for multinational planning teams. This paper discusses the prospects of enabling automatic evaluation of multinational team performance by combining recent advances in cultural modelling with enhanced ontology languages.' as abstract.

there is a document named 'doc-1242' that
  has 'Current major military deployments almost always involve collaboration between multinational teams. Joint operations often face operationally and environmentally complex and dynamic scenarios. Effective and efficient communication is a key enabler to success; however, the diverse backgrounds of multinational teams have presented serious challenges in coalition communication. This on-going exploratory study investigates miscommunications between US and UK military personnel (and civilians working with them). It focuses on understanding miscommunication due to differences in language forms and language use, including the context of use. Based on the data collected from a set of semi-structured interviews of military personnel, this study identified a number of categories and patterns of miscommunication. The preliminary results have presented a number of implications for improving communication between UK and US teams, which can serve as insights for improving multinational team communication in general.' as abstract.

there is a document named 'doc-1243a' that
  has 'We consider the use of threshold signatures in ad-hoc and dynamic groups such as MANETs ("mobile ad-hoc networks"). While the known threshold RSA signature schemes have several properties that make them good candidates for deployment in these scenarios, none of these schemes seems practical enough for realistic use in these highly-constrained environments. In particular, this is the case of the most efficient of these threshold RSA schemes, namely, the one due to Shoup. Our contribution is in presenting variants of Shoup\'s protocol that overcome the limitations that make the original protocol unsuitable for dynamic groups. The resultant schemes provide the efficiency and flexibility needed in ad-hoc groups, and add the capability of incorporating new members (share-holders) to the group of potential signers without relying on central authorities. Namely, any threshold of existing members can cooperate to add a new member. The schemes are efficient, fully non-interactive and do not assume broadcast.' as abstract.

there is a document named 'doc-1244' that
  has 'We argue that the traditional notion of trust as a relation among entities, while useful, becomes insufficient for emerging data-centric mobile ad hoc networks. In these systems, setting the data trust level equal to the trust level of the dataproviding entity would ignore system salient features, rendering applications ineffective and systems inflexible. This would be even more so if their operation is ephemeral, i.e., characterized by short-lived associations in volatile environments. In this paper, we address this challenge by extending the traditional notion of trust to data-centric trust: trustworthiness attributed to nodereported data per se. We propose a framework for data-centric trust establishment: First, trust in each individual piece of data is computed; then multiple, related but possibly contradictory, data are combined; finally, their validity is inferred by a decision component based on one of several evidence evaluation techniques. We consider and evaluate an instantiation of our framework in vehicular networks as a case study. Our simulation results show that our scheme is highly resilient to attackers and converges stably to the correct decision.' as abstract.

there is a document named 'doc-1246a' that
  has 'The engineering of heterogenous distributed systems is a complex task. Traditional software engineering methods fail to account for new demands of flexibility and adaptability in the construction of software systems. On the other hand, concepts of Virtual Organizations and Electronic Institutions cater for the need of open, heterogenous software environments, where agents may dynamically organize themselves into organizational structures, determined by roles, norms and contracts. Our work aims to facilitate the engineering of heterogenous and distributed systems by providing only a specification of the desired overall system behavior, expressed as a set of norms, and rely on capabilities and properties of individual agents that allow them to dynamically form the desired complete software system. In particular, we present a framework, called Requirement-driven Contracting (RdC), for automatically deriving executable norms from requirements and associated relevant information. RdC facilitates the governance of MAS by ensuring that all requirements, along with runtime changes of requirements are appropriately and automatically reflected in the norms regulating the behavior of MAS.' as abstract.

there is a document named 'doc-1247' that
  has 'Coalition forces are engaged in distributed collaborative decision making in time-pressured, high-stakes situations. Providing automated decision support for such environments is a very challenging problem, due to shortening decision cycles, the changing nature of threats, opponent tactics, and environmental unpredictability. Intelligent agents have the promise to provide timely assistance in various areas of decentralized, collaborative decision making, such as information gathering, information dissemination, monitoring of team progress and alerting the team to various unexpected events. In order to fulfil the promise of agent technology in providing effective team assistance, better understanding of robust human-agent teamwork is crucial. The goal of our research project is to develop a theoretically grounded and empirically tested framework to allow for effective agent support for human teams that are engaged in adaptive teamwork in dynamic environments. In order to (a) establish an experimental baseline of the performance of human-only teams and (b) understand where agents can provide the best utility in supporting human teamwork, we designed a scenario and experimentally evaluated team work where human teams performed a time-stressed, collaborative search task in a multi-player gaming environment. The collaborative search task recreates some of the challenges faced by human teams during search and rescue operations in the real world. In our experiments, we analyze (1) verbal communication between team members and (2) team coverage patterns. By ascertaining the information processing and coordination requirements of this team task, we can identify ``insertion points\'\' for agent assistance to human teams. The search patterns demonstrated by the experimental subjects exhibited similar problems to the behavior of actual search and rescue teams: (1) the creation of accidental holes in the search pattern due to poor execution of the search plan, and (2) poor priority assignments in the search plan due to false clues and hunches. We have identified that this is a promising area for agent assistance. By having agents monitor and track individual team members\' coverage, gaps in the team coverage are exposed earlier in the search process allowing repairs to be made in a more timely fashion. Our model predicts that aiding the state of coordination between team members will result in task performance improvement.' as abstract.

there is a document named 'doc-1249' that
  has 'Transmit-power estimation is an important part in power-aware designs of Mobile Ad-hoc Networks (MANETs). In this paper, we consider the cooperation among multiple monitor-nodes to estimate the transmit power of other nodes. Utilizing a geometric approach, we characterize the theoretical performance of such cooperative monitoring schemes and propose transmit-power estimation techniques with different number of cooperating nodes. We introduce the novel concept of confidence region that provides a fundamental confidence level for the accuracy of the power estimation and enables the development of techniques for allocating network monitors. Finally, we present a simple, distributed cooperative estimation scheme for a large-scale wireless network and give illustrative simulation results to quantify its performance.' as abstract.

there is a document named 'doc-1250' that
  has 'This paper examines the practical challenges in the application of the distributed network utility maximization (NUM) framework to the problem of resource allocation and sensor device adaptation in a mission-centric wireless sensor network (WSN) environment. By providing rich (multi-modal), real-time information about a variety of (often inaccessible or hostile) operating environments, sensors such as video, acoustic and short-aperture radar enhance the situational awareness of many battlefield missions. Prior work on the applicability of the NUM framework to mission-centric WSNs has focused on tackling the challenges introduced by i) the definition of an individual mission\'s utility as a collective function of multiple sensor flows and ii) the dissemination of an individual sensor\'s data via a multicast tree to multiple consuming missions. However, the practical application and performance of this framework is influenced by several parameters internal to the framework and also by implementation-specific decisions. This is made further complex due to mobile nodes. In this paper, we use discrete-event simulations to study the effects of these parameters on the performance of the protocol in terms of speed of convergence, packet loss, and signaling overhead thereby addressing the challenges posed by wireless interference and node mobility in ad-hoc battlefield scenarios. This study provides better understanding of the issues involved in the practical adaptation of the NUM framework. It also helps identify potential avenues of improvement within the framework and protocol.' as abstract.

there is a document named 'doc-1251a' that
  has 'Making decisions on how best to utilise limited intelligence, surveillance and reconnaisance (ISR) resources is a key issue in mission planning. This requires judgements about which kinds of available sensors are more or less appropriate for specific ISR tasks in a mission. A methodological approach to addressing this kind of decision problem in the military context is the Missions and Means Framework (MMF), which provides a structured way to analyse a mission in terms of tasks, and assess the effectiveness of various means for accomplishing those tasks. Moreover, the problem can be defined as knowledge-based matchmaking: matching the ISR requirements of tasks to the ISR-providing capabilities of available sensors. In this paper we show how the MMF can be represented formally as an ontology (that is, a specification of a conceptualisation); we also represent knowledge about ISR requirements and sensors, and then use automated reasoning to solve the matchmaking problem. We adopt the Semantic Web approach and the Web Ontology Language (OWL), allowing us to import elements of existing sensor knowledge bases. Our core ontologies use the description logic subset of OWL, providing efficient reasoning. We describe a prototype tool as a proof-of-concept for our approach. We discuss the various kinds of possible sensor-mission matches, both exact and inexact, and how the tool helps mission planners consider alternative choices of sensors.' as abstract.

there is a document named 'doc-1252' that
  has 'One of the challenges in military wireless sensor networks is the determination of an information collection infrastructure that minimizes battery power consumption while being highly resilient against sensor and link failures. In our previous work we have proposed a heuristic for constructing an information flow graph in wireless sensor networks based on the mammalian circulatory system, with the goal of minimizing the energy consumption. In this paper we focus mainly on the resilience benefits that can be achieved when constructing such information flow graphs. We analyze the resilience of circulatory graphs constructed on top of regular as well as random topologies. We assume two modes of failure, random and targeted attacks, and we compare the resilience of the circulatory graphs against tree graphs.' as abstract.

there is a document named 'doc-1253' that
  has 'Mobile Ad-Hoc Networks (MANETs), that do not rely on pre-existing infrastructure and that can adapt rapidly to changes in their environment, are coming into increasingly wide use in military applications. At the same time, the large computing power and memory available today even for small, mobile devices, allows us to build extremely large, sophisticated and complex networks. Such networks, however, and the software controlling them are potentially vulnerable to catastrophic failures because of their size and complexity. Biological networks have many of these same characteristics and are potentially subject to the same problems. But in successful organisms, these biological networks do in fact function well so that the organism can survive. In this paper, we present a MANET architecture developed based on a feature, called homeostasis, widely observed in biological networks but not ordinarily seen in computer networks. This feature allows the network to switch to an alternate mode of operation under stress or attack and then return to the original mode of operation after the problem has been resolved. We explore the potential benefits such an architecture has, principally in terms of the ability to survive radical changes in its environment using an illustrative example.' as abstract.

there is a document named 'doc-1255' that
  has 'In a typical military application, a wireless sensor network will operate in difficult and dynamic conditions. Communication will be affected by local conditions, platform characteristics and power consumption constraints, and sensors may be lost during an engagement. It is clearly of great importance to decision makers to know what quality of information they can expect from a network in battlefield situations. We propose the development of a supporting technology founded in formal modeling, using stochastic process algebras for the development of quality of information measures. A simple example illustrates the central themes of outcome probability distribution prediction, and time-dependency analysis.' as abstract.

there is a document named 'doc-1257' that
  has 'This paper describes a method for assignment and deployment of sensors in a virtual environment using constraint programming. We extend an existing model (multiple knapsack problem) to implement this assignment and placement, according to a given set of requirements (modelled as a utility extension).' as abstract.

there is a document named 'doc-1259a' that
  has 'In this paper, we analyze the stateless SYNSYN&ACK and SYN-FIN/RST detection mechanisms for TCP SYN attacks. We indicate the inherent vulnerability of the SYN-FIN/RST detection mechanism caused by the computation of the RST packet counts. We indicate why SYNSYN&ACK is a more efficient and reliable detection mechanism than SYN-FIN/RST. We come up with \'Bot Buddies\' for TCP SYN attacks and explain how the use of them can compromise both mechanisms. We propose an enhanced detection mechanism incorporating the Bloom filter to handle these variations of TCP SYN attacks. We show that our enhanced mechanism overcomes the problems of the use of Bot Buddies and analyse its efficiency.' as abstract.

there is a document named 'doc-1260a' that
  has 'This paper presents a mission management architecture and a policy-based methodology for specifying and assigning missions to be accomplished by a secure Community of Interest (CoI). By using policies and roles to specify missions we enable dynamic mission assignment and adaptation. The specific entities forming the community may not be known in advance, so they have to be discovered, authenticated and assigned to roles. Policies are needed to define what services and resources a role can use from another role; what information can be exchanged, and the protocols to be used for communication; and, how to deal with heterogeneity of communications, security mechanisms, etc' as abstract.

there is a document named 'doc-1262a' that
  has 'We consider a class of geometric coverage problems in which the objects used to cover the region of interest are, because of practical difficulties, positioned with only approximate accuracy. This changes the character of some coverage problems that solve for optimal disk positions or disk sizes, ordinarily assuming the disks can be placed precisely in their chosen positions, and motivates new problems. These include guaranteed and probabilistic coverage of a region using few inexactly placed unit disks, maximizing the sum of the allowable placement errors given a fixed set of disk positions, and choosing error bounds that satisfy max-min fairness, which we do in O(n log n) time. The allowable placement areas need not be disks, however; we conclude with a general method of constructing them.' as abstract.

there is a document named 'doc-1263' that
  has 'The quality of information (QoI) that sensor networks provide to the applications they support is an important design goal for their deployment and use. In this paper, we introduce a layered framework for QoI-centered evaluation of sensor network deployment. The layered framework allows decomposing the deployment evaluation in three steps: input pre-processing, core analysis, and result post-processing. The layering allows the creation of a rich, modular toolkit for QoI-centered analysis that can accommodate both existing and new system modeling and analysis techniques. We demonstrate the utility of the framework by comparing the QoI performance of finite-sized sensor networks with general deployment topology. We also derive some new analysis results for the class of applications considered herein.' as abstract.

there is a document named 'doc-1264b' that
  has 'The capacity gain of network coding has been extensively studied in wired and wireless networks. Recently, it has been shown that network coding improves network reliability by reducing the number of packet retransmissions in lossy networks. However, the extent of the reliability benefit of network coding is not known. This paper quantifies the reliability gain of network coding for reliable multicasting in a wireless network where network coding is the most promising. We define the expected number of transmissions per packet as the performance metric for reliability and derive analytical expressions characterizing the performance of network coding. For a tree-based multicast, we derive expressions for the expected number of transmissions at the source of the multicast and inside the multicast tree. We also analyze the performance of error control mechanisms based on rateless codes and automatic repeat request (ARQ). We then use the analytical expressions to study the impact of multicast group size on the performance of different error control schemes. Our numerical results show that network coding significantly reduces the number of retransmissions in lossy networks compared to end-to-end ARQ scheme, however, rateless coding and link-by-link ARQ are able to achieve performance results comparable to that of network coding. Interestingly, link-by-link ARQ can outperform rateless coding depending on the network size and loss probability. We conjecture that network coding achieves a logarithmic reliability gain with respect to multicast group size compared to a simple ARQ scheme.' as abstract.

there is a document named 'doc-1266a' that
  has 'TCP has not fared well in wireless ad hoc networks. Even in a perfectly static network, there is the problem of throughput degradation due to the interference between flows, which becomes more severe as the nodes come close to each other. Moreover, when two or more TCP flows compete in the same collision domain, often one of the flows "captures" the channel and blocks other flows, giving flows an unfair access to the channel. These problems have been traced in the 802.11 MAC protocol and the interaction between TCP and MAC protocol retransmission timeouts. Proposed solutions range from modifications in the 802.11 MAC protocols to the use of network layer selective drop strategies. However, none of these schemes offers full protection. Moreover, they require change in the existing protocols. The advent of Multiple-Input Multiple-Output (MIMO) system offers a way of increasing total throughput by reducing the interference. The same interference blocking feature leads to unfairness reduction. Recently proposed MIMO MAC protocols such as SPACE-MAC reduce interference by separating flows. In this paper, we test the efficacy of SPACE-MAC in eliminating the TCP capture problem and reducing throughput degradation due to interference. We evaluate the fairness and throughput performance of SPACE-MAC in various scenarios, and then compare with conventional MAC results via Qualnet simulation.' as abstract.

there is a document named 'doc-1269a' that
  has 'This paper addresses the problem of plan recognition for multi-agent teams. Complex multi-agent tasks typically require dynamic teams where the team membership changes over time. Teams split into subteams to work in parallel, merge with other teams to tackle more demanding tasks, and disband when plans are completed. We introduce a new multi-agent plan representation that explicitly encodes dynamic team membership and demonstrate the suitability of this formalism for plan recognition. From our multi-agent plan representation, we extract local temporal dependencies that dramatically prune the hypothesis set of potentially-valid team plans. The reduced plan library can be efficiently processed using existing tree search techniques to obtain the team state history. Although multi-agent plan recognition is theoretically more computationally expensive than singleagent plan recognition, we show that, in practice, the presence of agent resource dependencies significantly reduces the set of potentially-valid plans.' as abstract.

there is a document named 'doc-1271a' that
  has 'This paper introduces a lightweight and secure framework enabling the refreshing of private keys in identitybased public key infrastructures. The framework is applied to enable secure inter-operation between entities with different trusted authorities in dynamic coalition environments. The approach is particularly well-suited to coalition forming in computation- and bandwidth-limited MANETs.' as abstract.

there is a document named 'doc-1273a' that
  has 'In this paper we describe a mechanism for disseminating key revocation information across a distributed trust authority in a tactical MANET environment in which nodes may be compromised or exhibit Byzantine failure. As this requires a precise characterization of message complexity and the location of the distributed trust authority nodes to ensure that security-related constraints are satisfied, we introduce a mobility model for use in simulation intended to capture platoon-level movements. We propose and analyze key revocation mechanisms to optimize the requirements of fast revocation propagation, complete coverage, and low message complexity. We then evaluate the performance of algorithms in a network simulation environment using the previously described mobility model.' as abstract.

there is a document named 'doc-1274a' that
  has 'Norms, that is, obligations, permissions and prohibitions, are a useful abstraction to specify and regulate the actions of self-interested software agents in open, heterogeneous systems. Any realistic account of norms must address their dynamic nature: the norms associated to agents will change as agents act (and interact) - prohibitions can be lifted, obligations can be fulfilled and permissions can be revoked as a result of agents\' behaviours. These norms may at times conflict with one another, that is, an action may be simultaneously prohibited and obliged (or prohibited and permitted). Norm conflicts prevent agents from rationally deciding on their behaviour. In this paper, we present mechanisms to detect and resolve normative conflicts. We achieve more expressiveness, precision and realism in our norms by using constraints over first-order variables. The mechanism to detect and resolve norm conflicts takes into account such constraints and is based on first-order unification and constraint satisfaction. We also explain how the mechanisms can be deployed in the management of an explicit account of all norms associated with a society of agents.' as abstract.

there is a document named 'doc-1275b' that
  has 'In recent years, we have seen the arrival of Distributed Denial-ofService (DDoS) open-source bot-based attack tools facilitating easy code enhancement, and so resulting in attack tools becoming more powerful. Developing new techniques for detecting and responding to the latest DDoS attacks often entails using attack traces to determine attack signatures and to test the techniques. However, obtaining actual attack traces is difficult, because the high-profile organizations that are typically attacked will not release monitored data as it may contain sensitive information. In this paper, we present a detailed study of the source code of the popular DDoS attack bots, Agobot, SDBot, RBot and Spybot to provide an in-depth understanding of the attacks in order to facilitate the design of more effective and efficient detection and mitigation techniques.' as abstract.

there is a document named 'doc-1276' that
  has 'Management in pervasive systems cannot rely on human intervention or centralised decision-making functions. It must be devolved, based on local decision-making and feedback control-loops embedded in autonomous components. We have previously proposed the self-managed cell (SMC) as an architectural pattern for building ubiquitous applications, where a SMC consists of hardware and software components that form an autonomous administrative domain. SMCs may be realised at different scales, from body-area networks for health monitoring, to an entire room or larger distributed settings. However, to scale to larger systems, SMCs must collaborate with each other, and federate or compose in larger SMC structures. This paper discusses requirements for interactions between SMCs and proposes key abstractions and protocols for realising peer-to-peer and composition interactions. These enable SMCs to exchange data, react to external events and exchange policies that govern their collaboration. Dynamically customisable interfaces are used for encapsulation and interaction mediation. Although the examples used here are based on healthcare scenarios, the principles and abstractions described in the paper are more generally applicable.' as abstract.

there is a document named 'doc-1277a' that
  has 'In the coalition forces, users are increasingly challenged with the issues of information overload and correlation of information from heterogeneous sources. Users might need different pieces of information, ranging from information about a single building, to the resolution strategy of a global conflict. Sometimes, the time, location and past history of information access can also shape the information needs of users.  Information systems need to help users pull together data from disparate sources according to their expressed needs (as represented by system queries), as well as less specific criteria. Information consumers have varying roles, tasks/missions, goals and agendas, knowledge and background, and personal preferences. These factors can be used to shape both the execution of user queries and the form in which retrieved information is packaged. However, full automation of this daunting information aggregation and customization task is not possible with existing approaches.  In this paper we present an infrastructure for context-aware information retrieval to enhance situation awareness. The infrastructure provides each user with a customized, mission-oriented system that gives access to the right information from heterogeneous sources in the context of a particular task, plan and/or mission. The approach lays on five intertwined fundamental concepts, namely Workflow, Context, Ontology, Profile and Information Aggregation. The exploitation of this knowledge, using appropriate domain ontologies, will make it feasible to provide contextual assistance in various ways to the work performed according to a user\'s taskrelevant information requirements. This paper formalizes these concepts and their interrelationships.' as abstract.

there is a document named 'doc-1278a' that
  has 'IEEE 802.11 MAC does not use RTS/CTS exchange in multicast/broadcast mode. This may cause throughput loss and packet collision by hidden terminals. We propose a new cross-layer ad hoc multicast protocol, named MIMOCAST, to improve efficiency of IEEE 802.11 MAC. MIMOCAST builds a multicast tree on-demand by using control packets exchange. To reduce duplicated transmissions, MIMO-CAST employs Multi Point Relay with which only designated nodes forward packets and all nodes in the network hear those packets. It also exploits Multiple-Input Multiple-Output (MIMO) by giving different weights to its multiple antennas so that a node can receive from one neighbor while blocking the interference from other nodes. We investigate the performance improvement of MIMOCAST through a simulation study' as abstract.

there is a document named 'doc-1279a' that
  has 'In this paper we study the capacity of wireless ad hoc networks with infrastructure support of an overlay of wired base stations. Such a network architecture is often referred to as hybrid wireless network or multihop cellular network. Previous studies on this topic are all focused on the twodimensional disk model proposed by Gupta and Kumar in their original work on the capacity of wireless ad hoc networks. We further consider a one-dimensional network model and a two-dimensional strip model to investigate the impact of network dimensionality and geometry on the capacity of such networks. Our results show that different network dimensions lead to significantly different capacity scaling laws. Specifically, for a one-dimensional network of n nodes and b base stations, even with a small number of base stations, the gain in capacity is substantial, increasing linearly with the number of base stations as long as b log b _ n. However, a two-dimensional square (or disk) network requires a large number of base stations b = _(_ n) before we see such a capacity increase. For a 2-dimensional strip network, if the width of the strip is at least on the order of the logarithmic of its length, the capacity follows the same scaling law as in the 2-dimensional square case. Otherwise the capacity exhibits the same scaling behavior as in the 1-dimensional network. We find that the different capacity scaling behaviors are attributed to the percolation properties of the respective network models.' as abstract.

there is a document named 'doc-1283a' that
  has 'We study traces taken from UMass DieselNet, a Disruption-Tolerant Network consisting of WiFi nodes attached to buses. As buses travel their routes, they encounter other buses and in some cases are able to establish pair-wise connections and transfer data between them. We analyze the bus-to-bus contact traces to characterize the contact process between buses and its impact on DTN routing performance. We find that the all-bus-pairs aggregated intercontact times show no discernible pattern. However, the intercontact times aggregated at a route level exhibit periodic behavior. Based on analysis of the deterministic inter-meeting times for bus pairs running on route pairs, and consideration of the variability in bus movement and the random failures to establish connections, we construct generative route-level models that capture the above behavior. Through trace-driven simulations of epidemic routing, we find that the epidemic performance predicted by traces generated with this finer-grained route-level model is much closer to the actual performance that would be realized in the operational system than traces generated using the coarse-grained all-bus-pairs aggregated model. This suggests the importance in choosing the right level of model granularity when modeling mobility-related measures such as inter-contact times in DTNs.' as abstract.

there is a document named 'doc-1286' that
  has 'There has been considerable effort over the past few years to develop MANETs and to show their suitability for sensor networks. Concurrently, there has been vast improvement in radio link capability, primarily through the adoption of MIMO antenna technologies. The combination of these elements has been proposed as a means to provide higher performance sensing capability, but models currently assume that only single nodes, or single nodes within clusters, have MIMO links. This paper describes an approach using cooperation between nodes to collaboratively create MIMO, diversity and beam formed antennas from the distributed sensor elements. The power trade off in creating such distributed intelligent adaptive antennas versus using single nodes with smart antennas is addressed. The routing issues associated with both approaches are also addressed. The benefits promised by cooperative intelligent adaptive antennas for sensor networks include better power management, longer distance communication and higher data rates.' as abstract.

there is a document named 'doc-1288' that
  has 'This paper presents a novel protocol, Self-Healing Routing (SHR), for opportunistic multi-hop wireless communication. The protocol uses broadcast communication and a prioritized slotted transmission backoff delay scheme to empower a receiving node to use its hop distance from the destination to autonomously decide whether to forward a packet. This enables dynamic traversal of the shortest available routes without requiring nodes to explicitly decide to which neighbors to forward packets. When severed routes are encountered, the protocol dynamically and locally re-routes packets so they traverse the surviving shortest route. The result, as shown by simulation data reported here, is an efficient fault-tolerant protocol that performs well even when the network topology changes spontaneously.' as abstract.

there is a document named 'doc-1289' that
  has 'Random projection (RP) is a common technique for dimensionality reduction under L2 norm for which many significant space embedding results have been demonstrated. In particular, random projection techniques can yield sharp results for R d under the L2 norm in time linear to the product of the number of data points and dimensionalities in question. Inspired by the use of symmetric probability distributions in previous work, we propose a RP algorithm based on the hyper-spherical symmetry and give its probabilistic analyses based on Beta and Gaussian distribution.' as abstract.

there is a document named 'doc-1290' that
  has 'Recent theoretical and applied works have demonstrated the appropriateness of wavelets for analysing signals containing non-stationarity, unsteadiness, self-similarity, and non-Markovity. We applied wavelets to study packet traffic in a packet switching network model, focusing on the spectral properties of packet traffic near phase transition (critical point) from free flow to congestion, and considered different dynamic & static routing metrics. We show that "wavelet power spectra"and variance are important estimators of the changes occurring with source load increasing from sub-critical, through critical, to super-critical and it depends on the routing algorithm.' as abstract.

there is a document named 'doc-1292' that
  has 'Mobile ad hoc networks often support sensitive applications. These applications may require that users\' identity, location, and correspondents be kept secret. This is a challenge in a MANET because of the cooperative nature of the network and broadcast nature of the communication media. In this paper, we propose a Privacy Preserving Communication System (PPCS) which provides a comprehensive solution to anonymize communication endpoints, keep the location and identifier of a node unlinkable, and mask the existence of communication flows. We present an analysis of the security of PPCS against passive internal attackers, provide a qualitative discussion on its strength against external attackers, and characterize its performance trade-offs. The simulation results demonstrate that PPCS has only 3% lower packet delivery ratio than existing multi-path routing protocols, while effectively providing privacy service in MANETs.' as abstract.

there is a document named 'doc-1295a' that
  has 'Content sharing using cooperative peer-to-peer model has become increasingly more popular in a vehicular ad hoc network (VANET). The small transmission window from a vehicle to an access point (AP), high mobility of vehicles, and intermittent and short-lived connectivity to an AP provide incentives for vehicles to cooperate with one another to obtain information from the Internet. These characteristics of VANETs naturally stipulate the use of cooperative peer-to-peer paradigm and motivate related content sharing application such as CarTorrent. Building upon previous research on SPAWN[6, 1], we have implemented CarTorrent and deployed it on a real VANET. We have run extensive field tests to affirm the feasibility of the peer-to-peer file sharing application tailored to VANET. To the best of our knowledge, the deployment of such a content sharing application on a real vehicular ad hoc testbed is the first of its kind.' as abstract.

there is a document named 'doc-1296a' that
  has 'In an urban environment, vehicles can opportunistically exploit infrastructure through open Access Points (APs) to efficiently communicate with other vehicles. This is to avoid long wireless ad hoc paths, and to alleviate congestion in the wireless grid. Analytic and simulation models are used to optimize the communications and networking strategies. For realistic results, one important challenge is the accurate representation of traffic mobility patterns. In this paper we introduce realistic vehicular mobility traces of downtown Portland, Oregon, obtained from extremely detailed large scale traffic simulations performed at the Los Alamos National Laboratories (LANL). To the best of our knowledge, these are among the most accurate synthetic motion traces available for study, with the exception of actual car trace measurements. The new mobility model is used to evaluate AODV [1] in flat and opportunistic infrastructure routing. To assess the importance of a realistic mobility model for this evaluation, we compare these results with those obtained with CORSIM [2] traces. The paper makes the following contributions: (a) introduction of efficient, opportunistic strategies for extending the AP infrastructure to use vehicle to vehicle paths, and (b) assessment of different mobility models - CORSIM traces and LANL\'s realistic vehicular traces - in the modeling of different routing strategies.' as abstract.

there is a document named 'doc-1298a' that
  has 'In this paper we describe an algorithm for the distribution of trust authority functions such as key generation and distribution in tactical mobile ad hoc networks. Such networks cannot rely on existing infrastructures and must operate under severe resource constraints. Moreover, network partitioning and node failure, including Byzantine failures must be compensated in tactical networks. We propose the combination of metrics on both network state and beliefs or trust in other nodes to form a composite metric for use in a clustering algorithm. The effectiveness and other characteristics of this improved clustering algorithm are then evaluated and analyzed in a simulation environment, demonstrating a significant improvement over the baseline clustering algorithm.' as abstract.

there is a document named 'doc-1299' that
  has 'Bipolar argumentation frameworks have been instrumental in capturing the notion of support between arguments. However, some of their semantics are counterintuitive in situations regarding evidential reasoning, and we extend them to account for this. To account for our long term research goals, we also update Bipolar argumentation frameworks to be able to handle support and attack by multiple argument in the same style as Nielsen\'s work.' as abstract.

there is a document named 'doc-1300' that
  has 'Operational effectiveness in coalition environments is based on the need for inter-operability at a variety of levels. While inter-operability concerns are most easily thought of in terms of technology, this paper emphasizes the importance of consensual interpretations of the semantic significance of exchanged information. In this paper we outline some of the challenges to effective modes of information exchange in coalition operational contexts. We also discuss potential approaches to these challenges in the context of a semantically-enabled technological framework for information exploitation - the Semantic Battlespace Infosphere (SBI). Relevant aspects of this framework are introduced and some of the socio-technical challenges that are likely to be encountered are discussed.' as abstract.

there is a document named 'doc-1302' that
  has 'Supporting human collaboration is challenging partly because of variability in how people collaborate. Even within a single organization, there can be many variants of processes which have the same purpose. When diverse coalition members must work together, the differences can be especially large, baffling and disruptive. Coordination theory provides a method and vocabulary for modeling complex collaborative activities in a way that makes both the similarities and differences between them more visible. To demonstrate this, we modeled three very different engineering change management processes and found: (1) most of the work is coordination-related; (2) despite large apparent differences, a coordination-theoretic analysis revealed substantial commonalities among the three processes; and (3) differences in the processes were due to choices regarding coordination mechanisms. This approach has promise for helping to merge or integrate different processes and to suggest ways that agents can participate in complex collaborative processes.' as abstract.

there is a document named 'doc-1305' that
  has 'This paper addresses the problem of recognizing policies given logs of battle scenarios from multi-player games. The ability to identify individual and team policies from observations is important for a wide range of applications including automated commentary generation, game coaching, and opponent modeling. We define a policy as a preference model over possible actions based on the game state, and a team policy as a collection of individual policies along with an assignment of players to policies. This paper explores two promising approaches for policy recognition: (1) a modelbased system for combining evidence from observed events using Dempster-Shafer theory, and (2) a data-driven discriminative classifier using support vector machines (SVMs). We evaluate our techniques on logs of real and simulated games played using Open Gaming Foundation d20, the rule system used by many popular tabletop games, including Dungeons and Dragons.' as abstract.

there is a document named 'doc-1306a' that
  has 'The annotation of transcription binding sites in new sequenced genomes is an important and challenging problem. We have previously shown how a regression model that linearly relates gene expression levels to the matching scores of nucleotide patterns allows us to identify DNA-binding sites from a collection of co-regulated genes and their nearby non-coding DNA sequences. Our methodology uses Bayesian models and stochastic search techniques to selecttranscription factor binding site candidates. Here we show that this methodology allows us to identify binding sites in nearby species. We present examples of annotation crossing from Schizosaccharomyces pombe to Schizosaccharomyces japonicus. We found that the eng1 motif is also regulating a set of 9 genes in S. japonicus. Our framework may have an effective interest in conveying information in the annotation process of a new species. Finally we discuss a number of statistical and biological issues related to the identification of binding sites through covariates of genes expression and sequences.' as abstract.

there is a document named 'doc-1314' that
  has 'The objectives of this paper are twofold. First, we introduce a novel policy language, called CIM-SPL (Simple Policy Language for CIM) that complies with the CIM (Common Information Model) Policy Model and fully incorporates CIM constructs. Currently, the CIM standards from Distributed Management Task Force (DMTF) include a Policy Model, but there is no satisfactory way to render this policy model. CIM-SPL is a language that has been defined for this purpose. Second, we address design and implementation issues for policy languages in general, and for CIM-SPL in particular. The design of CIM-SPL was inspired by our previous experiences in designing various policy languages (e.g., PDL from Bell Laboratories, and ACPL from IBM) and lessons learned from studying other well-known policy languages (e.g., Ponder from Imperial College). We will discuss our design choices, evaluating the pros and cons of various alternatives. The ideas presented in this paper are meant to shed light on our design decisions, and provide guidance for those who want to build a CIM-based policy system or some other policy system in the future.' as abstract.

there is a document named 'doc-1316' that
  has 'In this paper, we present a set of attributes that are being proposed to characterize quality of information (QoI) for sensor-enabled applications in a domain-agnostic manner. We then focus on two important of these attributes, timeliness and data reliability, which capture the quality of detection processes with respect to how fast and how accurately a detection is made. With special emphasis on transient phenomena, i.e., phenomena of limited duration, using traditional Bayesian-based hypothesis testing techniques, we investigate the detection of these phenomena and we analytically derive relationships that capture the QoI of a phenomenon detector as a function of the duration of the observed phenomena and the rate with which observations of the phenomena are collected.' as abstract.

there is a document named 'doc-1317a' that
  has 'Underwater environment represents a challenging and promising application scenario for sensor networks. Due to hard constraints imposed by acoustic communications and to high power consumption of acoustic modems, in Underwater Sensor Networks (USN) energy saving becomes even more critical than in traditional sensor networks. In this paper we propose Delay-tolerant Data Dolphin (DDD), an approach to apply delaytolerant networking in the resource-constrained underwater environment. DDD exploits the mobility of a small number of capable collector nodes (namely dolphins) to harvest information sensed by low power sensor devices, while saving sensor battery power. DDD avoids energy-expensive multi-hop relaying by requiring sensors to perform only one-hop transmissions when a dolphin is within their transmission range. The paper presents simulation results to evaluate the effectiveness of randomly moving dolphins for data collection and paves the way for future analytic investigations.' as abstract.

there is a document named 'doc-1318' that
  has 'This paper presents the results of implementation of a novel protocol, Self-Healing Routing (SHR) for opportunistic multi-hop wireless communication, on MicaZ sensor motes. The protocol uses broadcast communication and a prioritized transmission back-off delay scheme to empower a receiving mote to use its hop distance from the destination to decide autonomously whether to forward a packet. When severed routes are encountered, the protocol dynamically and locally re-routes packets so they traverse the surviving shortest route.  We have implemented this protocol on a set of MicaZ motes as well as in the SENSE sensor network simulator and conducted field testing with different mote and network configurations. We also tested scenarios with the motes turned off (modeling permanent failures) and in simulation also temporarily off line (modeling transient failures). We compared SHR with two traditional protocols: MintRoute and AODV. The results, as shown by experimental measurement and simulations reported in the paper, demonstrate that Self-Healing Routing is an efficient fault-tolerant protocol that performs well even with spontaneous network topology changes.' as abstract.

there is a document named 'doc-1319' that
  has 'People wish to enjoy their everyday lives in various ways, among which entertainment plays a major role. In order to improve lifestyle with more ready access to entertainment content, we propose BlueTorrent, a P2P file sharing application based on ubiquitous Bluetoothenabled devices such as PDAs, cellphones and smart phones. Using BlueTorrent, people can share audio/video contents as they move about shopping malls, airports, subway stations etc. BlueTorrent poses new challenges caused by limited bandwidth, short communications range, mobile users and variable population density. A key ingredient is efficient peer discovery. This paper approaches the problem by analyzing the Bluetooth periodic inquiry mode and by finding the optimum inquiry/connection time settings. At the application layer, the BlueTorrent index/block dissemination protocol is then designed and analyzed. The entire system is integrated and implemented both in simulation and in an experimental testbed. Simulation and measurement results are used to evaluate and validate the performance of BlueTorrent in content sharing scenarios.' as abstract.

there is a document named 'doc-1323' that
  has 'Fingerprinting is a widely used technique among the networking and security communities for identifying different implementations of the same piece of networking software running on a remote host. A fingerprint is essentially a set of queries and a classification function that can be applied on the responses to the queries in order to classify the software into classes. So far, identifying fingerprints remains largely an arduous and manual process. This paper proposes a novel approach for automatic fingerprint generation, that automatically explores a set of candidate queries and applies machine learning techniques to identify the set of valid queries and to learn an adequate classification function. Our results show that such an automatic process can generate accurate fingerprints that classify each piece of software into its proper class and that the search space for query exploration remains largely unexploited, with many new such queries awaiting discovery. With a preliminary exploration, we are able to identify new queries not previously used for fingerprinting.' as abstract.

there is a document named 'doc-1324' that
  has 'Few existing argumentation frameworks are designed to deal with probabilistic knowledge, and none are designed to represent possibilistic knowledge, making them unsuitable for many real world domains. In this paper we present a subjective logic based framework for argumentation which overcomes this limitation. Reasoning about the state of a literal in this framework can be done in polynomial time. A dialogue game making use of the framework and a utility based heuristic for playing the dialogue game are also presented. We then show how these components can be applied to contract monitoring. The dialogues that emerge bear some similarity to the dialogues that occur when humans argue about contracts, and our approach is highly suited to complex, partially observable domains with fallible sensors where determining environment state cannot be done for free.' as abstract.

there is a document named 'doc-1325' that
  has 'The speed with which Internet worms propagate, and their potential for carrying devastating payloads makes them a significant threat to the stability of the Internet. Current approaches for containing these worms are ineffective due to their completely local protection mechanisms - requiring complete deployment for global worm containment. This paper suggests an alternate approach wherein the containment mechanisms are moved within the network itself rather than at end-points. This internetwork-centric approach allows networks within the Internet to not only protect themselves, but also other networks that may not have the containment technology deployed. A novel reputationbased alerting mechanism is used to ensure fair and fast information sharing. The combination of the internetworkcentric containment and reputation-based alerting allows for the creation of an Internet-wide containment mechanism that provides greater protection against fast scanning worms than any previously proposed system, and at the same time providing unequaled resilience to false positives and malicious nodes.' as abstract.

there is a document named 'doc-1327' that
  has 'Nowadays data dissemination often happens in vehicular sensor networks (VSN) and other mobile ad hoc networks in military & surveillance scenarios. For example, VSN is usually used to proactively monitor urban environment. In VSN, each sensor can generate a large amount of data which must be reliably reported to actuator agents. Data dissemination in conjunction with efficient harvesting has been proven to be very effective in this type of applications. The performance depends on many different parameters including speed, motion pattern, node density, topology, data rate, and transmission range. This multitude makes it difficult to accurately evaluate and compare data gathering protocols implemented in different simulation or testbed scenarios. In this paper, we introduce Neighborhood Change Rate (NCR), a unifying measurement for different motion patterns used in epidemic dissemination, a contact-based data dissemination. By its intrinsic property, the NCR measurement is able to describe the spatial and temporal dependencies and well characterize a dissemination/harvesting scenario. We illustrate our approach by applying the NCR concept to MobEyes, a lightweight data gathering protocol. We further analytically studied the effective NCR for Markov type motion models, such as Real Track mobility model. A closed-form expression has been derived. From this analytic solution, the NCR can be approximated from the initial scenario settings, such as velocity range, transmission range, and real map/street information. The closed-form formula for NCR can be further employed to evaluate the ED process. The mathematical relationship between the dissemination index and the effective NCR is established and it allows predicting the performance of the ED process in realistic track motion scenarios. The experiment results showed that the analytic expressions for the NCR and for the evaluation of the ED process closely match the discrete-event simulations.' as abstract.

there is a document named 'doc-1328' that
  has 'Agents may choose to ignore contract violations if the costs of enforcing the contract exceed the compensation they would receive. In this paper we describe an argumentation based framework for agents to both decide whether to enforce a contract, and to undertake contract enforcement actions. The framework centres around agents presenting beliefs to justify their position, and backing up these beliefs with facts as necessary. Presenting facts costs an agent utility, and our framework operates by using a reasoning mechanism which is based on the agent comparing the utility it would gain for proving a set of literals with the costs incurred during this process.' as abstract.

there is a document named 'doc-1331' that
  has 'In order to provide network connectivity in highly partitioned ad-hoc networks, we propose a routing strategy that incorporates an existing ad-hoc routing protocol, ad hoc on demand distance vector (AODV), with disruption tolerant networking (DTN) a la store-carry-forward mechanisms using unmanned aerial vehicles (UAVs) as carriers. This paper focuses on the design, implementation, and evaluation of the routing strategy. The major contribution of this work is the implementation of our DTN aware routing protocol on top of existing and mostly unmodified AODV. We show the advantage of the DTN protocol through simulation using ns-2' as abstract.

there is a document named 'doc-1332' that
  has 'In-network aggregation is an essential primitive for performing queries on sensor network data. However, most aggregation algorithms assume that all intermediate nodes are trusted. In contrast, the standard threat model in sensor network security assumes that an attacker may control a fraction of the nodes, which may misbehave in an arbitrary (Byzantine) manner. We present the first algorithm for provably secure hierarchical in-network data aggregation. Our algorithm is guaranteed to detect any manipulation of the aggregate by the adversary beyond what is achievable through direct injection of data values at compromised nodes. In other words, the adversary can never gain any advantage from misrepresenting intermediate aggregation computations. Our algorithm incurs only O(_log2 n) node congestion, supports arbitrary tree-based aggregator topologies and retains its resistance against aggregation manipulation in the presence of arbitrary numbers of malicious nodes. The main algorithm is based on performing the SUM aggregation securely by first forcing the adversary to commit to its choice of intermediate aggregation results, and then having the sensor nodes independently verify that their contributions to the aggregate are correctly incorporated. We show how to reduce secure MEDIAN, COUNT, and AVERAGE to this primitive.' as abstract.

there is a document named 'doc-1333' that
  has 'Automatic identification of anomalies on network data is a problem of fundamental interest to ISPs to diagnose incipient problems in their networks. ISPs gather diverse data sources from the network for monitoring, diagnostics or provisioning tasks. Finding anomalies in this data is a huge challenge due to the volume of the data collected, the number and diversity of data sources and the diversity of anomalies to be detected. In this paper we introduce a framework for anomaly detection that allows the construction of a black box anomaly detector. This anomaly detector can be used for automatically finding anomalies with minimal human intervention. Our framework also allows us to deal with the different types of data sources collected from the network. We have developed a prototype of this framework, TrafficComber, and we are in the process of evaluating it using the data in the warehouse of a tier-1 ISP.' as abstract.

there is a document named 'doc-1334b' that
  has 'Numerous sensor-based applications depend upon the detection of certain events. The proper operation of these applications depend on the quality of the information (QoI) that they receive from their sensor-based event detectors. In this paper, we establish relationships that tie the QoI attributes of timeliness and confidence to the operational characteristics of a sensor system and the events they detect. By building upon the Neyman-Pearson hypothesis testing procedure, we study the dependence of these characteristics and attributes on each other and establishing their theoretical performance boundaries.' as abstract.

there is a document named 'doc-1335' that
  has 'Mobile Ad-hoc NETworks (MANETs) are most useful in unprepared emergencies where critical applications must be launched quickly. However, they often operate in an adverse environment where end-to-end connectivity is highly susceptible to disruption. Adjusting the motion of existing nodes or deploying additional nodes can improve the connectivity under some circumstances, but for scenarios where connectivity cannot be immediately improved, disruption must be coped with properly. In this paper we propose the Ad-hoc Storage Overlay System (ASOS). ASOS is a self-organized overlay of storage-abundant nodes to jointly provide distributed and reliable storage to data flows under disruption. ASOS is a Delay-Tolerant Networking (DTN) approach that significantly improves the applicability of MANETs in practice.' as abstract.

there is a document named 'doc-1336' that
  has 'Gupta and Kumar established that the per node throughput of ad hoc networks with multi-pair unicast traffic scales as _________ ____________________ , thus indicating that network performance does not scale well with an increasing number of nodes. However, the model of Gupta and Kumar did not allow for the possibility of network coding and broadcasting, and recent work has suggested that such techniques have the potential to greatly improve network throughput. Here, for multiple unicast flows in a random topology under the protocol communication model of Gupta and Kumar [1], we show that for arbitrary network coding and broadcasting in a two-dimensional random topology that the throughput scales as _____________________________ where _ is the total number of nodes and _______ is the transmission radius. When _______ is set to ensure connectivity, ______________________________ , which is of the same order as the lower bound for the throughput without network coding and broadcasting; in other words, network coding and broadcasting at most provides a constant factor improvement in the throughput. This result is also extended to one-dimensional and three-dimensional random deployment topologies, where it is shown that _____________________ for the one-dimensional topology and _____________ _ _ ____ _______ _ for three-dimensional networks.' as abstract.

there is a document named 'doc-1337' that
  has 'Agents may choose to ignore contract violations if the costs of enforcing the contract exceed the compensation they would receive. In this paper we provide an argumentation based framework for agents to both decide whether to enforce a contract, and to undertake contract enforcement actions. The framework centers around the agent reasoning about what arguments to put forth based on a comparison between the utility it would gain for proving its case and the utility it loses for probing environment state.' as abstract.

there is a document named 'doc-1338a' that
  has 'Mobile peer-to-peer systems have recently got in the limelight of the research community that is striving to build efficient and effective mobile content addressable networks. Along this line of research, we propose a network coding based file swarming protocol targeting vehicular ad hoc networks (VANET). We argue that file swarming protocols in VANET should deal with typical mobile network issues such as dynamic topology and intermittent connectivity as well as various other issues that have been disregarded in previous mobile peer-to-peer researches such as addressing, node/user density, non-cooperativeness, and unreliable channel. Through simulation, we show that the efficiency and effectiveness of our protocol allows shorter file downloading time compared to an existing VANET file swarming protocol.' as abstract.

there is a document named 'doc-1339' that
  has 'We describe a novel architecture for supporting mobile information access in the occasionally connected computing environment. By utilizing user workflow, profiles, and environmental information, information from various sources published to the system is prioritized, cached and synchronized in the staging server, and eventually disseminated to mobile devices. Furthermore, workflow and user profile can help in predicting near future data needs and therefore can enable intelligent data charging.' as abstract.

there is a document named 'doc-1349' that
  has 'We continually face the problem of making sense of the world by resolving conflicting reports from multiple sources of information. This is particularly so if we attempt to formulate Qualitative Safety Arguments. Traditional logic offers little to assist in this process. In every day reasoning we usually assume that, without information to the contrary, we should use all information from all sources and that "information to the contrary" is the presence of an inconsistency between the sources. In order to resolve these conflicts we must make use of additional information which gives preference to one source of information over another when conflict arises between sources. The suggestion put forward in this note is that it is possible to reach a \'best\' conclusion by taking the most coherent theory that respects the preference ordering on sources and that this theory is the maximal consistent theory with respect to the ordering. This process is parameterized by an underlying notion of logic that provides the notions of consistency and entailment. This notion is straightforward when applied to the standard notion of a strict preference ordering but is a little more involved when we consider partially ordered preferences.' as abstract.

there is a document named 'doc-1377a' that
  has 'Wireless sensor networks have proven useful for applications in diverse domains. The challenges of scale and resource constraints posed by these systems have led to development of novel network protocols and services, but their focus has been on traditional metrics of quality of service of network data transport. Rather, sensor networks require combining networking quality of service concerns with metrics of quality and integrity of sensor data sources and performance of sensor fusion algorithms. We describe how network protocols, data integrity management, and sensor fusion algorithms can be design to cooperatively optimize the "Quality of Information" returned by a sensor network.' as abstract.

there is a document named 'doc-1388' that
  has 'This paper presents a biologically inspired routing protocol called Self Selective Routing with preferred path selection (SSR(v3)). Its operation resembles the behavior of a biological ant that finds a food source by following the strongest pheromone scent left by scout ants at each fork of a path. Likewise, at each hop of a multi-hop path, a packet using the Self Selective Routing (SSR) protocol moves to the node with the shortest hop distance to the destination. Each intermediate node on a route to the destination uses a transmission back-off delay to select a path to follow for each packet of a flow. Neither an ant nor a packet knows in advance the route that each will follow as it is decided at each step. Therefore, when a route becomes severed by a failure, they can dynamically and locally adjust their routing to traverse the shortest surviving path. Preferred path selection reduces transmission delay by essentially removing back-off delay for the node that carried the previous packet of the same flow. The results reported here for both simulation and execution of a MicaZ mote implementation, show that this is an efficient and fault-tolerant protocol with small transmission delay, high reliability and high delivery rate.' as abstract.

there is a document named 'doc-1389b' that
  has 'This paper considers rate adaptation in a wireless sensor network (WSN) consisting of multiple missions, where each mission subscribes to data streams from multiple sensors, and each sensor\'s data is utilized by multiple heterogenous missions. We specifically consider the application of the distributed network utility maximization (NUM) framework to a previously unconsidered scenario where the different missions have different priorities, as well as minimum utility demands. When all the utility demands are feasible, we show that the introduction of an additional term reflecting a penalty for failing to reach the sensor rates required to satisfy the minimum utility demand in the gradient-based optimization scheme enables the NUM protocol to maximize the global utility, while enforcing resource sharing among a mix of high priority and best-effort missions. In situations where the minimum demands cannot all be satisfied we present a) a family of modified NUM techniques to determine the optimal satisfied set, when the missions have unique priority order, and b) heuristics for applying NUM, when multiple missions have the same priority. We also specify the protocols that allow individual sensors, forwarding and receiver nodes in the WSN to implement these enhanced NUM algorithms with only localized coordination.' as abstract.

there is a document named 'doc-1396' that
  has 'This paper investigates the relationships between identity-based non-interactive key distribution (ID-NIKD) and identity-based encryption (IBE). It provides a new security model for ID-NIKD, and a construction that converts a secure ID-NIKD scheme satisfying certain conditions into a secure IBE scheme. This conversion is used to explain the relationship between the IDNIKD scheme of Sakai, Ohgishi and Kasahara and the IBE scheme of Boneh and Franklin. The paper then explores the construction of ID-NIKD and IBE schemes from general trapdoor discrete log groups. Two different concrete instantiations for such groups provide new, provably secure ID-NIKD and IBE schemes. These schemes are suited to applications in which the Trusted Authority is computationally well-resourced, but clients performing encryption/decryption are highly constrained.' as abstract.

there is a document named 'doc-1404' that
  has 'Our work stems from the consideration that the spreading of a disease is modulated by the individual\'s perception of the infected neighborhood and his/her strategy to avoid being infected as well. We introduced a general ``cellular agent\'\' model that accounts for a hetereogeneous and variable network of connections. The probability of infection is assumed to depend on the perception that an individual has about the spreading of the disease in her local neighborhood and on broadcasting media. In the one-dimensional homogeneous case the model reduces to the DK one, while for long-range coupling the dynamics exhibits large fluctuations that may lead to the complete extinction of the disease.' as abstract.

there is a document named 'doc-1416' that
  has 'The ability to transmit a message securely in the presence of eavesdroppers in a dense wireless network is considered. As with a number of recent schemes, system nodes other than the transmitter and receiver are chosen to generate noise that confuses the eavesdropper. By exploiting the dynamics of the fading, significantly improved performance is achieved beyond that generated from the standard multi-user diversity gain expected from opportunistic relaying. In particular, the node with the best fading characteristics takes responsibility for message relaying, while those whose fading will significantly reduce their impact on the desired communication play the role of noise generators. For a source transmitting to a destination using a set of intermediate relays, we consider the number of eavesdroppers that can be present without the interception of packets, in both the case where the eavesdroppers operate independently and in the case where they collude. The latter case also encompasses the more likely scenario of a single eavesdropper with a sophisticated receiver.' as abstract.

there is a document named 'doc-1440' that
  has 'This brief report presents the architecture and functionality of LigNite, a hybrid search engine for the semantic web. LigNite provides a web platform for querying semantic web ontologies, using both a controlled as well as a free-text search functionality. Ontologies may pre-exist in the system, or may be provided by the user via LigNite\'s import mechanism. A user searching an ontology using the controlled search engine will be guided to construct ontology triples, which are matched against the ontology\'s contents to retrieve results meeting the user\'s criteria. A user searching an ontology using the free-text search engine will not be restricted to a triple pattern, but may choose freely the structure of his query (hence the name "free-text"). The free-text search engine greatly relies on producing results that are ranked according to their correlation to the user\'s query.' as abstract.

there is a document named 'doc-1481b' that
  has 'The quality of computing certain aggregation functions based on incomplete measurements for the purpose of distributed network monitoring is considered. Network monitoring plays a fundamental role in network management systems by providing timely information on the network status, which is crucial for network administration purposes. To reduce network overhead and for easier assimilation, this information is usually presented by calculating a few key aggregate metrics. The aggregates are periodically computed from a large number of detailed events collected continuously during the course of the network operations. Under errors induced by network delays, the accuracy of typical aggregation functions used in network management systems (e.g., sum, average, maximum) is evaluated both analytically and by simulations. The results provide a quantifiable trade-off between accuracy and timeliness of the information acquired, which can then be used to design and optimize network management systems.' as abstract.

there is a document named 'doc-1533' that
  has 'Inter-domain routing in an important component to allow interoperation between heterogeneous domains operated by different organizations. Although inter-domain routing has been extensively studied in the Internet, it remains unexplore in the Mobile Ad Hoc Networks (MANETs) context. In MANETs, the inter-domain routing problem becomes more challenging because of 1) dynamic topology changes, and 2) diverse intra-domain ad hoc routing protocols. In this paper, we propose a framework called MAIN (Mobile Ad hoc Inter-domain Networking framework) to enable interoperation among multi-domain MANETs. MAIN can handle the dynamic nature of MANETs and support policy-based routing similar to that of BGP. We first discuss the challenges to design inter-domain routing in MANETs, then we describe our MAIN framework and some open issues.' as abstract.

there is a document named 'doc-1541d' that
  has 'Nowadays data dissemination often happens in vehicular sensor networks (VSN) and other mobile ad hoc networks in military & surveillance scenarios. The performance of data dissemination depends on many different parameters including speed, motion pattern, node density, topology, data rate, and transmission range. This multitude makes it difficult to accurately evaluate and compare data gathering protocols implemented in different simulation or testbed scenarios. In this paper, we introduce Neighborhood Change Rate (NCR), a unifying measurement for different motion patterns used in epidemic dissemination, a contact-based data dissemination. By its intrinsic property, the NCR measurement is able to describe the spatial and temporal dependencies and well characterize a dissemination / harvesting scenario. We illustrate our approach by applying the NCR concept to MobEyes, a lightweight data gathering protocol. We further analytically study the effective NCR for Markov type motion models, such as Real Track mobility model. A closed-form expression has been derived. From this analytic solution, the NCR can be approximated from the initial scenario settings, such as velocity range, transmission range, and real map/street information. The closed-form formula for NCR can be further employed to evaluate the ED process. The mathematical relationship between the dissemination index and the effective NCR is established and it allows predicting the performance of the ED process in realistic track motion scenarios. The experiment results showed that the analytic expressions for the NCR and for the evaluation of the ED process closely match the discrete-event simulations.' as abstract.

there is a document named 'doc-1554b' that
  has 'This paper investigates the interaction between network coding and link-layer transmission rate diversity in multihop wireless networks. By appropriately mixing data packets at intermediate nodes, network coding allows a single multicast flow to achieve higher throughput to a set of receivers. Broadcast applications can also exploit link-layer rate diversity, whereby individual nodes can transmit at faster rates at the expense of corresponding smaller coverage area. We first demonstrate how combining rate-diversity with network coding can provide a larger capacity again for data dissemination of a single multicast flow. We present a linear programming model to compute the maximal throughput that a multicast application can achieve with network coding in a rate-diverse wireless network. We then present simulation results comparing the performance of network coding in combination with transmission rate diveristy, for a realistic stream-oriented application. Our results provide preliminary evidence that wireless network coding may lead to a latency-vs-throughput tradeoff.' as abstract.

there is a document named 'doc-1561a' that
  has 'Network coding, where relay nodes combine the information received from multiple links rather than simply replicating and forwarding the received packets, has shown the promise of significantly improving system performance. In very recent works, multiple researchers have presented methods for increasing system throughput by employing network coding inspired methods to mix packets at the physical layer: physicallayer network coding (PNC). A common example used to validate much of this work is that of two sources exchanging information through a single intervening relay - a situation that we denote the "exchange channel". In this paper, achievable rates of various schemes on the exchange channel are considered. Achievable rates for traditional multi-hop routing approaches, network coding approaches, and various PNC approaches are considered. A new method of PNC inspired by Tomlinson-Harashima precoding (THP), where a modulo operation is used to control the power at the relay, is introduced.' as abstract.

there is a document named 'doc-1566e' that
  has 'Our primary hypothesis is that it should be possible to enrich data fusion by semantic processing, with wide potential application. In order to achieve our aim we need to represent the semantic data and enable reasoning about it in a framework that can be aligned with data fusion. Ontologies are most suited to this task as they allow for appropriate representation of data structure; some approaches include probabilistic representation. These can be aligned with data fusion approaches, such as Bayesian, which can fuse by including estimates of uncertainty. We shall describe our initial approaches towards establishing our hypothesis. We shall survey the enabling technologies, showing how they can contribute to our goal. We shall describe our selection of application data which derives from an acoustic sensor (military) scenario. We shall show how feature subset selection can reduce information-redundancy and improve efficiency in these domains, prior to fusion to enhance performance further. We shall explore the semantic attributes and the representations that can be deployed for enrichment purposes, showing how ontologies can be used in this context. In these respects we are aiming to show how we can approach enrichment of data fusion by semantic technologies, how this can capitalise on the current stock of techniques, and illustrate the potential benefits associated with this new approach.' as abstract.

there is a document named 'doc-1583a' that
  has 'The ability of a sensor device is affected significantly by the surroundings and environment in which it is placed. In almost all sensor modalities, some directions are better observed by a sensor than others. Furthermore, the exact impact on the sensing ability of the device is dependent on the position assigned to the sensor. While the problem of determining good coverage schemes for sensors of a field have many good solutions, not many approaches are known to address the challenges arising due to location specific distortion. In this paper, we look at the problem of incorporating terrain specific challenges in sensor coverage, and propose a geometric solution to address them.' as abstract.

there is a document named 'doc-1668b' that
  has 'This paper explores the idea of knowledge-based security policies, which are used to decide whether to answer queries over secret data based on an estimation of the querier\'s (possibly increased) knowledge given the results. Limiting knowledge is the goal of existing information release policies that employ mechanisms such as noising, anonymization, and redaction. Knowledge-based policies are more general: they increase flexibility by not fixing the means to restrict information flow. We enforce a knowledge-based policy by explicitly tracking a model of a querier\'s belief about secret data, represented as a probability distribution, and denying any query that could increase knowledge above a given threshold. We implement query analysis and belief tracking via abstract interpretation using a novel probabilistic polyhedral domain, whose design permits trading off precision with performance while ensuring estimates of a querier\'s knowledge are sound. Experiments with our implementation show that several useful queries can be handled efficiently, and performance scales far better than would more standard implementations of probabilistic computation based on sampling.' as abstract.

there is a document named 'doc-1669' that
  has 'Information flow analysis is a powerful technique for reasoning about sensitive information that may be exposed during program execution. One promising approach is to adopt a program as a communication channel model and leverage information theoretic metrics to quantify such information flows. However, recent research has shown discrepancies in such information theoretic metrics: for example, Smith et. al. [5] showed examples wherein using the classical Shannon entropy measure for quantifying information flows may be counter-intuitive. Smith et. al. [5] proposed a vulnerability measure in an attempt to resolve this problem, and this measure was subsequently enhanced by Hamadou et. al. [2] into a belief-vulnerability metric. However, as pointed out by Smith et. al., the vulnerability metric fails to distinguish between certain classes of programs (such as the password checker and the binary search program). In this paper, we propose a simple and intuitive approach to quantify program information leakage as a probability distribution over the residual uncertainty of the high input whose mean, variance and worst case measures offer insights into program vulnerability.' as abstract.

there is a document named 'doc-1674' that
  has 'Heterogeneous sensor networks are increasingly deployed to support users in the field requiring many different kinds of sensing tasks. There may be multiple alternative kinds of sensors suitable for a given task. Sensing tasks might compete for the exclusive usage of available sensors. Such an environment is highly dynamic with users moving and generating tasks at different rates. Users typically lack the time and expertise to manually decide which are the best sensors for their tasks. We need therefore to design a distributed system to automatically allocate sensors to tasks. We formalize this problem as MultiSensor Task Allocation (MSTA) and show that the heterogeneity of sensors and tasks requires knowledge-based sensor-task matching. We extend a pre-existent well known coalition formation protocol to propose a novel layered distributed system which by using qualitative and quantitative measures provides allocation flexibility. We demonstrate that it is feasible to perform the knowledge-based sensor-task matching on a user\'s device by presenting a proof-of-concept mobile app which allows a user in the field to interact with the system. We run simulations to demonstrate that our architecture is scalable and that the allocation quality improves by allowing preemption of sensing resources from ongoing tasks and a reallocation mechanism.' as abstract.

there is a document named 'doc-1679' that
  has 'In this paper we describe a software assistant agent that can proactively assist human users situated in a time-constrained environment to perform normative reasoning-reasoning about prohibitions and obligations-so that the user can focus on her planning objectives. In order to provide proactive assistance, the agent must be able to 1) recognize the user\'s planned activities, 2) reason about potential needs of assistance associated with those predicted activities, and 3) plan to provide appropriate assistance suitable for newly identified user needs. To address these specific requirements, we develop an agent architecture that integrates user intention recognition, normative reasoning over a user\'s intention, and planning, execution and replanning for assistive actions. This paper presents the agent architecture and discusses practical applications of this approach.' as abstract.

there is a document named 'doc-1680' that
  has 'On the semantic web, information from several sources (with disparate trust levels) may be fused with the goal of answering complex queries from end users. In such context, it is critical to aggregate information from heterogeneous sources and support trust-based query answering over the integrated knowledge base. In this paper, we describe a scalable probabilistic query answering over complex, uncertain and partially consistent knowledge bases. The key contributions in this paper are fourfold. First, our approach provides an intuitive query answering semantics over a probabilistic knowledge base. Second, our approach tolerates inconsistencies in knowledge bases. Third, we propose and evaluate a scalable error-bounded approximation query answering over a large knowledge base. Finally, we empirically show the value of taking into account trust information in the query answering process.' as abstract.

there is a document named 'doc-1685' that
  has 'We present achievable scaling results on the pernode secure throughput that can be realized in a large random wireless network of n legitimate nodes in the presence of m eavesdroppers of unknown location. We consider both one-dimensional and two-dimensional networks. In the one-dimensional case, we show that a per-node secure throughput of order 1/n is achievable if the number of eavesdroppers satisfies m = o(n/log n). We obtain similar results for the two-dimensional case, where a secure throughput of order 1/ _ n log n is achievable under the same condition. The number of eavesdroppers that can be tolerated is significantly higher than previous works that address the case of unknown eavesdropper locations. The key technique introduced in our construction to handle unknown eavesdropper locations forces adversaries to intercept a number of packets to be able to decode a single message. The whole network is divided into regions, where a certain subset of packets is protected from adversaries located in each region. In the one-dimensional case, our construction makes use of artificial noise generation by legitimate nodes to degrade the signal quality at the potential locations of eavesdroppers. In the two-dimensional case, the availability of many paths to reach a destination is utilized to handle collaborating eavesdroppers of unknown location.' as abstract.

there is a document named 'doc-1726' that
  has 'In a broadcast channel where idle users are internal eavesdroppers, the secrecy capacity approaches zero when the number of users is large. We propose a two-way secrecy scheme to improve the secrecy capacity and analyse it in large network. The proposed scheme makes use of the forward and backward channels to transmit a secret key and a one-time pad encoded secret message. Relays are also used to prevent the eavesdropper from gaining an advantage of multi-user diversity. Analytical and simulation results justify that the proposed two-way secrecy scheme achieves positive secrecy rate even when the number of users approaches infinity.' as abstract.

there is a document named 'doc-1738' that
  has 'The net-centric ISR/ISTAR networks are expected to play a crucial role in the success of critical tasks such as base perimeter protection, border patrol and so on. To accomplish these tasks in an effective and expedient manner, it is important that these networks have the embedded capabilities to discover, delegate, and gather relevant information in a timely and robust manner. In this paper, we present a system architecture and an implementation that combines a service based reasoning mechanism with a sensor middleware infrastructure so that tasks can be executed efficiently and effectively. A knowledge base, utilising the Semantic Web technologies, provides the foundation for reasoning mechanism that assists users to discover, identify and allocate resources that are made available through the middleware, in order to satisfy the needs of tasks. Once resources are allocated to any given task, they can be accessed, controlled, shared, and their data feeds consumed through the Fabric middleware. We use the semantic descriptions from the knowledge base to annotate the resources (types, capabilities, etc.) in the sensor middleware so that they can be retrieved for reasoning during the discovery and identification phases. The reasoner is implemented as a HTTP web service, with the following characteristics: 1) Computational intensive operations are off-loaded to dedicated nodes, preserving the resources in the ISR/ISTAR networks. 2) HTTP services are accessible through a standard set of APIs irrespective of the reasoner technology used. 3) Support for seamless integration of different reasoners into the system.' as abstract.

there is a document named 'doc-1742' that
  has 'Sensor networks are used for applications in monitoring harsh environments including reconnaissance and surveillance of areas that may be inaccessible to humans. Such applications depend on reliable collection, distribution and delivery of information to processing centres which may involve multi-hop wireless networks which experience disruptions in communication and exhibit packet drops, connectivity loss and congestion. Some of these faults are periodic, attributed to external, recurring factors. In this paper, we study an effective way to forecast such repetitive conditions using time-series analysis. We, further, present an application-level, autonomic routing service that adapts sensor readings routes to avoid areas in which failures or congestion are expected. A prototype system of the approach is developed based on an existing middleware solution for sensor network management. Simulation results on the performance of this approach are also presented.' as abstract.

there is a document named 'doc-1744' that
  has 'Our previous work has explored the application of enterprise middleware techniques at the edge of the network to address the challenges of delivering complex sensor network solutions over heterogeneous communications infrastructures. In this paper, we develop this approach further into a practicable, semantically rich, model-based design and analysis approach that considers the sensor network and its contained services as a service-oriented architecture. The proposed model enables a systematic approach to service composition, analysis (using domain-specific techniques), and deployment. It also enables cross intelligence domain integration to simplify intelligence gathering, allowing users to express queries in structured natural language (Controlled English).' as abstract.

there is a document named 'doc-1748b' that
  has 'A wide range of forwarding strategies have been developed for multi-hop wireless networks, considering the broadcast nature of the wireless medium and the presence of random fading that results in time-varying and unreliable transmission quality. Two recently proposed strategies are opportunistic forwarding, which exploits relay diversity by opportunistically selecting an overhearing relay as a forwarder, and cooperative forwarding, which relies on the synchronized transmissions of relays to reinforce received signal strengths. Although these strategies are well-known in the literature, there lacks a thorough comparative analysis of their network-level performance in a realistic SINR (signal-to-interference-and-noise ratio) setting. In this paper, we develop Markov chain models for these protocols in the case of multiple competing flows in a general network setting. We first evaluate these models in simple small-scale networks, finding that opportunism often outperforms cooperation - a result corroborated by simulations in more general network settings. We also present an approximate fixedpoint model to efficiently compute the throughput of the Markov chain performance models in large networks. We identify the interference resulting from the larger number of transmissions under cooperative forwarding as a cause for mitigating the potential gains achievable with cooperative forwarding.' as abstract.

there is a document named 'doc-1753' that
  has 'In this paper we present a framework and a set of algorithms for determining faults in networks when large scale outages occur. The design principles of our algorithm, netCSI, are motivated by the fact that failures are geographically clustered in such cases. We address the challenge of determining faults with incomplete symptom information due to a limited number of reporting nodes in the network. netCSI consists of two parts: hypotheses generation algorithm, and ranking algorithm. When constructing the hypotheses list of potential causes, we make novel use of the positive and negative symptoms to improve the precision of the results. The ranking algorithm is based on conditional failure probability models that account for the geographic correlation of the network objects in clustered failures. We evaluate the performance of netCSI for networks with both random and realistic topologies. We compare the performance of netCSI with an existing fault diagnosis algorithm, MAX-COVERAGE, and achieve an average gain of 128% in accuracy for realistic topologies.' as abstract.

there is a document named 'doc-1773' that
  has 'In multi-hop wireless networks, per-hop forwarding strategies that optimize local transmissions can have a subtle impact on the network-wide performance. Motivated by a number of scenarios for improving signal strength or mitigating interference, we study a fundamental problem that arises in a wireless ad hoc network with directional transmission (e.g., using directional antennas), where nodes are randomly placed with their transmission footprints (each as a sector) aligned toward the destinations. Only the nodes located in the transmission footprint of a transmitter will act as its forwarders. Our study addresses the connectivity of this setting. We first examine through simulation the percolation probability and the number of cross-area paths available to directional transmission, at different spread angles of transmission footprints. We observe that there is a critical spread angle, above which there is little impact on these properties. Analytically, we derive upper and lower bounds for the critical thresholds. Moreover, we show that with high probability there exist at least _(n/ log n) number of disjoint paths across an strip area of n _ _(n), when above the critical thresholds. Our results cast insights on optimizing directional transmission in wireless ad hoc networks.' as abstract.

there is a document named 'doc-1776' that
  has 'We study the problem of computing on large datasets that are stored on an untrusted server. We follow the approach of amortized verifiable computation introduced by Gennaro, Gentry, and Parno in CRYPTO 2010. We present the first practical verifiable computation scheme for high degree polynomial functions. Such functions can be used, for example, to make predictions based on polynomials fitted to a large number of sample points in an experiment. In addition to the many non-cryptographic applications of delegating high degree polynomials, we use our verifiable computation scheme to obtain new solutions for verifiable keyword search, and proofs of retrievability. Our constructions are based on the DDH assumption and its variants, and achieve adaptive security, which was left as an open problem by Gennaro et al (albeit for general functionalities). Our second result is a primitive which we call a verifiable database (VDB). Here, a weak client outsources a large table to an untrusted server, and makes retrieval and update queries. For each query, the server provides a response and a proof that the response was computed correctly. The goal is to minimize the resources required by the client. This is made particularly challenging if the number of update queries is unbounded. We present a VDB scheme based on the hardness of the subgroup membership problem in composite order bilinear groups. This is the first such construction that relies on a "constant-size" assumption, and does not require expensive generation of primes per operation.' as abstract.

there is a document named 'doc-1778' that
  has 'In a recent paper by Boneh et al. [BSW11], the authors define of a new cryptographic primitive called functional encryption, which allows the issuer of decryption keys to determine not just who can decrypt, but also the precise function of the plaintext that the key holder will learn during decryption. The authors demonstrate the generality of their definition, showing that it captures many of the existing primitives that cryptographers know and love: identity based encryption, attribute based encryption, predicate encryption, and others. In this work, we extend the definition of functional encryption to capture the notion of homomorphic encryption. We demonstrate connections between functional encryption and fully homomorphic encryption, showing how to build the latter from any functional encryption scheme that supports a very natural set of functions. We also draw connections to simpler, single-operation homomorphic encryption schemes. Finally, in the process of reaching these results, we demonstrate several problems with the existing definitions of functional encryption, which we believe will help guide future progress.' as abstract.

there is a document named 'doc-1802' that
  has 'We describe a method for policy refinement. The refinement process involves stages of decomposition, operationalization, deployment and re-refinement, and operates on policies expressed in a logical language flexible enough to be translated into many different enforceable policy dialects. We illustrate with examples from a coalition scenario, and describe how the stages of decomposition and operationaliztion work internally, and fit together in an interleaved fashion. Domains are represented in a logical formalization of UML diagrams. Both authorization and obligation policies are supported.' as abstract.

there is a document named 'doc-1809' that
  has 'We show that the probability of source routing success in dynamic networks, where the link up-down dynamics is governed by a time-varying stochastic process, exhibit critical phase-transition (percolation) phenomena as a function of the end-to-end message latency per unit path length. We evaluate the probability of routing success on dynamic network (1D and 2D) lattices with links going up and down as per an arbitrary binary-valued stationary random process (such as a Markov process), in a source-routing framework. We find percolation thresholds on the time deadline for high-probability of routing success in terms of the first and second order moments of the link state process and we also demonstrate percolation thresholds on the parameters characterizing the link process for a fixed time deadline for a 1D Markov network as an example. This work happens to generalize results reported in two articles from the 80\'s that appeared in the Physical Review Letters on directed percolation theory. We analyzed the performance of a stateless single-copy opportunistic forwarding algorithm on a 2D probabilistic grid - it does not demonstrate a non-trivial percolation threshold in link-up probability as does a flooding based approach. However, interestingly, when we add a time dimension, i.e., let the network evolve as per a potentially timecorrelated link dynamics, the opportunistic "store and forward" routing algorithm also exhibits a critical threshold behavior. In this 2D grid network case (as in the 1D network case), the normalized messaging latency (ratio of routing latency to path length) exhibits a critical phase transition, where we evaluate the critical latency to path-length ratio as a function of the moments of the link up-down process.' as abstract.

there is a document named 'doc-1835' that
  has 'The emergence of large scale, distributed, sensorenabled, machine-to-machine pervasive applications necessitates engaging with providers of information on demand to collect the information, of varying quality levels, to be used to infer about the state of the world and decide actions in response. In these highly fluid operational environments, involving information providers and consumers of various degrees of trust and intentions, obfuscation of information is used to protect providers from misuses of the information they share, while still providing benefits to their information consumers. In this paper, we develop the initial principles for relating to trust and obfuscation within the context of this emerging breed of applications. We start by extending the definitions of trust and obfuscation into this emerging application space. We, then, highlight their role as we move from tightlycoupled to loosely-coupled sensory-inference systems. Finally, we present the interplay between trust and obfuscation as well as the implications for reasoning under obfuscation.' as abstract.

there is a document named 'doc-1838' that
  has 'In multi-agent systems, agents often depend on others to perform different aspects of shared goals. In such settings, deciding to whom to delegate a task could be complicated because activities of collaborators may be regulated by policies. Especially when such policies are private, learning these policies may become crucial to estimate the outcome of proposed task delegation decisions. In this paper, we present an approach that utilises ontological reasoning to aid the learning of policies. Our approach combines ontological reasoning, machine learning and argumentation in a novel way to accomplish this. Using our approach, autonomous agents can reason about the policies that others are operating with, and make informed decisions about to whom to delegate a task (or approach for the provision of certain resources). In a series of experiments, we demonstrate the utility of this novel combination of techniques. Our empirical evaluation shows that models of others\' policies can be refined through ontological reasoning, and such models can be used to guide future task delegations.' as abstract.

there is a document named 'doc-1839' that
  has 'ABox abduction is the process of finding statements that should be added to an ontology to entail a specific conclusion. In this paper, we propose an approach for probabilistic abductive reasoning for SHIQ. Our evaluations show that the proposed approach significantly extends classical abduction by effectively and correctly estimating probabilities for abductive explanations. Lastly, based on the ideas proposed for SHIQ, we describe a tractable algorithm for DL-LiteR.' as abstract.

there is a document named 'doc-1842' that
  has 'In coalition operations, information from different sources belonging to different organisations has to be gathered and aggregated. Such information may not be consistent, and inconsistencies in the gathered information create severe uncertainties that hinder the usefulness of the information. In this paper, we propose a Subjective Logic based approach for modelling the trustworthiness of information sources within a specific context. This model is used to handle inconsistencies through filtering information from less trustworthy sources.' as abstract.

there is a document named 'doc-1847' that
  has 'Heterogeneous sensor networks are increasingly deployed to support users in the field requiring many different kinds of ISR tasks. There may be multiple alternative kinds of sensors suitable for a given task. A task may be best served with a combination of sensors of different types. Sensing tasks might compete for the exclusive usage of available sensors. Access to sensors will typically be governed by policies. Such an environment is highly dynamic with users moving and generating tasks at different rates. Users typically lack the time and expertise to manually decide which are the best sensors for their tasks. We formalize this problem as Multi-Sensor Task Allocation (MSTA) and show that the heterogeneity of sensors and tasks requires knowledge-based sensor-task matching. We propose a layered distributed system that uses qualitative and quantitative mechanisms to provide sensor-task allocation flexibility and agility. We demonstrate that it is feasible to perform knowledge-based sensor-task matching on a local device by presenting a proof-of-concept mobile app that allows a user in the field to interact with the system. Our system exploits local knowledge at the edges of the network, and supports "social" sharing of ISR resources.' as abstract.

there is a document named 'doc-1849a' that
  has 'In a military scenario, commanders need to determine what kinds of information will help them execute missions. The amount of information available to support each mission is constrained by the availability of information assets. For example, there may be limits on the numbers of sensors that can be deployed to cover a certain area, and limits on the bandwidth available to collect data from those sensors for processing. Therefore, options for satisfying information requirements should take into consideration constraints on the underlying information assets, which in certain cases could simultaneously support multiple missions. In this paper, we propose a system architecture for modeling missions and allocating information assets among them. We model a mission as a graph of tasks with temporal and probabilistic relations. Each task requires some information provided by the information assets. Our system suggests which information assets should be allocated among missions. Missions are compatible with each other if their needs do not exceed the limits of the information assets; otherwise, feedback is sent to the commander indicating information requirements need to be adjusted. The decision loop will eventually converge and the utilization of the resources is maximized.' as abstract.

there is a document named 'doc-1855a' that
  has 'We introduce an approach to representing intelligence, surveillance, and reconnaissance (ISR) tasks at a relatively high level in controlled natural language. We demonstrate that this facilitates both human interpretation and machine processing of tasks. More specifically, it allows the automatic assignment of sensing assets to tasks, and the informed sharing of tasks between collaborating users in a coalition environment. To enable automatic matching of sensor types to tasks, we created a machine-processable knowledge representation based on the Military Missions and Means Framework (MMF), and implemented a semantic reasoner to match task types to sensor types. We combined this mechanism with a sensor-task assignment procedure based on a well-known distributed protocol for resource allocation. In this paper, we re-formulate the MMF ontology in Controlled English (CE), a type of controlled natural language designed to be readable by a native English speaker whilst representing information in a structured, unambiguous form to facilitate machine processing. We show how CE can be used to describe both ISR tasks (for example, detection, localization, or identification of particular kinds of object) and sensing assets (for example, acoustic, visual, or seismic sensors, mounted on motes or unmanned vehicles). We show how these representations enable an automatic sensor-task assignment process. Where a group of users are cooperating in a coalition, we show how CE task summaries give users in the field a high-level picture of ISR coverage of an area of interest. This allows them to make efficient use of sensing resources by sharing tasks.' as abstract.

there is a document named 'doc-1857' that
  has 'Plan recognition, cognitive workload estimation and human assistance have been extensively studied in the AI and human factors communities, resulting in many techniques being applied to domains of various levels of realism. These techniques have seldom been integrated and evaluated as complete systems. In this paper, we report on the development of an assistant agent architecture that integrates plan recognition, current and future user information needs, workload estimation and adaptive information presentation to aid an emergency response manager in making high quality decisions under time stress, while avoiding cognitive overload. We describe the main components of a full implementation of this architecture as well as a simulation developed to evaluate the system. Our evaluation consists of simulating various possible executions of the emergency response plans used in the real world and measuring the expected time taken by an unaided human user, as well as one that receives information assistance from our system. In the experimental condition of agent assistance, we also examine the effects of different error rates in the agent\'s estimation of user\'s stat or information needs.' as abstract.

there is a document named 'doc-1858' that
  has 'This work investigates decision level fusion by extending the framework of subjective logic to account for hidden observations. Bayes rule might suggest that decision level fusion is simply calculated as the normalized product of the class likelihoods of the various classifiers. However, this product rule suffers from a veto issue. The problem with the classical Bayes formulation is that it does not account for uncertainties inherent in the likelihoods exclaimed by the classifiers. This paper uses subjective logic as a rigorous framework to incorporate uncertainty. First, a class appearance model is introduced that roughly accounts for the disparity between training and testing conditions. Then, the subjective logic framework is expanded to account for the fact that class appearances are not directly observed. Rather, a classifier only returns the likelihood for the class appearance. Finally, the paper uses simulations to compare the new subjective logic framework to traditional classifier fusion methods in terms of classification performance and the ability to estimate the parameters of the class appearance model.' as abstract.

there is a document named 'doc-1860' that
  has 'The combination of service-oriented applications, with their run-time service binding, and mobile ad hoc networks, with their transient communication topologies, brings a new level of complex dynamism to the structure and behavior of software systems. This complexity challenges our ability to understand the dependence relationships among system components when attempting to perform analyses such as fault localization and impact analysis. Current methods of dynamic dependence discovery, developed for use in fixed networks, assume that dependencies change slowly. Moreover, they require relatively long monitoring periods as well as substantial memory and communication resources, which are impractical in the mobile ad hoc network environment. We describe a new method, designed specifically for this environment, that allows the engineer to trade accuracy against cost, yielding dynamic snapshots of dependence relationships. Through extensive simulations, we evaluate the performance of our method in terms of the accuracy of the discovered dependencies, and draw insights on the selection of critical parameters under various operational conditions.' as abstract.

there is a document named 'doc-1861' that
  has 'Trust mechanisms allow a trustor to identify the most trustworthy trustee with which to interact. Such interactions can take the form of task assignments, in which case the trustor can be seen to delegate the task to the trustee. Now this process of delegation can operate recursively, with a task being repeatedly sub-delegated until some agent acts upon it. Such delegation chains present a problem for current trust evaluation mechanisms, which reward or penalise a single agent, as responsibility for the outcome of a task must be shared among all members of the delegation chain. As a result, current approaches lead to agents being unfairly penalised (or rewarded) during trust update, adversely affecting the quality of the model\'s evaluations, and through this, future agent interactions. In this paper we investigate the effects of sub-delegation on a probabilistic trust model and propose a model of weighting trust updates based on shared responsibility. We evaluate this model in the context of a simulated multi-agent system and describe how different weighting strategies can affect probabilistic trust updates when sub-delegation is possible.' as abstract.

there is a document named 'doc-1862' that
  has 'Low-cost navigation solutions for indoor environments have a variety of real-world applications ranging from emergency evacuation to mobility aids for people with disabilities. Challenges for indoor navigation include robust localization in the absence of GPS, intuitive recognition of user navigation goals, and efficient route-planning and replanning techniques. In this paper, we present an architecture for indoor navigation that integrates observed behavior for recognizing user navigation goals and estimating future paths without direct input from the user. Our integrated system comprises of three core components: effective localization, map representation and route planning, and plan recognition. To evaluate the feasibility of the proposed architecture, we develop a prototype application on a commercial smart-phone. The developed application was tested in an indoor environment and was found to accurately predict intended destination and to effectively navigate the user to the identified destination.' as abstract.

there is a document named 'doc-1863' that
  has 'Systems of autonomous and self-interested agents interacting to achieve individual and collective goals may exhibit undesirable or unexpected properties if left unconstrained. Using deontic concepts of obligations, permissions and prohibitions to describe, what must, may and should not be done, norms have been widely proposed as a means of defining and enforcing societal constraints. Recent efforts to provide norm-enabled agent architectures that limit plan choices suffer from interfering with an agent\'s reasoning process, and thus limit autonomy more than is required by the norms alone. In response, in this paper we describe an extension of the Beliefs-DesiresIntentions (BDI) architecture which enables normative reasoning, providing agents with a means to choose and customise plans (and their constituent actions), so as to ensure compliance with norms. The paper makes three significant contributions in providing: fine-grained tailoring of plan restrictions; a plan annotation mechanism to identify plan instances which violate norms, and limit their adoption; and a technique to enable the selective and incremental violation of norms in cases where goal achievement would not otherwise be possible.' as abstract.

there is a document named 'doc-1866' that
  has 'We consider the problem of endhost-based shortest path routing in a network with unknown, time-varying link qualities. Endhost-based routing is needed when internal nodes of the network do not have the scope or capability to provide globally optimal paths to given source-destination pairs, as can be the case in networks consisting of autonomous subnetworks or those with endhost-based routing restrictions. Assuming the source can probe links along selected paths, we formulate the problem as an online learning problem, where an existing solution achieves a performance loss (called regret) that is logarithmic in time with respect to (wrt) an offline algorithm that knows the link qualities. Current solutions assume coupled probing and routing; in contrast, we give a simple algorithm based on decoupled probing and routing, whose regret is only constant in time. We then extend our solution to support multi-path probing and cooperative learning between multiple sources, where we show an inversely proportional decay in regret wrt the probing rate. We also show that without the decoupling, the regret grows at least logarithmically in time, thus establishing decoupling as critical for obtaining constant regret. Although our analysis assumes certain conditions (i.i.d.) on link qualities, our solution applies with straightforward amendments to much broader scenarios where these conditions are relaxed. The efficacy of the proposed solution is verified by trace-driven simulations.' as abstract.

there is a document named 'doc-1872' that
  has 'Physical layer secrecy in wireless networks in the presence of eavesdroppers of unknown location is considered. Many schemes have been proposed to deal with eavesdroppers based on the introduction of cooperative jamming. However, this required energy expenditure that may not be tolerated. In this paper, we develop schemes that multiple transmitters send their signals in a cooperative fashion to confuse the eavesdroppers. In contrast to previous approaches, power is not expended on "artificial noise"; rather, the signal of a given transmitter is protected by the aggregate interference produced by the other transmitters. We introduce a two-hop strategy for the case of equal path-loss between all pairs of nodes, and a multi-hop strategy for the general case of an extended network. In each case, we derive an achievable number of eavesdroppers that can be present in the system while secure communication between all sources and intended destinations is ensured.' as abstract.

there is a document named 'doc-1874' that
  has 'An understanding of information flow has many applications, including for maximising marketing impact on social media, limiting malware propagation, and managing undesired disclosure of sensitive information. This paper presents scalable methods for both learning models of information flow in networks from data, based on the Independent Cascade Model; and predicting probabilities of unseen flow from these models. Our approach is based on a principled probabilistic construction and results compare favourably with existing methods in terms of accuracy of prediction and scalable evaluation, and we describe how to evaluate a broader range of queries than previously shown. Exact evaluation of flow probabilities is exponential in the number of edges and naive sampling can also be expensive, so we propose sampling in an efficient Markov-Chain Monte-Carlo fashion using the Metropolis-Hastings algorithm, described in detail in the paper. Other models make simplifying assumptions about information propagation, for example, constraining the network topology or treating information as a fluid; our model is flexible without requiring any such constraints, and can be used to learn joint and conditional flow probabilities, as well as distributions over flow likelihood. We identify two types of data, those where the paths of past flows are known - attributed data, and those where only the endpoints are known - unattributed data. Both data types are addressed in this paper, including training methods, example real world data sets, and experimental evaluation. In particular, we investigate flow data from the Twitter micro-blogging service, exploring the flow of messages through retweets (tweet forwards) for the attributed case, and the propagation of hashtags (metadata tags) and urls for the unattributed case.' as abstract.

there is a document named 'doc-1875' that
  has 'This paper addresses the making of security decisions, such as access-control decisions or spam filtering decisions, under uncertainty, when the benefit of doing so outweighs the need to absolutely guarantee these decisions are correct. For instance, when there are limited, costly, or failed communication channels to a policy-decision-point. Previously, local caching of decisions has been proposed, but when a correct decision is not available, either a policy-decision-point must be contacted, or a default decision used. We improve upon this model by using learned classifiers of access control decisions. These classifiers, trained on known decisions, infer decisions when an exact match has not been cached, and uses intuitive notions of utility, damage and uncertainty to determine when an inferred decision is preferred over contacting a remote PDP. Clearly there is uncertainty in the predicted decisions, introducing a degree of risk. Our solution proposes a mechanism to quantify the uncertainty of these decisions and allows administrators to bound the overall risk posture of the system. The learning component continuously refines its models based on inputs from a central policy server in cases where the risk is too high or there is too much uncertainty. We have validated our models by building a prototype system and evaluating it with requests from real access control policies. Our experiments show that over a range of system parameters, it is feasible to use machine learning methods to infer access control policies decisions. Thus our system yields several benefits, including reduced calls to the PDP, reducing latency and communication costs; increased net utility; and increased system survivability.' as abstract.

there is a document named 'doc-1877' that
  has 'Miniaturized smart sensors are increasingly being used to collect personal data which embed minute details of our everyday life. The sensory data stream can easily be mined to draw a rich set of behavioral inferences such as addictions, travel patterns and social inclinations - some of which may be sensitive to the data provider. This notion of behavioral privacy is unique to sensory data streams and is different from the traditional identity privacy typically addressed in literature. In this paper, we summarize the privacy concerns of a provider into three basic questions: (i) How much data to share? (ii) Whom to share it with? and (iii) What data to share? We first define a linear program which takes into account the underlying trust network between consumers and providers to derive the optimal resolution at which to share data with each consumer. We then describe SensorSafe, which allows the provider to express fine-grained access control policies. Finally, we formalize behavioral privacy within the framework of white and black listed functions represented in a feature space and illustrate a compressive sensing based privacy mechanism for solving this class of inference problems.' as abstract.

there is a document named 'doc-1881' that
  has 'An approach to the modelling of team sensemaking is presented that relies on the use of multiple agents integrated into larger communication network structures. Sensemaking is cast as a type of constraint satisfaction problem, and thus the cognitive architecture of each agent within the model is implemented as a constraint satisfaction network. The effect of manipulating a number of communication variables (the frequency of inter-agent communication, the type of information communicated and the point at which inter-agent communication takes place) are explored in three computer simulation studies. The results suggest that precipitant forms of information sharing may result in agents assigning undue significance to information that is largely consistent or compatible with pre-existing or prevailing cognitions. These results are consistent with other results reported in the distributed cognition literature, and they suggest that the future use of constraint satisfaction network models could have value in terms of improving our understanding of sociallydistributed cognition in military coalition environments.' as abstract.

there is a document named 'doc-1895' that
  has 'As the semantic web expands, ontological data becomes distributed over a large network of data sources on the Web. Consequently, evaluating queries that aim to tap into this distributed semantic database necessitates the ability to consult multiple data sources efficiently. In this paper, we propose methods and heuristics to effi- ciently query distributed ontological data based on a series of properties of summarized data. In our approach, each source summarizes its data as another RDF graph, and relevant section of these summaries are merged and analyzed at query evaluation time. We show how the analysis of these summaries enables more efficient source selection, query pruning and transformation of expensive distributed joins into local joins.' as abstract.

there is a document named 'doc-1897' that
  has 'Resources in modern computer systems include not only CPU, but also memory, hard disk, bandwidth, etc. To serve multiple users simultaneously, we need to satisfy their requirements in all resource dimensions. Meanwhile, their demands follow a certain distribution and may change over time. Our goal is then to admit as many users as possible to the system without violating the resource capacity more often than a predefined overflow probability. In this paper, we study the problem of allocating multiple resources among a group of users/tasks with stochastic demands. We model it as a stochastic multidimensional knapsack problem. We extend and apply the concept of effective bandwidth in order to solve this problem efficiently. Via numerical experiments, we show that our algorithms achieve near-optimal performance with specified overflow probability.' as abstract.

there is a document named 'doc-1901' that
  has 'Location-based services, which employ data from smartphones, vehicles, etc., are growing in popularity. To reduce the threat that shared location data poses to a user\'s privacy, some services anonymize or obfuscate this data. In this paper, we show these methods can be effectively defeated: a set of location traces can be deanonymized given an easily obtained social network graph. The key idea of our approach is that a user may be identified by those she meets: a contact graph identifying meetings between anonymized users in a set of traces can be structurally correlated with a social network graph, thereby identifying anonymized users. We demonstrate the effectiveness of our approach using three real world datasets: University of St Andrews mobility trace and social network (27 nodes each), SmallBlue contact trace and Facebook social network (125 nodes), and Infocom 2006 bluetooth contact traces and conference attendees\' DBLP social network (78 nodes). Our experiments show that 80% of users are identified precisely, while only 8% are identified incorrectly, with the remainder mapped to a small set of users.' as abstract.

there is a document named 'doc-1902' that
  has 'In this paper, we propose an algorithm to effi- ciently diagnose large-scale clustered failures. The algorithm, Cluster-MAX-COVERAGE (CMC), is based on greedy approach. We address the challenge of determining faults with incomplete symptoms. CMC makes novel use of both positive and negative symptoms to output a hypothesis list with a low number of false negatives and false positives quickly. CMC requires reports from about half as many nodes as other existing algorithms to determine failures with 100% accuracy. Moreover, CMC accomplishes this gain significantly faster (sometimes by two orders of magnitude) than an algorithm that matches its accuracy. Furthermore, we propose an adaptive algorithm called Adaptive-MAX-COVERAGE (AMC) that performs efficiently during both kinds of failures, i.e., independent and clustered. During a series of failues that include both independent and clustered, AMC results in a reduced number of false negatives and false positives.' as abstract.

there is a document named 'doc-1924' that
  has 'A wide range of wireless channel models have been developed to model variations in received signal strength. In contrast to prior work, which has focused primarily on channel modeling on a short, per- packet timescale (millisecond), we develop and validate a finite-state Markov chain model that captures variations due to shadowing, which occur at coarser time scales. The Markov chain is constructed by partitioning the entire range of shadowing into a finite number of intervals. We determine the Markov chain transition matrix in two ways: (i) via an abstract modeling approach in which shadowing effects are modeled as a log-normally distributed random variable affecting the received power, and the transition probabilities are derived as functions of the variance and autocorrelation function of shadowing; (ii) via an empirical approach, in which the transition matrix is calculated by directly measuring the changes in signal strengths collected in a 802.16e (WiMAX) network. We validate the abstract model by comparing its steady state and transient performance predictions with those computed using the empirically derived transition matrix and those observed in the actual traces themselves.' as abstract.

there is a document named 'doc-1925' that
  has 'Although wireless networks have become ubiquitous, surprisingly few models of user-level mobility have been developed and validated against traces of measured user behavior. In this paper, we develop and validate a simple mixed queueing network model of user mobility among access points in a campus network. We identify two classes of users, an open and a closed class, corresponding to mobile users that visit the network for a short time before departure, and users that are always resident in the network during the observation period. Using CRAWDAD traces of user-access-point affiliation over time, we compare model-predicted performance with the performance actually observed in the traces, and find that such a mixed queueing model can indeed be used to accurately predict a number of performance measures of interest.' as abstract.

there is a document named 'doc-1926' that
  has 'Current and future coalition operations involve multi-team and/or multi-nation collaborations. While large volumes of structured/unstructured data are often available, improvement of data access, information extraction, and knowledge sharing is critically important but remains a major challenge for effective and efficient C2 operations. In this paper, we propose an approach to information extraction using International Technology Alliance Controlled English (CE) to improve fact extraction and knowledge sharing, aiming to enhance situation awareness and support decision-making. CE is a subset of English with a restricted grammar to reduce complexity and avoid ambiguity. The current version of CE has a formal syntax and semantics and is consistent with First Order Predicate Logic. CE is used to model both the inputs and outputs of the information extraction process, and to support end-users in configuring information extraction tools. Thus, CE provides, among other things: (i) A user-friendly language for queries and system-to-user report representation. (ii) A common form of expression that supports extending and modifying domain models (ontologies), and enables mapping between models and terminology or language variants. CE-based information extraction will greatly facilitate the processes in the cognitive and social domains that enable forces with diverse backgrounds to collaborate effectively and efficiently' as abstract.

there is a document named 'doc-1927' that
  has 'Subjective logic (SL) is an effective tool to manage and update beliefs over a set of mutually exclusive assertions. The method to update subjective beliefs from direct observations of assertions is well understood. Recent work has incorporated the SL framework to derive the belief update equations for partial observations where the measurements are only statistically related to the assertions. This work further expands the notion of SL to consider uncertainty in the underlying statistical relationship between measurements and assertions. In other words, new methods are derived for SL that incorporate uncertainty in the reported likelihood of the assertions. Simulations demonstrate the utility of the new likelihood uncertainty aware belief update methods.' as abstract.

there is a document named 'doc-1928' that
  has 'Sensory information is characterized by its inherent quality and context-specific value. It is thus natural that the information provider would want to exercise control over the information shared based on her perception of the risk of possible misuse due to sharing and also depending on the consumer requirements. To attain this utility vs. risk trade-off, information is subjected to varying but deliberate quality modifying transformations which we term as obfuscation. In this paper, treating privacy as the primary motivation for information control, we highlight initial considerations of using feature sharing as an obfuscation mechanism to control the inferences possible from shared sensory data. We provide results from an activity tracking scenario to illustrate the use of feature selection in identifying the various trade-off points.' as abstract.

there is a document named 'doc-1929' that
  has 'Over the last 15 years, many policy languages have been developed for specifying policies and credentials under the trust management paradigm. What has been missing is a formal semantics - in particular, one that would capture the inherently dynamic nature of trust management, where access decisions are based on the local policy in conjunction with varying sets of dynamically submitted credentials. The goal of this paper is to rest trust management on a solid formal foundation. To this end, we present a model theory that is based on Kripke structures for counterfactual logic. The semantics enjoys compositionality and full abstraction with respect to a natural notion of observational equivalence between trust management policies. Furthermore, we present a corresponding Hilbert-style axiomatization that is expressive enough for reasoning about a system\'s observables on the object level. We describe an implementation of a mechanization of the proof theory, which can be used to prove non-trivial metatheorems about trust management systems, as well as analyze probing attacks on such systems. Our benchmark results show that this logic-based approach performs significantly better than the only previously available, ad-hoc analysis method for probing attacks.' as abstract.

there is a document named 'doc-1930' that
  has 'THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD. We present a square root limit on low probability of detection (LPD) communication over additive white Gaussian noise (AWGN) channels. Specifically, if a warden has an AWGN channel to the transmitter with non-zero noise power, we prove that o( _ n) bits can be sent from the transmitter to the receiver in n AWGN channel uses with probability of detection by the warden less than  for any  > 0, and, if a lower bound on the noise power on the warden\'s channel is known, then O( _ n) bits can be covertly sent in n channel uses. Conversely, trying to transmit more than O( _ n) bits either results in detection by the warden with probability one or a non-zero probability of decoding error as n _ _. Further, we show that LPD communication on the AWGN channel allows one to send a nonzero symbol on every channel use, in contrast to what might be expected from the square root law found recently in image-based steganography.' as abstract.

there is a document named 'doc-1934' that
  has 'The use of market mechanisms to solve computer science problems such as resource sharing, load distribution and network routing, is gaining significant traction. In this paper, we investigate new market mechanisms to solve the problem of bandwidth sharing in wireless networks for transient traffic congestion often generated by event-driven packet flows. Typically, such congestion is transient at each node it arises since the bursts of data move following the event. We first demonstrate that a previously proposed strategy that greedily selects winners in repeated routing auctions is not globally optimal in such a case. We also demonstrate, by evaluating a lookahead mechanism for winner selection in the corresponding auctions, that greedy algorithm approximates optimal selection very closely. Then, we introduce and evaluate a novel mechanism which we call Traveling Auctions to address the problem of transient congestion. We experimentally show that using Traveling Auctions mechanism improves the network performance.' as abstract.

there is a document named 'doc-1936' that
  has 'Overlay network topologies provide different networking applications an abstraction over underlying network architecture. Therefore, their construction and the resulting topological characteristics play a crucial role in the performance of operations running in these applications. Thanks to their small diameters, scale-free (power-law) overlay network topologies are one of the structures that offer high performance for these networks. However, a key problem for such networks is the high connectivity (i.e., load) in only a small portion (i.e., hubs) of nodes. In fact, the nodes in such scale-free overlay networks may not want or be able to accomplish such high connectivity due to technical restrictions. Therefore, some hard cutoffs are often imposed on the number of edges that each node can have, making them limited scale-free networks. In this paper, we discuss and analyze the growth of such limited scale-free networks and propose an algorithm aiming to achieve perfect scale-free overlay network topologies with low communication overhead and without global information usage during its construction phase. Through extensive simulations, we also evaluate the proposed approach and show its superiority over the existing solutions.' as abstract.

there is a document named 'doc-1939' that
  has 'Since dynamic wireless networks evolve over time, optimal routing computations need to be performed frequently on time-varying network topologies. However, it is often infeasible or expensive to gather the current state of links for the entire network all the time. We provide a thorough analytical characterization of the effect of various link-state sampling strategies operating under a limited sampling budget on the performance of the minimum latency routing policy in a special class of dynamic networks. We show that for a two-state Markov link-dynamics model parameterized by probabilities p, q, if links are more likely to turn on than off at each time instant (p > q), a "depth-first" sampling strategy is optimal, whereas a "breadth-first" sampling strategy is optimal if links are more likely to turn off than on (p < q). We precisely characterize the optimal-latency spatial-sampling schedules for one-shot interrogation. Furthermore, we present numerical simulation results on comparing various spatio-temporal sampling schedules under an overall sampling rate constraint, and a few other probing cost models.' as abstract.

there is a document named 'doc-1941' that
  has 'In the standard setting of broadcast encryption, information about the receivers is transmitted as part of the ciphertext. In several broadcast scenarios, however, the identities of the users authorized to access the content are often as sensitive as the content itself. In this paper we propose the first broadcast encryption scheme with sublinear ciphertexts to attain meaningful guarantees of receiver anonymity. We formalize the notion of outsider-anonymous broadcast encryption (oABE), and describe generic constructions in the standard model that achieve outsider-anonymity under adaptive corruptions in the chosen-plaintext and chosen-ciphertext settings. We also describe two constructions with enhanced decryption, one under the gap Diffie-Hellman assumption, in the random oracle model, and the other under the decisional Diffie-Hellman assumption, in the standard model.' as abstract.

there is a document named 'doc-1952a' that
  has 'We introduce an approach to integrating access to hard and soft information sources to provide better exploitation of all available sources in the context of coalition data-to-decision (D2D) chains. In terms of hard (sensor-based) sources we show how intelligence, surveillance, and reconnaissance (ISR) assets can be represented at a relatively high level in controlled natural language, and how this allows the automatic assignment of sensing assets to D2D tasks. We demonstrate how the use of Controlled English (CE) - a type of controlled natural language designed to be readable by a native English speaker whilst representing information in a structured, unambiguous form - supports the informed sharing of D2D tasks and assets between collaborating users in a coalition environment. Moreover, we show how CE can be used in the automatic extraction of information from unstructured and semi-structured text information sources, providing us with a uniform way to integrate these soft sources with the aforementioned hard sources.' as abstract.

there is a document named 'doc-1954' that
  has 'Fast-paced data-to-decision systems are heavily dependent on the reliable sharing of sensor-derived information. At the same time a diverse collection of sensory information providers would want to exercise control over the information shared based on their perception of the risk of possible misuse due to sharing and also depending on the consumer requirements. To attain this utility vs. risk trade-off, information is subjected to varying but deliberate quality modifying transformations which we term as obfuscation. In this paper, treating privacy as the primary motivation for information control, we highlight initial considerations of using feature sharing as an obfuscation mechanism to control the inferences possible from shared sensory data. We provide results from an activity tracking scenario to illustrate the use of feature selection in identifying the various trade-off points.' as abstract.

there is a document named 'doc-1959' that
  has 'Collective sensemaking is a form of socially-distributed cognition (see Hutchins, 1995) in which multiple agents attempt to interpret (make sense of) specific bodies of environmental information. In order to optimize performance at the collective level, agents often need to share information about the results of their own processing activity, and this raises questions about how the structure of communication networks affects collective sensemaking abilities. In the current study, we used a computational model of collective sensemaking in which individual agents were implemented as constraint satisfaction networks (CSNs) (see Smart & Shadbolt, 2012). We then investigated how the cognitive responses of agents were affected by different kinds of communication network structure.' as abstract.

there is a document named 'doc-1997' that
  has 'Data-gathering or convergecast problems have traditionally been studied in two combinations of settings: oneshot scheduling of data items with no aggregation, and periodic scheduling of data items with full aggregation meaning that any number of unit-size data items can, if available, be aggregated into a single (unit-size) data item (e.g., by summing or averaging values). In this paper, we extend beyond these problem settings in two ways. First, we study a) one-shot throughput maximization in settings with aggregation and b) periodic scheduling in settings without aggregation. Second, we generalize the notion of aggregatability in both one-shot and periodic scheduling beyond the binary choice of either all sets of items being aggregatable or none being so. Modeling the presence of multiple semantic data types (e.g., target counts to be summed and temperature readings to be averaged), we partition data items into classes, whereby items are aggregatable if they belong to the same class, in both periodic and non-periodic settings. For these two problems we provide guaranteed approximations and heuristics, for a variety of general and special cases. We then evaluate the algorithms in a systematic simulation study, both under the conditions in which our provable guarantees apply and in more general settings, where we find the algorithms continue to perform well on typical problem inputs.' as abstract.

there is a document named 'doc-1998' that
  has 'A series of experiments were performed in order to explore the effect of communication network structure on collective sensemaking under a variety of informational conditions. A multi-agent computational model of collective sensemaking was used in which each agent was implemented as a constraint satisfaction network. Within the simulations, agents were tasked with the interpretation of information indicating the presence of a particular object, and they were allowed to share information with other agents while performing this task subject to the constraints imposed by the structure of a communication network. In all simulations, a minority of agents (5) received evidence in favor of one interpretation, while a majority of agents (15) received evidence in favor of a conflicting interpretation. Communication networks with four types of topological structure (i.e., disconnected, random, smallworld and fully-connected) were used in the experiments. The results suggest that network topology influences the extent to which minority views are able to influence collective cognitive outcomes. In particular, fully-connected networks deliver a performance profile in which minority influence is minimized in situations where both minority and majority groups are exposed to weak evidence. However, the same networks serve to maximize minority influence when minority group members are selectively exposed to strong evidence. These results suggest that fully-connected networks differentially regulate minority influence based on the kinds of evidence presented to both minority and majority group members.' as abstract.

there is a document named 'doc-2003' that
  has 'Protocols for secure multiparty computation (SMC) allow a set of mutually distrusting parties to compute a function f of their private inputs while revealing nothing about their inputs beyond what is implied by the result. Depending on f, however, the result itself may reveal more information than parties are comfortable with. Almost all previous work on SMC treats f as given. Left unanswered is the question of how parties should decide whether it is "safe" for them to compute f in the first place. We propose here a way to apply belief tracking to SMC in order to address exactly this question. In our approach, each participating party is able to reason about the increase in knowledge that other parties could gain as a result of computing f, and may choose not to participate (or participate only partially) so as to restrict that gain in knowledge. We develop two techniques-the belief set method and the SMC belief tracking method-prove them sound, and discuss their precision/performance tradeoffs using a series of experiments.' as abstract.

there is a document named 'doc-2007' that
  has 'Service composition in sensor networks combines elementary services with a specific functionality to create a service with higher level functionality. The previous efforts in automating composition were sending full information about all services across the entire sensor network, creating a security risk and imposing significant communication overhead. Furthermore, learning based composition or error detection methods do not consider global information, leading to inefficiencies in the generated composition graphs. In this paper, we propose a probabilistic context-free grammar (PCFG) based modeling technique to construct service compositions. The successful compositions created for the given application are treated as statements belonging to an efficient composition PCFG of this application. The given set of such compositions is used to derive this PCFG automatically. Future composition could be then easily constructed with the help of such PCFG. We present our methodology for achieving such modeling and provide examples of its use to demonstrate its advantage over previous work. We also evaluate the resulting improvements in performance of compositions and in the costs of their creation.' as abstract.

there is a document named 'doc-2018' that
  has 'Fault localization in general refers to a technique for identifying the likely root causes of failures observed in systems formed from components. Fault localization in systems deployed on mobile ad hoc networks (MANETs) is a particularly challenging task because those systems are subject to a wider variety and higher incidence of faults than those deployed in fixed networks, the resources available to track fault symptoms are severely limited, and many of the sources of faults in MANETs are by their nature transient. We present a method for localizing the faults occurring in service-based systems hosted on MANETs. The method is based on the use of dependence data that are discovered dynamically through decentralized observations of service interactions. We employ both Bayesian and timing-based reasoning techniques to analyze the data in the context of a specific fault propagation model, deriving a ranked list of candidate fault locations. We present the results of an extensive set of experiments exploring a wide range of operational conditions to evaluate the accuracy of our method.' as abstract.

there is a document named 'doc-2028c' that
  has 'Gennaro et al. (Crypto 2010) introduced the notion of verifiable computation, which allows a computationally weak client to outsource the computation of a function F on dynamically chosen inputs x1, . . . , x` to a more powerful but untrusted server. Following a pre-processing phase (that is only carried out once), the client can send some representation of its input xi to the server; the server returns an answer that allows the client to recover the correct result yi = F(xi), accompanied by a proof of correctness that prevents the server from convincing the client to accept an incorrect result. The crucial property of the scheme is that the work done by the client in preparing its input and verifying the server\'s proof is less than the time required for the client to compute F on its own. In this paper we extend their notion of verifiable computation to the multi-client setting, where N computationally weak clients wish to outsource to an untrusted server the computation of a function F over their joint inputs x1, . . . , xN without communicating with each other. We present a construction for (noninteractive) multi-client verifiable computation based on fully homomorphic encryption, Yao\'s garbled-circuit construction, and any identity-based encryption scheme.' as abstract.

there is a document named 'doc-2032' that
  has 'Sensor applications are typically composed of a number of functional components that run distributedly on the nodes of a sensor network, communicating and interacting with one another. Service composition is emerging as a viable approach towards the automatic synthesis of such sensor applications. However, for service composition to be practical, it has to comply with policies that define security and management constraints on the use of these service components and the interconnections amongst them. Prior research efforts have primarily focused on efficient evaluation of security policies during the composition process, which is not sufficient when generic network management constraints need to be expressed and evaluated. In this work, we propose a policy model and evaluation approach that enables us to define and check attribute-based policies, for controlling the sensor service composition process. Attributebased policies are generic and allows us to express a wider spectrum of constraints than currently possible. Using this model and based on a previously-proposed sensor service composition algorithm, we introduce a policy evaluation method that allows for efficient checking of policy constraints. We further present a novel implementation of the proposed approach in the IBM Sensor Fabric, a middleware framework that simplifies the development of distributed, sensor network services. We also present preliminary performance evaluation results using our prototype.' as abstract.

there is a document named 'doc-2040' that
  has 'Outsourced computations (where a client requests a server to perform some computation on its behalf) are becoming increasingly important due to the rise of Cloud Computing and the proliferation of mobile devices. Since cloud providers may not be trusted, a crucial problem is the verification of the integrity and correctness of such computation, possibly in a public way, i.e., the result of a computation can be verified by any third party, and requires no secret key - akin to a digital signature on a message. We present new protocols for publicly verifiable secure outsourcing of Evaluation of High Degree Polynomials and Matrix Multiplication. Compared to previously proposed solutions, ours improve in efficiency and offer security in a stronger model. The paper also discusses several practical applications of our protocols.' as abstract.

there is a document named 'doc-2042' that
  has 'We define and construct fully homomorphic message authenticators. In such a scheme, anybody can perform arbitrary computations over authenticated data and produce a short tag that authenticates the result of the computation. The user verifies this tag with her private key to ensure that the claimed result is indeed the correct output of the specified computation over her previously authenticated data, without needing to know the original data itself. For example, a user can outsource the storage of her authenticated data to a remote server, and later verify the outputs of various computations performed by the server over her data. Our construction uses fully homomorphic encryption in a novel way' as abstract.

there is a document named 'doc-2043' that
  has 'In modern coalition operations, decision makers must be capable of obtaining and fusing data from diverse sources. The reliability of these sources may be variable, and, in order to protect their interests, the data they provide may be obfuscated. The trustworthiness of fused data is dependent not only on the reliability of the sources, but also on the type and extent of the obfuscation used. New problems arise for both data providers and consumers in these contexts; the consumers must determine how to evaluate the trustworthiness of providers in the presence of differing levels of obfuscation, while the providers must be able to determine the appropriate level of obfuscation to ensure that trust in them is maintained. In this paper, we outline this rich problem area. We discuss trust and obfuscation in these contexts and the complex relationships between them.' as abstract.

there is a document named 'doc-2045' that
  has 'We introduce a new characterization of the NP complexity class, called Quadratic Span Programs (QSPs), which is a natural extension of span programs defined by Karchmer and Wigderson. Our main motivation is the construction of succinct arguments of NP-statements that are quick to construct and verify. QSPs seem well-suited for this task, perhaps even better than Probabilistically Checkable Proofs (PCPs). In 2010, Groth constructed a NIZK argument in the common reference string (CRS) model for Circuit-SAT consisting of only 42 elements in a bilinear group. Interestingly, his argument does not (explicitly) use PCPs. But his scheme has some disadvantages - namely, the CRS size and prover computation are both quadratic in the circuit size. In 2011, Lipmaa reduced the CRS size to quasi-linear, but with prover computation still quadratic. Using QSPs we construct a NIZK argument in the CRS model for Circuit-SAT consisting of just 7 group elements. The CRS size is linear in the circuit size, and prover computation is quasi-linear, making our scheme seemingly quite practical. (The prover only needs to do a linear number of group operations; the quasi-linear computation is a multipoint evaluation and interpolation.) Our results are complementary to those of Valiant (TCC 2008) and Bitansky et al. (2012), who use "bootstrapping" (recursive composition) of arguments to reduce CRS size and prover and verifier computation. QSPs also provide a crisp mathematical abstraction of some of the techniques underlying Groth\'s and Lipmaa\'s constructions.' as abstract.

there is a document named 'doc-2047' that
  has 'Video surveillance applications are examples of complex distributed coalition tasks. Real-time capture and analysis of image sensor data is one of the most important tasks in a number of military critical decision making scenarios. In complex battlefield situations, there is a need to coordinate the operation of distributed image sensors and the analysis of their data as transmitted over a heterogeneous wireless network where bandwidth, power, and computational capabilities are constrained. There is also a need to automate decision making based on the results of the analysis of video data. Declarative Networking is a promising technology for controlling complex video surveillance applications in this sort of environment. This paper presents a flexible and extensible architecture for deploying distributed video surveillance applications using the declarative networking paradigm, which allows us to dynamically connect and manage distributed image sensors and deploy various modules for the analysis of video data to satisfy a variety of video surveillance requirements. With declarative computing, it becomes possible for us not only to express the program control structure in a declarative fashion, but also to simplify the management of distributed video surveillance applications.' as abstract.

there is a document named 'doc-2055' that
  has 'Densely deployed fixed-infrastructure wireless networks such as cellular networks and WiFi hotspots have fundamentally changed the way people communicate with each other, since they allow people to be on the move while using their communication devices. On the contrary, infrastructure-less wireless networks such as mobile ad hoc networks (MANETs) have been popular in military, disaster relief and sensor network applications. Nodes in these networks suffer from a limited range of connectivity and wireless interference, thus often resulting in "islands of connectivity" and low throughput capacity, respectively. To alleviate such problems, hybrid network models have been proposed where nodes have the capability of bridging short-range MANET islands using long-range cellular links. Since cellular links are expensive, we impose a practical constraint that only a fraction (p) of nodes have such capabilities. We explore the impact of p and the density of MANET deployment, _, on the connectivity of the overall hybrid network.' as abstract.

there is a document named 'doc-2057e' that
  has 'We consider the multicast routing problem under operational communication constraints, such as in practical deployment scenarios, e.g., multi-domain ad hoc networks where two or more teams form a coalition, or in tactical networks where information flows need to adhere to specified policies regardless of the physical connectivity of nodes. First, we consider the problem of minimum-cost multicast routing on a multi-domain network by constructing a node-weighted Steiner tree (for mesh networks) and a Steiner connected dominating set (for wireless broadcast networks) that is subject to a non-additive cost constraint. This is because the multi-domain multicast cost is not just the sum of node costs of a Steiner tree, but it instead depends on the domains of the connected neighbors. We give an efficient algorithm that provides an O(logk) approximation guarantee, where k is the number of terminal (or sink) nodes in the network. Taking multi- domain cost constraints into account can help reduce the cost of a multicast tree by up to 40%. We also consider a constraint imposed due to hierarchy compliance. We show that the overall multicast can be decomposed into several smaller multicasts, each of which might be efficiently solvable. We find that necessary hierarchical constraints could cause a significant increase in the total cost of multicast - up to 25%, as per our simulations based on a realistic deployment scenario.' as abstract.

there is a document named 'doc-2118' that
  has 'Aggregation schemes allow to combine several cryptographic values like message authentication codes or signatures into a shorter value such that, despite compression, some notion of unforgeability is preserved. Recently, Eikemeier et al. (SCN 2010) considered the notion of history-free sequential aggregation for message authentication codes, where the sequentially-executed aggregation algorithm does not need to receive the previous messages in the sequence as input. Here we discuss the idea for signatures where the new aggregate does not rely on the previous messages and public keys either, thus inhibiting the costly verifications in each aggregation step as in previous schemes by Lysyanskaya et al. (Eurocrypt 2004) and Neven (Eurocrypt 2008). Analogously to MACs we argue about new security definitions for such schemes and compare them to previous notions for history-dependent schemes. We finally give a construction based on the BLS signature scheme which satisfies our notion.' as abstract.

there is a document named 'doc-2119' that
  has 'In a verifiable data streaming protocol, the client streams a long string to the server who stores it in its database. The stream is verifiable in the sense that the server can neither change the order of the elements nor manipulate them. The client may also retrieve data from the database and update them. The content of the database is publicly verifiable such that any party in possession of some value s and a proof _ can check that s is indeed in the database. We introduce the notion of verifiable data streaming and present an efficient instantiation that supports an exponential number of values based on general assumptions. Our main technique is an authentication tree in which the leaves are not fixed in advanced such that the user, knowing some trapdoor, can authenticate a new element on demand without pre- or re-computing all other leaves. We call this data structure chameleon authentication tree (CAT). We instantiate our scheme with primitives that are secure under the discrete logarithm assumption. The algebraic properties of this assumption allow us to obtain a very efficient verification algorithm. As a second application of CATs, we present a new transformation from any one-time to many-time signature scheme that is more efficient than previously known solutions.' as abstract.

there is a document named 'doc-2131' that
  has 'The development of future pervasive sensor-enabled systems, where information is distributed on-demand across heterogeneous networks, highlights the necessity for an efficient framework to determine the relevancy of provided information with respect to one\'s needs. This paper considers the problem of selecting the most "spatiotemporally" relevant providers in order to meet a user\'s information needs over a time period of interest. Initially, a definition and a measure of spatiotemporal relevancy is developed to measure the degree of relevancy of sensory information with respect to both its spatial and temporal characteristics. Based on these, the selection of the most relevant set of providers under budget constraints is expressed as an integer programming optimization problem and a two-level dynamic programming (DP) algorithm is proposed to solve it optimally. Moreover, a number of alternative methods are proposed in order to accelerate the provider selection process by making approximations either to the overall optimization problem formulation or the relevancy calculation method itself. Finally, the performance of the proposed methods are examined both analytically and by simulation for a number of provider scenarios.' as abstract.

there is a document named 'doc-2143a' that
  has 'Recently, there has been a tremendous increase in mobile data usage with the wide-spread proliferation of smartphone like devices. However, this increased demand from users has caused severe traffic overloading in cellular networks. Of- floading the traffic through several other devices (femtocells, WiFi access points) have been considered to be immediate remedy for such a problem. Thus, in this paper, we study the deployment of WiFi access points (AP) in a metropolitan area for efficient offloading of mobile data traffic. We analyze a large scale real user mobility traces and propose a deployment algorithm based on the density of user data request frequency. In simulations, we present offloading ratio that our algorithm can accomplish with different number of APs. The results demonstrate that our algorithm can achieve close to optimal offloading ratio that is higher than offloading ratios that existing algorithms can achieve with the same number of APs.' as abstract.

there is a document named 'doc-2177' that
  has 'Considering physical sensors with certain sensing capabilities in an Internet-of-Things (IoT) sensory environment, in this paper, we propose an efficient energy management framework to control the duty cycles of these sensors under quality-of-information (QoI) experience in a multi-task-oriented IoT sensory environment. Contrary to past research efforts, our proposal is transparent and compatible both with the underlying low-layer protocols and diverse applications, and preserving energy-efficiency in the long run without sacrificing the QoI levels attained. Specifically, we first introduce the novel concept of QoI-aware "sensor-to-task relevancy" to explicitly consider the sensing capabilities offered by an sensor to the IoT sensory environments, and QoI requirements required by a task. Second, we propose a novel concept of the "critical covering set" of any given task in selecting the sensors to service a task over time. Third, energy management decision is made dynamically at runtime, to reach the optimum for long-term application arrivals and departures under the constraint of their service delay. Finally, an extensive case study based on utilizing the sensing sensors to perform water quality monitoring is given to demonstrate the ideas and algorithms proposed in this paper, and a complete simulation is made to support all performance analysis.' as abstract.

there is a document named 'doc-2191' that
  has 'In this paper we describe the implementation of a declarative framework to support the development of distributed management applications. The framework is based on an extension of declarative networking, an asynchronous computational model that uses recursive SQL as its foundation and has been successfully used for the implementation of multiple networking protocols including opinion-based preferential routing as well as standard path vector and link state routing protocols. The SQL implementation enables analysis capabilities that can help avoid implementation and logic errors.' as abstract.

there is a document named 'doc-2221' that
  has 'In information driven multi-agent systems, information consumers collect information about their environment from various sources such as sensors. Each source has its own limitations, capabilities, and goals. Therefore, there is no guarantee that a source will provide the requested information truthfully and correctly. Even if information is provided only by trustworthy sources, it can contain conflicts that hamper its usability. In this paper, we propose to exploit such conflicts to revise trust in information. This requires a reasoning mechanism that can accommodate domain constraints, uncertainty, and trust. Our formalism - SDL-Lite - is an extension of a tractable subset of Description Logics with DempsterShafer theory of evidence. SDL-Lite allows reasoning about uncertain information and enables conflict detection. Following the introduction of SDL-Lite, we propose methods for conflict resolution through trust revision and analyse these methods under different settings through simulations. We show that the proposed methods allow reasonably accurate estimations of trust in information in realistic settings.' as abstract.

there is a document named 'doc-2223b' that
  has 'Pervasive information consumers in open, looselycoupled systems, such as in Internet of Things and crowdsensing environment, will rely more and more often on streaming information from sensory sources with whom they have only ephemeral, transient relationships. In such settings, information uncertainties arise as the trustworthiness of the sources and their information become questionable. It is thus necessary to quantify the quality of inferences made with such information to aid more informed and effective decision making and action taking. One of the aspects of trust assessment systems is to provide for such quality metrics, however, these systems have been traditionally applied in static situations. In this paper, we introduce TAF, a trust assessment framework for streaming information that leverages the rich toolkit of subjective logic operators to estimate the quality of said inferences under information uncertainty. We present the system architecture, describe its components and provide some preliminary quality results for the framework.' as abstract.

there is a document named 'doc-2224' that
  has 'This paper is concerned with the combination of argumentation with the Dempster-Shafer theory of evidence. We are motivated by the use of argumentation to reason about trust, and the desire to combine argumentation with an existing approach to propagating numerical measures of trust that is based on the Dempster-Shafer theory. We show how logical elements of evidence, associated with numerical degrees of belief, can be combined into arguments, and how these arguments can be related to the standard Dungian argumentation semantics.' as abstract.

there is a document named 'doc-2225' that
  has 'In domains such as emergency response and military operations the sharing of Intelligence, Surveillance and Reconnaissance (ISR) assets among different coalition partners is regulated through policies. Traditionally, policies are created at the center of a coalitions network by high-level decision makers and expressed in low-level policy languages (e.g. Common Information Model SPL) by technical personnel, which makes them difficult to be understood by non-technical users at the edge of the network. Moreover, policies must often be modified by negotiation among coalition partners, typically in rapid response to the changing operational situation. Commonly, the users who must cope first with situational changes are those on the edge, so it would be very effective if they were able to create and negotiate policies themselves. We investigate the use of Controlled English (CE) as a means to define a policy representation that is both human-friendly and machine processable. We show how a CE model can capture a variety of policy types, including those based on a traditional asset ownership model, and those defining team-based asset sharing across a coalition. The use of CE is intended to benefit coalition networks by bridging the gap between technical and non-technical users in terms of policy creation and negotiation, while at the same time being directly processable by a policy-checking system without transformation to any other technical representation.' as abstract.

there is a document named 'doc-2228a' that
  has 'There is considerable interest in natural language conversational interfaces. These allow for complex user in- teractions with systems, such as fulfilling information requirements in dynamic environments, without requiring extensive training or a technical background (e.g. in formal query languages or schemas). To leverage the advantages of conversational interactions we propose CE-SAM (Controlled English Sensor Assignment to Mis- sions), a system that guides users through refining and satisfying their information needs in the context of Intelligence, Surveillance, and Reconnaissance (ISR) operations. The rapidly-increasing availability of sensing assets and other information sources poses substantial challenges to effective ISR resource management. In a coalition context, the problem is even more complex, because assets may be "owned" by different partners. We show how CE-SAM allows a user to refine and relate their ISR information needs to pre-existing concepts in an ISR knowledge base, via conversational interaction implemented on a tablet device. The knowledge base is represented using Controlled English (CE) - a form of controlled natural language that is both human-readable and machine processable (i.e. can be used to implement automated reasoning). Users interact with the CE-SAM conversational interface using natural language, which the system converts to CE for feeding-back to the user for confirmation (e.g. to reduce misunderstanding). We show that this process not only allows users to access the assets that can support their mission needs, but also assists them in extending the CE knowledge base with new concepts.' as abstract.

there is a document named 'doc-2229' that
  has 'Recent developments in hardware have shown an increase in parallelism as opposed to clock rates. In order to fully exploit these new avenues of performance improvement, computationally expensive workloads have to be expressed in a way that allows for fine-grained parallelism. In this paper, we address the problem of describing RDFS entailment in such a way. Different from previous work on parallel RDFS reasoning, we assume a shared memory architecture. We analyze the problem of duplicates that naturally occur in RDFS reasoning and develop strategies towards its mitigation, exploiting all levels of our architecture. We implement and evaluate our approach on two real-world datasets and study its performance characteristics on different levels of parallelization. We conclude that RDFS entailment lends itself well to parallelization but can benefit even more from careful optimizations that take into account intricacies of modern parallel hardware.' as abstract.

there is a document named 'doc-2230' that
  has 'The socially-distributed nature of cognitive processing in a variety of organizational settings means that there is increasing scientific interest in the factors that affect collective cognition. In military coalitions, for example, there is a need to understand how factors such as communication network topology, trust, cultural differences and the potential for miscommunication affects the ability of distributed teams to generate high quality plans, to formulate effective decisions and to develop shared situation awareness. The current paper presents a computational model and associated simulation capability for performing in silico experimental analyses of collective sensemaking. This model can be used in combination with the results of human experimental studies in order to improve our understanding of the factors that influence collective sensemaking processes.' as abstract.

there is a document named 'doc-2232' that
  has 'In hybrid wireless networks, where the fixed cellular network infrastructure is utilized to provide enhanced network coverage and communication performance for nodes in mobile ad-hoc networks, the selection of the gateway for each node towards the external network needs to be based on accurate and timely network performance perceived by each mobile node. Continuously monitoring these performance metrics by each individual node, however, would incur prohibitively high communication and processing overhead. In this paper, we propose a distributed network probing mechanism, called CoPing, that utilizes the cooperation among the nodes in the measurement process of path performance in MANETs towards the gateways of HWNs. In our approach, each node makes use of the end-to-end performance probing results measured by other nodes to estimate its own performance to the gateways in a fully distributed manner. Furthermore, the distributed process does not require explicit structure and coordination between the nodes, making it ideal for highly dynamic networking environments. Through the combination of analysis and experimental evaluation, we show our cooperative probing mechanism can achieve accurate and efficient path performance results for gateway selection in HWNs.' as abstract.

there is a document named 'doc-2244' that
  has 'In a coalition context, data fusion involves combining of soft (e.g., field reports, intelligence reports) and hard (e.g., acoustic, imagery) sensory data such that the resulting output is better than what it would have been if the data are taken individually. However, due to the lack of explicit semantics attached with such data, it is difficult to automatically disseminate and put the right contextual data in the hands of the decision makers. In order to understand the data, explicit meaning needs to be added by means of categorizing and/or classifying the data in relationship to each other from base reference sources. In this paper, we present a semantic framework that provides automated mechanisms to expose real-time raw data effectively by presenting appropriate information needed for a given situation so that an informed decision could be made effectively. The system utilizes controlled natural language capabilities provided by the ITA (International Technology Alliance) Controlled English (CE) toolkit to provide a human-friendly semantic representation of messages so that the messages can be directly processed in human/machine hybrid environments. The Real-time Semantic Enrichment (RTSE) service adds relevant contextual information to raw data streams from domain knowledge bases using declarative rules. The rules define how the added semantics and context information are derived and stored in a semantic knowledge base. The software framework exposes contextual information from a variety of hard and soft data sources in a fast, reliable manner so that an informed decision can be made using semantic queries in intelligent software systems.' as abstract.

there is a document named 'doc-2247' that
  has 'We initiate the study of broadcast steganography (BS), an extension of steganography to the multi-recipient setting. BS enables a sender to communicate covertly with a dynamically designated set of receivers, so that the recipients recover the original content, while unauthorized users and outsiders remain unaware of the covert communication. One of our main technical contributions is the introduction of a new variant of anonymous broadcast encryption that we term outsider-anonymous broadcast encryption with pseudorandom ciphertexts (oABE$). Our oABE$ construction achieves sublinear ciphertext size and is secure in the standard model. Besides being of interest in its own right, oABE$ enables an efficient construction of BS secure in the standard model against adaptive adversaries that also features sublinear ciphertexts.' as abstract.

there is a document named 'doc-2251' that
  has 'Fully homomorphic encryption (FHE) is a form of public-key encryption that enables arbitrary computation over encrypted data. The past few years have seen several realizations of FHE under different assumptions, and FHE has been used as a building block in many cryptographic applications. Adaptive security for public-key encryption schemes is an important security notion that was proposed by Canetti et al. over 15 years ago. It is intended to ensure security when encryption is used within an interactive protocol, and parties may be adaptively corrupted by an adversary during the course of the protocol execution. Due to the extensive applications of FHE to protocol design, it is natural to understand whether adaptively secure FHE is achievable. In this paper we show two contrasting results in this direction. First, we show that adaptive security is impossible for FHE satisfying the (standard) compactness requirement. On the other hand, we show a construction of adaptively secure FHE that is not compact, but which does achieve circuit privacy.' as abstract.

there is a document named 'doc-2252' that
  has 'In this paper, we propose a system architecture for decision-making support on ISR (i.e. Intelligence, Surveillance, Reconnaissance) missions via optimizing resource allocation. We model a mission as a graph of tasks, each of which often requires exclusive access to some resources. Our system guides users through refinement of their needs through an interactive interface. To maximize the chances of executing new missions, the system searches for pre-existent information collected on the field that best fit the needs. If this search fails, a set of new requests representing users\' requirements is considered to maximize the overall benefit constrained by limited resources. If an ISR request cannot be satisfied, feedback is generated to help the commander further refine or adjust their information requests in order to still provide support to the mission. In our work we model both demands for resources and the importance of the information retrieved realistically in that they are not fully known at the time a mission is submitted and may change overtime during execution.  The amount of resources consumed by a mission may not be deterministic; i.e., a mission may last slightly longer or shorter than expected, or more of a resource may be required to complete a task.  Furthermore, the benefits received from the mission, which we call profits, may also be non-deterministic; e.g. successfully localizing a vehicle might be more important than expected for accomplishing the entire operation. Therefore, when satisfying ISR requirements we take into account both constraints on the underlying resources and uncertainty of demands and profits.' as abstract.

there is a document named 'doc-2255' that
  has 'Probabilistic computation is a convenient means of mechanically reasoning about a variety of information security problems. At its core, information security concerns itself with measuring or limiting the knowledge an adversary might attain from interacting with a protected system. Probabilistic inference lets one compute this knowledge explicitly as long as the interaction can be described as a program in a suitable probabilistic language that supports conditioning. Security concerns, however, require soundness guarantees on probabilistic inference which are generally not present in machine learning applications. We summarize some recent work on probabilistic computing for information security and highlight challenging aspects that still need to be addressed.' as abstract.

there is a document named 'doc-2275' that
  has 'Resources in a modern networked application include not only bandwidth, but also memory, storage space, CPU, etc. To serve multiple users simultaneously, the system often needs to select a feasible subset of users\' tasks, with the goal of maximizing the total profit. The selection choice is constrained by the limited capacity of resources, which may in general range over multiple types. Each task yields its profit only when its demand is satisfied in all resource dimensions. In many realistic applications, however, it may not be practical to simply assume that both demands and profits are deterministic and known a priori. On one hand, the demands could be stochastic following certain distributions; on the other hand, often the profits cannot be predicted precisely. In this paper, we model this problem setting as a variant of the stochastic knapsack problem, and study a specific scenario in which both demands and profits follow normal distributions. In our formulation, we provide two tunable parameters to configure two types of probabilities: one limits the capacity overflow rate with which the combined demands of admitted tasks may exceed the available supply, and the other allows to set the minimum chance at which we will obtain the expected payoff. Also, we define the relative values of random variables in given conditions, and use them to search for the best resource allocation solution when available information is not deterministic. We propose several heuristics with different optimality/efficiency tradeoffs, and find that our algorithms converge relatively fast and provide results considerably close to the optimum.' as abstract.

there is a document named 'doc-2279' that
  has 'Probabilistic argumentation frameworks (PrAFs) are a novel extension to standard argumentation systems, enabling one to reason about the likelihood of a set of arguments appearing within an extension. However, PrAFs assume that the likelihood of arguments appearing is independent of the presence of other arguments. In this paper, we lift this restriction through the introduction of probabilistic evidential argumentation frameworks (PrEAFs). Our extension captures probabilistic dependencies through the use of a support relation, as used in bipolar argumentation frameworks. After describing PrEAFs and their properties, we present algorithms for computing PrEAF semantics.' as abstract.

there is a document named 'doc-2281' that
  has 'In this paper we introduce the notion of Algebraic (Trapdoor) One Way Functions, which, roughly speaking, captures and formalizes many of the properties of number-theoretic one-way functions. Informally, a (trapdoor) one way function F : X _ Y is said to be algebraic if X and Y are (finite) abelian cyclic groups, the function is homomorphic i.e. F(x) _ F(y) = F(x _ y), and is ringhomomorphic, meaning that it is possible to compute linear operations "in the exponent" over some ring (which may be different from Zp where p is the order of the underlying group X) without knowing the bases. Moreover, algebraic OWFs must be flexibly one-way in the sense that given y = F(x), it must be infeasible to compute (x 0 , d) such that F(x 0 ) = y d (for d 6= 0). Interestingly, algebraic one way functions can be constructed from a variety of standard number theoretic assumptions, such as RSA, Factoring and CDH over bilinear groups. As a second contribution of this paper, we show several applications where algebraic (trapdoor) OWFs turn out to be useful. In particular: - Publicly Verifiable Secure Outsourcing of Polynomials: We present efficient solutions which work for rings of arbitrary size and characteristic. When instantiating our protocol with the RSA/Factoring based algebraic OWFs we obtain the first solution which supports small field size, is efficient and does not require bilinear maps to obtain public verifiability. - Linearly-Homomorphic Signatures: We give a direct construction of FDH-like linearly homomorphic signatures from algebraic (trapdoor) one way permutations. Our constructions support messages and homomorphic operations over arbitrary rings and in particular even small fields such as F2. While it was already known how to realize linearly homomorphic signatures over small fields (Boneh-Freeman, Eurocrypt 2011), from lattices in the random oracle model, ours are the first schemes achieving this in a very efficient way from Factoring/RSA. - Batch execution of Sigma protocols: We construct a simple and efficient Sigma protocol for any algebraic OWP and show a "batch" version of it, i.e. a protocol where many statements can be proven at a cost (slightly superior) of the cost of a single execution of the original protocol. Given our RSA/Factoring instantiations of algebraic OWP, this yields, to the best of our knowledge, the first batch verifiable Sigma protocol for groups of unknown order.' as abstract.

there is a document named 'doc-2283' that
  has 'We demonstrate the achievability of a square root limit on the amount of information transmitted reliably and with low probability of detection (LPD) over the single-mode lossy bosonic channel if either the eavesdropper\'s measurements or the channel itself is subject to the slightest amount of excess noise. Specifically, Alice can transmit O( _ n) bits to Bob over n channel uses such that Bob\'s average codeword error probability is upperbounded by an arbitrarily small _ > 0 while a passive eavesdropper, Warden Willie, who is assumed to be able to collect all the transmitted photons that do not reach Bob, has an average probability of detection error that is lower-bounded by 1 2 _ for an arbitrarily small  > 0. We analyze the thermal noise and pure loss channels. The square root law holds for the thermal noise channel even if Willie employs a quantum-optimal measurement, while Bob is equipped with a standard coherent detection receiver. We also show that LPD communication is not possible on the pure loss channel. However, this result assumes Willie to possess an ideal receiver that is not subject to excess noise. If Willie is restricted to a practical receiver with a non-zero dark current, the square root law is achievable on the pure loss channel.' as abstract.

there is a document named 'doc-2293b' that
  has 'Controlled English is a human-readable information representation format that is implemented using a restricted subset of the English language, but which is unambiguous and directly accessible by simple machine processes. We have been researching the capabilities of CE in a number of contexts, and exploring the degree to which a flexible and more human-friendly information representation format could aid the intelligence analyst in a multi-agent collaborative operational environment; especially in cases where the agents are a mixture of other human users and machine processes aimed at assisting the human users. CE itself is built upon a formal logic basis, but allows users to easily specify models for a domain of interest in a human-friendly language. In our research we have been developing an experimental component known as the "CE Store" in which CE information can be quickly and flexibly processed and shared between human and machine agents. The CE Store environment contains a number of specialized machine agents for common processing tasks and also supports execution of logical inference rules that can be defined in the same CE language. This paper outlines the basic architecture of this approach, discusses some of the example machine agents that have been developed, and provides some typical examples of the CE language and the way in which it has been used to support complex analytical tasks on synthetic data sources. We highlight the fusion of human and machine processing supported through the use of the CE language and CE Store environment, and show this environment with examples of highly dynamic extensions to the model(s) and integration between different user-defined models in a collaborative setting.' as abstract.

there is a document named 'doc-2297' that
  has 'We study the competing goals of utility and privacy as they arise when a user shares personal sensor data with apps on a smartphone. On the one hand, there can be value to the user for sharing data in the form of various personalized services and recommendations; on the other hand, there is the risk of revealing behaviors to the app producers that the user would like to keep private. The current approaches to privacy, usually defined in multi-user settings, rely on anonymization to prevent such sensitive behaviors from being traced back to the user-a strategy which does not apply if user identity is already known, as is the case here. Instead of protecting identity, we focus on the more general problem of choosing what data to share, in such a way that certain kinds of inferences-i.e., those indicating the user\'s sensitive behavior-cannot be drawn. The use of inference functions allows us to establish a terminology to unify prior notions of privacy as special cases of this more general problem. We identify several information disclosure regimes, each corresponding to a specific privacyutility tradeoff, as well as privacy mechanisms designed to realize these tradeoff points. Finally, we propose ipShield as a privacy-aware framework which uses current user context together with a model of user behavior to quantify an adversary\'s knowledge regarding a sensitive inference, and obfuscate data accordingly before sharing. We conclude by describing initial work towards realizing this framework.' as abstract.

there is a document named 'doc-2303' that
  has 'We present a dynamic price based routing protocol in which packets from different applications dynamically choose their paths by evaluating the price to be paid for taking each path and their ability to pay. We propose a mechanism in which the prices reflect congestion on routers and thus the waiting time for packet to pass through the router. These prices increase as usage of the usually preferred shorter routes increases. The packet\'s ability to pay price on a router is defined by the product of application\'s priority and the delay experienced at the router. As a result, the low priority applications intelligently avoid paths with high prices and go via low price routes. The low price routes may possibly be longer but require shorter waiting for passage at congested routers making them faster for low priority packets. This enables high priority traffic to get through quickly via shorter paths as they are able to pay high prices after little wait. Thus, our approach distributes traffic flows of different applications in the network and lowers congestion and delays for all applications. We further show that our dynamic path allocation technique ensures robust communication in fully functional as well as partially damaged networks. Our dynamic pricing mechanism quickly adapts routing to the damaged network, increases utilization of the partial network to lower the impact on critical infrastructure and key resources. Moreover, our proposed mechanism is equally applicable to both communication networks and physical infrastructure networks.' as abstract.

there is a document named 'doc-2304' that
  has 'A wide range of forwarding strategies have been developed for multi-hop wireless networks, considering the broadcast nature of the wireless medium and the presence of random fading that results in time-varying and unreliable transmission quality. Two recently proposed strategies are opportunistic forwarding, which exploits relay diversity by opportunistically selecting an overhearing relay as a forwarder, and cooperative forwarding, which relies on the synchronized transmissions of relays to reinforce received signal strengths. Although these strategies are well-known in the literature, there is no comprehensive comparative analysis of their network-level performance in a realistic SINR (signal-to-interference-and-noise ratio) setting with multiple network flows. In this paper, we develop Markovian models for these protocols in the case of multiple competing flows in a general network setting; we also provide recurrence relations for the special case of linear networks. We first use these models to evaluate simple small-scale networks, and find that opportunism often outperforms cooperation - a result corroborated by simulations in larger networks. We also present a fixed-point model to efficiently estimate the throughput of large networks using these models. We identify the interference resulting from the larger number of transmissions under cooperative forwarding as a cause for mitigating the potential gains achievable with cooperative forwarding.' as abstract.

there is a document named 'doc-2305' that
  has 'Trust and reputation are significant components in open dynamic systems for making informed and reliable decisions. State-of-the-art information fusion models that exploit these mechanisms generally rely on reports from as many sources as possible. Situations exist, however, where seeking evidence from all possible sources is unrealistic. First, querying information sources is costly especially in resource-constrained environments, in terms of time, bandwidth. Secondly, reports from multiple sources exposes one to the risk of double-counting evidence, introducing an extra challenge of distinguishing fact from rumour. This paper describes TIDY, a trust-based approach to information fusion that exploits diversity among information sources in order to select a small number of candidates to query for evidence, and to minimise the effect of correlated evidence and bias. We demonstrate that reliable decisions can be reached using evidence from small groups of individuals. We show empirically that our approach is robust in contexts of variable trust in information sources, and to a degree of deception.' as abstract.

there is a document named 'doc-2306' that
  has 'Alongside existing research into the social, political and economic impacts of the Web, there is also a need to explore the effects of the Web on our cognitive profile. This is particularly so as the range of interactive opportunities we have with the Web expands under the influence of a range of emerging technologies. Embodied, extended and distributed approaches to cognition are relevant to understanding the potential cognitive impact of these new technologies because of the emphasis they place on extraneural and extra-corporeal factors in the shaping of our cognitive capabilities at both an individual and collective level. The current paper outlines a number of areas where embodied, extended and distributed approaches to cognition are useful in understanding the impact of emerging Web technologies on future forms of both human and machine intelligence.' as abstract.

there is a document named 'doc-2309' that
  has 'Cloud computing allows users to delegate data and computation to cloud service providers, at the cost of giving up physical control of their computing infrastructure. An attacker (e.g., insider) with physical access to the computing platform can perform various physical attacks, including probing memory buses and cold-boot style attacks. Previous work on secure (co-)processors provides hardware support for memory encryption and prevents direct leakage of sensitive data over the memory bus. However, an adversary snooping on the bus can still infer sensitive information from the memory access traces. Existing work on Oblivious RAM (ORAM) provides a solution for users to put all data in an ORAM; and accesses to an ORAM are obfuscated such that no information leaks through memory access traces. This method, however, incurs significant memory access overhead. This work is the first to leverage programming language techniques to offer efficient memory-trace oblivious program execution, while providing formal security guarantees. We formally define the notion of memory-trace obliviousness, and provide a type system for verifying that a program satisfies this property. We also describe a compiler that transforms a program into a structurally similar one that satisfies memory trace obliviousness. To achieve optimal efficiency, our compiler partitions variables into several small ORAM banks rather than one large one, without risking security. We use several example programs to demonstrate the efficiency gains our compiler achieves in comparison with the naive method of placing all variables in the same ORAM.' as abstract.

there is a document named 'doc-2310' that
  has 'In secure multi-party computation, mutually distrusting parties cooperatively compute functions of their private data; in the process, they only learn certain results as per the protocol (e.g., the final output). The realization of these protocols uses cryptographic techniques to avoid leaking information between the parties. A protocol for a secure computation can sometimes be optimized without changing its security guarantee: when the parties can use their private data and the revealed output to infer the values of other data, then this other data need not be concealed from them via cryptography. In the context of automatically optimizing secure multiparty computation, we define two related problems, knowledge inference and constructive knowledge inference. In both problems, we attempt to automatically discover when and if intermediate variables used in a protocol will (eventually) be known to the parties involved in the computation. Provably-known variables offer optimization opportunities. We formally state the problem of knowledge inference (and its constructive variant); we describe our solutions, which are built atop existing, standard technology such as SMT solvers. We show that our approach is sound, and further, we characterize the completeness properties enjoyed by our approach. We have implemented our approach, and present a preliminary experimental evaluation.' as abstract.

there is a document named 'doc-2312' that
  has 'Decision makers (humans or software agents alike) are increasingly faced with the challenge of examining large volumes of information originating from heterogeneous sources requiring them to ascertain trust in various pieces of information. While several authors have explored various trust computation models on static data and certain rules, past work has typically assumed: (i) a statistically significant number of ratings are available prior to trust assessment, and (ii) assessed trust values tend to vary slowly over time. In contrast, military settings warrant: (i) trust assessment over partial, uncertain and streaming (live and real-time) information from heterogeneous sources, (ii) coping up with the dynamic and evolving nature of the ground truth, and (iii) and more importantly, rules used for making inferences may by themselves be uncertain. Within the context of executing the OODA loop for decision making our research objective is to develop a family of trust operators for dynamic information flows for assessing trust over data-in-motion rather than a large corpus of static data. In this paper, we show how to exploit the computational toolset of subjective logic to build a framework for trust assessment in this case. Furthermore, we describe an implementation of the framework (using Information Fabric [6] and Controlled English Fact Store[5]) and present an experimental evaluation that quantifies the efficacy with respect to accuracy and overhead of the proposed framework.' as abstract.

there is a document named 'doc-2317' that
  has 'We investigate the gaps for Soldiers in information collection and resource management for Intelligence, Surveillance, and Reconnaissance (ISR). ISR comprises the intelligence functions supporting military operations; we concentrate on ISR for physical sensors (air and ground platforms). To identify gaps, we use approaches from Human Factors (interactions between humans and technical systems to optimize human and system performance) at the level of Soldier functions/activities in ISR. Key gaps (e.g., the loud auditory signatures of some air assets, unofficial ISR requests, and unintended battlefield effects) are identified. These gaps illustrate that ISR is not purely a technical problem. Instead, interactions between technical systems, humans, and the environment result in unpredictability and adaptability in using technical systems. To mitigate these gaps, we provide technology recommendations.' as abstract.

there is a document named 'doc-2320' that
  has 'Large scale adoption of MapReduce computations on public clouds is hindered by the lack of trust on the participating virtual machines, because misbehaving worker nodes can compromise the integrity of the computation result. In this paper, we propose a novel MapReduce framework, Cross Cloud MapReduce (CCMR), which overlays the MapReduce computation on top of a hybrid cloud: the master that is in control of the entire computation and guarantees result integrity runs on a private and trusted cloud, while normal workers run on a public cloud. In order to achieve high accuracy, CCMR proposes a result integrity check scheme on both the map phase and the reduce phase, which combines random task replication, random task verification, and credit accumulation; and CCMR strives to reduce the overhead by reducing cross-cloud communication. We implement our approach based on Apache Hadoop MapReduce and evaluate our implementation on Amazon EC2. Both theoretical and experimental analysis show that our approach can guarantee high result integrity in a normal cloud environment while incurring nonnegligible performance overhead (e.g., when 16.7% workers are malicious, CCMR can guarantee at least 99.52% of accuracy with 33.6% of overhead when replication probability is 0.3 and the credit threshold is 50).' as abstract.

there is a document named 'doc-2323' that
  has 'The Management of Information Processing Services (MIPS) project has two main objectives; the notification to analysts of the arrival of relevant new information and the automatic processing of the new information. Within these objectives a number of significant challenges were addressed. To achieve the first objective, the team had to demonstrate the capability for specific analysts to be "tipped-off" in real-time that textual reports and sensor-data have been received that are relevant to their analytical tasks, including the possibility that such reports have been made available by other nations. In the case of the second objective, the team had to demonstrate the capability for the infrastructure to automatically initiate processing of input data as it arrives, consistent with satisfying the analytical goals of teams of analysts, in as an efficient a manner as possible (including the case where data is made available by more than one nation). Using the Information Fabric middleware developed as part of the International Technology Alliance (ITA) research program, the team created a service based information processing infrastructure to achieve the objectives and challenges set by the customer. The infrastructure allows existing software to be wrapped as a service and/or specially written services to be integrated with each other as well as with other ITA technologies such as the Controlled English (CE) Store or the Gaian Database. This paper will identify the difficulties in designing and implementing the MIPS infrastructure together with describing its architecture and illustrating its use with a worked example use case.' as abstract.

there is a document named 'doc-2328' that
  has 'In this paper, we consider a problem related to service management and deployment in tactical military networks. Tactical networks are typically hybrid wireless networks in which there are both static and mobile nodes with several wireless interfaces, such as 802.11, 3G, satellite, etc. In tactical networks, performance degradation in services could prove fatal, so it must be diagnosed quickly. This degradation could be due to mobility or bottlenecks in capacity at network layer. We provide a cross-layer framework to detect and diagnose these causes of performance degradation as part of service management; it includes a monitoring model of services and a network model for hybrid wireless networks. In addition, we give a working example in tactical military networks to illustrate our framework. We provide an experimental setup to simulate our hybrid wireless tactical network scenario along with preliminary results.' as abstract.

there is a document named 'doc-2334' that
  has 'Computational trust mechanisms aim to produce a trust rating from both direct and indirect information about agents behaviour. Josang\'s Subjective Logic has been widely adopted as the core of such systems via its fusion and discount operators. Recently we proposed an operator for discounting opinions based on geometrical properties, and, continuing this line of investigation, this paper describes a new geometry based fusion operator. We evaluate this fusion operator together with our geometric discount operator in the context of a trust system, and show that our operators outperform those originally described by Josang. A core advantage of our work is that these operators can be used without modifying the remainder of the trust and reputation system.' as abstract.

there is a document named 'doc-2335' that
  has 'In this paper we describe a decision process framework allowing an agent to decide what information it should reveal to its neighbours within a communication graph in order to maximise its utility. We assume that these neighbours can pass information onto others within the graph, and that the communicating agent gains and loses utility based on the information which can be inferred by specific agents following the original communicative act. To this end, we construct an initial model of information propagation and describe an optimal decision procedure for the agent.' as abstract.

there is a document named 'doc-2343' that
  has 'Dempster-Shafer theory, which can be regarded as a generalisation of probability theory, is a widely used formalism for reasoning with uncertain information. The application of the theory hinges on the use of a rule for combining evidence from different sources. A number of different combination rules have been applied, each of which makes certain assumptions about the evidence. Here we describe several of these rules as argument schemes, using critical questions to capture the assumptions behind the rules. An example illustrates how we imagine these argument schemes might be used.' as abstract.

there is a document named 'doc-2344' that
  has 'Trust mechanisms allow a trustor to identify the most trustworthy trustee with which to interact. Such interactions can take the form of task assignments, in which case the trustor can be seen to delegate the task to the trustee. Now this process of delegation can operate recursively, with a task being repeatedly sub-delegated until some agent acts upon it. Such delegation chains present a problem for current trust evaluation mechanisms, which reward or penalise a single agent, as responsibility for the outcome of a task must be shared among all members of the delegation chain. As a result, current approaches lead to agents being unfairly penalised (or rewarded) during trust update, adversely affecting the quality of the model\'s evaluations, and through this, future agent interactions. In this paper we investigate the effects of sub-delegation on a probabilistic trust model and propose a model of weighting trust updates based on shared responsibility. We evaluate this model in the context of a simulated multi-agent system and describe how different weighting strategies can affect probabilistic trust updates when sub-delegation is possible.' as abstract.

there is a document named 'doc-2345' that
  has 'This work develops alternatives to the classical sub- jective logic deduction operator. Given antecedent and consequent propositions, the new operators form opinions of the consequent that match the variance of the consequent posterior distribution given opinions on the antecedent and the conditional rules connecting the antecedent with the consequent. As a result, the uncertainty of the consequent actually represent the confidence interval for the probability projection of the opinion. Monte Carlo simulations demonstrate this connection for the new operators. Finally, the work uses Monte Carlo simulations to evaluate the quality of fusing opinions from multiple agents before and after deduction.' as abstract.

there is a document named 'doc-2348' that
  has 'A limitation of standard Description Logics is its inability to reason with uncertain and vague knowledge. Although probabilistic and fuzzy extensions of DLs exist, which provide an explicit representation of uncertainty, they do not provide an explicit means for reasoning about second order uncertainty. Dempster-Shafer theory of evidence (DST) overcomes this weakness and provides means to fuse and reason about uncertain information. In this paper, we combine DL-Lite with DST to allow scalable reasoning over uncertain semantic knowledge bases. Furthermore, our formalism allows for the detection of conflicts between the fused information and domain constraints. Finally, we propose methods to resolve such conflicts through trust revision by exploiting evidence regarding the information sources. The effectiveness of the proposed approaches is shown through simulations under various settings.' as abstract.

there is a document named 'doc-2349' that
  has 'In this work, we consider a tactical hybrid network where a military adhoc network utilizes a commercial cellular network as backbone for data traffic. It is assumed that all traffic goes through a cache at the cellular base station with Least-Recently Used (LRU) policy. We study the possibility of inferring the military traffic rate from cache hit/miss observations of a single adversary node. We show the possibility of such an inference by presenting algorithms for estimating the parameters of the hybrid network. First, we show that an adversary node can estimate the cache and workload parameters of a hybrid network by observing the hit/miss results of her queries. We then show that these estimates can be used to infer the military traffic rate.' as abstract.

there is a document named 'doc-2355' that
  has 'We present our on-going research on constructing and extending a version of Controlled English (CE) in support of knowledge sharing and decision-making for effective and efficient operations in the military coalition environment. This work would be useful for any multinational English speaking environment. This CE is intended for both human use and machine processing, providing: (i) A user-friendly language in a form of English enabling the user to use it in a fairly intuitive way. (ii) A precise language that enables clear, unambiguous representation of information that is amenable to rule-based interpretation and inferencing. The paper focuses on the discussion of methods for CE construction while optimizing a balance between the naturalness for humans and machine readability of the CE language in light of theoretical considerations and empirical experimentations. We discuss certain aspects of CE syntax, semantics and the lexical model as examples. We also show sample CE-based knowledge-sharing capabilities.' as abstract.

there is a document named 'doc-2359' that
  has 'We formalize the notion of Verifiable Oblivious Storage (VOS), in which a client outsources the storage of data to a server while ensuring privacy of the data and verifiability and obliviousness of access to that data. VOS generalizes the notion of Oblivious RAM (ORAM) in that it allows the server to perform computation, and also explicitly considers the issue of data integrity and freshness. We show that allowing server-side computation allows us to circumvent the known lower bound on the bandwidth required for ORAM constructions. Specifically, for large block sizes we can construct a VOS scheme with constant bandwidth per query; furthermore, answering queries requires only poly-logarithmic server-side computation. We also show how to apply our VOS construction to achieve a dynamic proof-of-retrievability scheme that is asymptotically more bandwidth-efficient than existing state-of-the-art.' as abstract.

there is a document named 'doc-2360' that
  has 'A long-standing open problem in cryptography is proving the existence of (deterministic) hardcore predicates for the Diffie-Hellman problem defined over finite fields. In this paper we make progress on this problem by defining a very natural variation of the Diffie-Hellman problem over Fp2 and proving the unpredictability of every single bit of one of the coordinates of the secret DH value. To achieve our result we modify an idea presented at CRYPTO\'01 by Boneh and Shparlinski [4] originally developed to prove that the LSB of the Elliptic Curve Diffie-Hellman problem is hard. We extend this idea in two novel ways: 1. We generalize it to the case of finite fields Fp2 ; 2. We prove that any bit, not just the LSB, is hard using the list decoding techniques of Akavia et al. [1] (FOCS\'03) as generalized at CRYPTO\'12 by Duc and Jetchev [6]. In the process we prove several other interesting results: _ Our result hold also for a larger class of predicates, called segment predicates in [1]; _ We extend the result of Boneh and Shparlinski to prove that every bit (and every segment predicate) of the Elliptic Curve Diffie-Hellman problem is hard-core; _ We define the notion of partial one-way function over finite fields Fp2 and prove that every bit (and every segment predicate) of one of the input coordinate for these functions is hard-core.' as abstract.

there is a document named 'doc-2362' that
  has 'Federated coalition networks are formed by interconnected nodes belonging to different friendly-but-curious parties cooperating for common objectives and each party has its policy regarding what information may be accessed by which other parties. Data delivery in coalition networks must provide both data confidentiality and robustness. First, a source and destination pair are usually connected by intermediate nodes belonging to other parties that may not be allowed to see the content of transported data. Second, in coalition networks, data delivery has to be robust against dynamic topology changes caused by frequent node churn. Conventionally, data confidentiality is achieved by end-to-end encryption. In this paper, we explore a lightweight alternative approach using linear network coding where the original data are transformed into m coded packets and sent over multiple different paths. As long as only the destination, but not any party owning intermediate nodes, receives k (k _ m) or more packets, it can reconstruct the original data. We formulate the problem of confidentiality preserving and optimal-cost path selection for the coded packets into an integer programming problem. To solve this problem, we first relax it into a linear programming problem, and use column generation framework to address the huge number of variables. Based on the proposed algorithms, we develop a Robust Confidentiality Preserving (RCP) data delivery protocol. Our evaluation demonstrates that the proposed method can find the optimum solution in several seconds for networks of a few thousands nodes, and deliver data at a high success rate.' as abstract.

there is a document named 'doc-2364' that
  has 'We study the competing goals of utility and privacy as they arise when a provider delegates the processing of its personal information to a recipient who is better able to handle this data. We formulate our goals in terms of the inferences which can be drawn using the shared data. A whitelist describes the inferences that are desirable, i.e., providing utility. A blacklist describes the unwanted inferences which the provider wants to keep private. We formally define utility and privacy parameters using elementary information-theoretic notions and derive a bound on the region spanned by these parameters. We provide constructive schemes for achieving certain boundary points of this region. Finally, we improve the region by sharing data over aggregated time slots.' as abstract.

there is a document named 'doc-2368' that
  has 'Military networks have a huge range of performance requirements, many of which are continually changing in the face of evolving operations. In such circumstances, ensuring that network performance is adequate, let alone optimal, is a significant and ever more challenging management burden. The panacea to this problem is autonomous self-optimizing networks that continually adapt themselves as situations change. This work looks at some of the benefits and challenges of attempting to design such networks to address this uniquely military problem. Specifically we investigated simplified analytical problems on link state routing protocols, as a first step to gain insight that is more widely applicable. A crucial component of such protocols is the dissemination process by which links periodically broadcast their current state across the network. While this allows nodes to correctly compute routes, it also incurs significant control overhead that diminishes the effective capacity of the network. In this work, we study how nodes can make the best routing decisions as a fundamental tradeoff between link state dissemination cost and the accuracy of route computation. Using a Markovian model for link dynamics and a parameterized model for the state dissemination process, we investigate the impact of selectively sending link state updates on the packet delivery ratio performance in a lightly loaded dynamic network. Our analysis reveals the optimal state dissemination strategy that maximizes the packet delivery ratio given a total budget for the network-wide link state update rate, under sparse information flows. We instantiate our results explicitly for a simple path disjoint topology.' as abstract.

there is a document named 'doc-2371' that
  has 'With the popularity of mobile devices and the pervasive use of cellular technology, people can now access the Internet ubiquitously. As most smart phones and mobile devices are equipped with dual interfaces (WiFi and 3G/4G), they provide a natural platform to connect to the Internet using multipath TCP, which leverages path diversity to improve performance and provide robust data transfer. However, little has been explored about how people can benefit from using multi-path TCP under different traffic types, such as Web browsing or online video streaming. Furthermore, little has been investigated on the impact multi-path TCP may have at the application level due to delay and latency variation. In this paper, we take some initial steps to understand how Multi-path TCP performs in the wild, and focus on simple 2- path multi-path TCP scenarios (as most mobile devices have dual interfaces). We seek to answer the following questions: How much can a user benefit from using multi-path TCP when an additional cellular network interface is available, relative to using the WiFi interface alone? What are the performance impacts when the associated multi-path TCP flows are of different sizes? We are especially interested in understanding how the application level performance is affected when path characteristics (e.g., round trip times and loss rates) are diverse. We address these questions by conducting measurements using one commercial Internet service provider and three major cellular carriers in the US.' as abstract.

there is a document named 'doc-2372' that
  has 'Multicast group communications are essential for armed forces to effectively coordinate and execute missions in live theater, and given all the dynamics that a network can be subjected to in mission-critical situations (i.e., any combination of fast fading, slow fading, frequency jamming, network congestion, mobility), ensuring an appropriate level of network fidelity for supporting reliable communications becomes a difficult wireless networking problem. We argue for designing tunable reliability more explicitly into the act of topology construction based on the severity of forecasted dynamics in the network. This paper studies the number of nodes in the network supporting multicast traffic for a group of nodes; with more nodes supporting multicast traffic, the number of available redundant paths increases, enhancing reliability. We investigate the fundamental tradeoff of the amount of nodes supporting multicast communications and the gains in the number of node-disjoint paths among nodes in the multicast group. Traditionally, wired and wireless approaches to multicast focus on constructing some sort of tree- based topology with the benefit of having the minimum amount of resources allocated and a simplified routing protocol without redundant paths. Others have argued for ring-based topologies, augmented trees or rings allowing for alternate paths, and others have explored the mesh-based routing problem for multicast. We formulate a set of mixed integer linear program (MILP) for determining the nodes participating in the multicast group; we denote the set of nodes supporting multicast traffic as the multicast cloud. We investigate the fundamental tradeoffs of the fidelity of the cloud over a wide-range of parameters, accounting for various types of multicast topology allocation strategies (e.g. tree- and meshed-based approaches). We find that many nodes are needed to construct a single path among all nodes in a multicast group, but the cost for an additional node-disjoint path requires a fraction of additional nodes. We characterize this sublinear cost growth in terms of the number of nodes needed to provide connectivity to the cloud, and we characterize how the diameter of the cloud (i.e., hop across) decreases as more nodes participate in group communications.' as abstract.

there is a document named 'doc-2377' that
  has 'Mission-oriented sensor networks present challenging problems in terms of human-machine collaboration. Human users need to task the network to help them achieve mission objectives, while humans (sometimes the same individuals) are also sources of mission-critical information. We propose a natural language-based conversational approach to supporting human-machine working in mission-oriented sensor networks. We present a model for human-machine and machine-machine interactions in a realistic mission context, and evaluate the model using an existing surveillance mission scenario. The model supports the flow of conversations from full natural language to a form of Controlled Natural Language (CNL) amenable to machine processing and automated reasoning, including high-level information fusion tasks. We introduce a mechanism for presenting the gist of verbose CNL expressions in a more convenient form for human users. We show how the conversational interactions supported by the model include requests for expansions and explanations of machine-processed information.' as abstract.

there is a document named 'doc-2478' that
  has 'Intelligence analysis is the process of interpreting scattered information to form hypotheses and testing those against evidence. Collaboration enhances this process reducing effort and bias of an individual, while permitting more criticism and different perspectives to be considered. Existing analytical tools support an analyst in collecting information and evaluating hypotheses. However, for effective collaboration, analysts must work together in forming hypotheses from information. We propose an evidential reasoning service that aims at improving collaborative sense-making. It exploits argumentation schemes for structuring annotation and analysis of information, and records provenance to track data and reviews. This service aims to support the analysis by introducing automated reasoning about competing evidence for identifying plausible hypotheses. It provides a uniform reasoning structure that permits integration and facilitates sharing of analyses from different contributors.' as abstract.

there is a document named 'doc-2481' that
  has 'We investigate the problem of identifying individual link performance metrics in a communication network by measuring end-to-end metrics of selected paths between monitors, under the assumption that link metrics are additive and constant during the measurement, and measurement paths cannot contain cycles. In a previous work, we developed an algorithm that places the minimum number of monitors to identify all link metrics. However, even the minimum number can be large in some practical networks (e.g., 60% of all the nodes), suggesting high monitor deployment cost. In this paper, we study the dual problem where given a fixed number of monitors, we want to place them to maximize the number of identifiable link metrics, with concrete results for the case of two monitors. The significance of the two-monitor case is that all the tomographic computation can be performed at the destination monitor without shipping measurements to a central node, thus enabling endhost-based network monitoring. We develop an efficient algorithm to determine all identifiable links in an arbitrary network with a given placement of two monitors, based on which we propose an optimal two-monitor placement algorithm to maximize the number of identifiable links. Our evaluation on real ISP topologies shows that although a large number of monitors is needed to identify all link metrics, we can usually identify a substantial portion (up to 97%) of the links using a single pair of optimally placed monitors.' as abstract.

there is a document named 'doc-2482' that
  has 'We investigate the problem of identifying individual link metrics in a communication network from accumulated end-to-end metrics over selected measurement paths, under the assumption that link metrics are additive and constant during the measurement, and measurement paths cannot contain cycles. We know from linear algebra that all link metrics can be uniquely identified when the number of linearly independent measurement paths equals n, the number of links. It is, however, inefficient to collect measurements from all possible paths, whose number can grow exponentially in n, as the number of useful measurements (from linearly independent paths) is at most n. The aim of this paper is to develop efficient algorithms for constructing linearly independent measurement paths and calculating link metrics. We show that whenever there exists a set of n linearly independent measurement paths, there must exist a set of three pairwise independent spanning trees. We exploit this property to develop an algorithm that can construct n linearly independent, cycle-free paths between monitors without examining all candidate paths, whose complexity is quadratic in n. A further benefit of the proposed algorithm is that the generated paths satisfy a nested structure that allows linear-time computation of link metrics without explicitly inverting the measurement matrix. Our evaluations on both synthetic and real network topologies verify the superior efficiency of the proposed algorithms, which are orders of magnitude faster than benchmark solutions for large networks.' as abstract.

there is a document named 'doc-2483' that
  has 'We investigate the problem of identifying individual link metrics in a communication network from end-to-end path measurements, under the assumption that link metrics are additive and constant. To uniquely identify the link metrics, the number of linearly independent measurement paths must equal the number of links. Our contribution is to characterize this condition in terms of the network topology and the number/placement of monitors, under the constraint that measurement paths must be cycle-free. Our main results are: (i) it is generally impossible to identify all the link metrics by using two monitors; (ii) nevertheless, metrics of all the interior links not incident to any monitor are identifiable by two monitors if the topology satisfies a set of necessary and sufficient connectivity conditions; (iii) these conditions naturally extend to a necessary and sufficient condition for identifying all the link metrics using three or more monitors. We show that these conditions not only allow efficient identifiability tests, but also enable an efficient algorithm to place the minimum number of monitors in order to identify all link metrics. Our evaluations on both random and real topologies show that the proposed algorithm achieves identifiability using a much smaller number of monitors than a baseline solution.' as abstract.

there is a document named 'doc-2484' that
  has 'We investigate the problem of placing a given number of monitors in a communication network to identify the maximum number of link metrics from end-to-end measurements between monitors, assuming that link metrics are additive, and measurement paths cannot contain cycles. Motivated by our previous result that complete identification of all link metrics can require a large number of monitors, we focus on partial identification using a limited number of monitors. The basis to our solution is an efficient algorithm for determining all identifiable links for a given monitor placement. Based on this algorithm, we develop a polynomial-time algorithm to incrementally place monitors such that each newly placed monitor maximizes the number of additional identifiable links. We show that the proposed algorithm is optimal for 2-vertex-connected networks, and near-optimal for real ISP topologies that are not 2-vertex-connected. Our solution provides a quantifiable tradeoff between level of identifiability and available monitor resources.' as abstract.

there is a document named 'doc-2486' that
  has 'MPTCP is a new transport protocol that enables mobile devices to use several physical paths simultaneously through multiple network interfaces, such as WiFi and cellular. However, wireless path characteristics change frequently in mobile environments, causing difficulties for MPTCP. For example, WiFi associated paths often become unavailable as devices move, since WiFi has intermittent connectivity caused by its short signal range and susceptibility to interference. In this work, we improve MPTCP to manage path usage based on the associated link status. This variant, called MPTCP-MA, uses MAC-Layer information to locally estimate path quality and connectivity. By suspending/releasing paths based on their quality, MPTCP-MA can more effectively utilize restored paths. We have implemented and deployed MPTCP-MA in Linux and Android. Our experimental results show that MPTCP-MA can more efficiently utilize an intermittently available path than MPTCP, with Wifi throughput improvements of up to 72 percent.' as abstract.

there is a document named 'doc-2488' that
  has 'In recent years, the number of information sources available to support decision-making has increased dramatically. However, more information sources do not always mean higher precision in the fused information. This is partially due to the fact that some of these sources may be erroneous or malicious. Therefore, it is critical to asses the trust in information before performing fusion. To estimate trust in information, existing approaches use trustworthiness of its source as a proxy. We argue that conflicts between information may also serve as evidence to reduce trust in information. In this paper, we use subjective opinions to represent information from diverse sources. We propose to exploit conflicts between opinions to revise their trustworthiness. For this purpose, we formalise trust revision as a constraint optimisation problem. Through extensive empirical studies, we show that our approach significantly outperform existing ones in the face of malicious information sources.' as abstract.

there is a document named 'doc-2491' that
  has 'Multipath TCP (MPTCP) allows the concurrent use of multiple paths between two end points, and as such holds great promise for significantly improving application performance. However, in this paper, we report a newly discovered class of attacks on MPTCP that may jeopardize and hamper its wide-scale adoption. The attacks stem from the interdependence between the multiple subflows in an MPTCP connection. MPTCP congestion control algorithms are designed to achieve resource pooling and fairness with single-path TCP users at shared bottlenecks. Therefore, multiple MPTCP subflows are inherently coupled with each other, resulting in potential side-channels that can be exploited to infer cross-path properties. In particular, an ISP monitoring one or more paths used by an MPTCP connection can infer sensitive and proprietary information (e.g., level of network congestion, end-to-end TCP throughput, packet loss, network delay) about its competitors. Since the side-channel information enabled by the coupling among the subflows in an MPTCP connection results directly from the design goals of MPTCP congestion control algorithms, it is not obvious how to circumvent this attack easily. We believe our findings provide insights that can be used to guide future security-related research on MPTCP and other similar multipath extensions.' as abstract.

there is a document named 'doc-2506' that
  has 'Coalition operations involve multi-team and/or multi-nation collaborations. Linguistic variations and cultural differences often create unexpected challenges for effective communication and thus for Command and Control (C2) during military operations. In this paper, we propose using a controlled natural language, namely International Technology Alliance Controlled English (CE), and CE-based tools to improve crosslinguistic/cross-cultural communication. We will discuss various types of linguistic variations and cultural differences manifested by US and UK groups during coalition operations. The differences include not only lexical differences but more importantly differences in language use. These differences often result in miscommunication and impede effective operations. CE (Mott 2010) is a subset of English with a restricted grammar based on a formal syntax and semantics, which is human friendly but allows machine processing. The current version of CE provides a common form of expression that: _ promotes standard terminology and usage to reduce ambiguity in person to person communication _ allows end-users to create new concepts with associated syntax and semantics _ provides a basis for automated and assistive applications and tools that support natural human-computer interaction, reasoning, and explanation CE and CE-based tools can play an important role in facilitating cross-linguistic and cross-culture communication and enabling multi-nation teams to work together effectively and efficiently.' as abstract.

there is a document named 'doc-2526' that
  has 'The IEEE 802.11 protocols are used by millions of smartphone and tablet devices to access the Internet via Wi-Fi wireless networks or communicate with one another directly in a peer-to-peer mode. Insider attacks are those originating from a trusted node that had initially passed all the authentication steps to access the network and then got compromised. A trusted node that has turned rogue can easily perform Denial-of-Service (DoS) attacks on the Media Access Control (MAC) layer by illegally capturing the channel and preventing other legitimate nodes from communicating with one another. Insider attackers can alter the implementation of the IEEE 802.11 Distributed Coordination Function (DCF) protocol residing in the Network Interface Card (NIC) to illegally increase the probability of successful packet transmissions into the channel at the expenses of nodes that follow the protocol standards. The attacker fools the NIC to upgrade its firmware and forces in a version containing the malicious code. In this paper, we present a distributed solution to detect and isolate the attacker in order to minimize the impact of the DoS attacks on the network. Our detection algorithm enhances the DCF firmware to enable honest nodes to monitor each other\'s traffic and compare their observations against honest communication patterns derived from a two-dimensional Markov chain. A channel hopping scheme is then used on the physical layer (PHY) to evade the attacker. To facilitate communication among the honest member stations and minimize network downtime, we introduce two isolation algorithms, one based on identity-based encryption and another based on broadcast encryption. Our simulation results show that the latter enjoys quicker recovery time and faster network convergence.' as abstract.

there is a document named 'doc-2535' that
  has 'Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose InterityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytic and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon EC2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce).' as abstract.

there is a document named 'doc-2563' that
  has 'Cognitive social simulation is a computer simulation technique that aims to improve our understanding of the dynamics of socially-situated and socially-distributed cognition. This makes cognitive social simulation techniques particularly appealing as a means to undertake experiments into team cognition. The current paper reports on the results of an ongoing effort to develop a cognitive social simulation capability that can be used to undertake studies into team cognition using the ACT-R cognitive architecture. This capability is intended to support simulation experiments using a team-based problem solving task, which has been used to explore the effect of different organizational environments on collective problem solving performance. The functionality of the ACT-R-based cognitive social simulation capability is presented and a number of areas of future development work are outlined. The paper also describes the motivation for adopting cognitive architectures in the context of social simulation experiments and presents a number of research areas where cognitive social simulation may be useful in developing a better understanding of the dynamics of team cognition. These include the use of cognitive social simulation to study the role of cognitive processes in determining aspects of communicative behavior, as well as the impact of communicative behavior on the shaping of task-relevant cognitive processes (e.g., the social shaping of individual and collective memory as a result of communicative exchanges). We suggest that the ability to perform cognitive social simulation experiments in these areas will help to elucidate some of the complex interactions that exist between cognitive, social, technological and informational factors in the context of team-based problem-solving activities.' as abstract.

there is a document named 'doc-2566' that
  has 'Quantitative information-flow models typically assume that secret information is constant. But real-world secrets, such as mobile device locations and account passwords, evolve over time. So it\'s not just the current value of a secret that matters, but also how the secret changes. If information leaks about how secrets change, adversaries might be able to predict future (or past) secrets. Hiding the correlation between time and the value of a secret can even be more important than hiding the secret itself. This paper formalizes information flow in the context of dynamic systems with adaptive adversaries, and shows how to quantify leakage of information about secrets and about how secrets change. Careful modeling of the adversary\'s resources, and his decision about when to attack a system, turns out to be essential. We give operational interpretations of our metrics, relative to well-defined attack scenarios and attacker capabilities.' as abstract.

there is a document named 'doc-2569a' that
  has 'Recent developments in sensing technologies, mobile devices and context-aware user interfaces have made it pos- sible to represent information fusion and situational awareness for Intelligence, Surveillance and Reconnaissance (ISR) activities as a conversational process among actors at or near the tactical edges of a network. Motivated by use cases in the domain of Company Intelligence Support Team (CoIST) tasks, this paper presents an approach to information collection, fusion and sense-making based on the use of natural language (NL) and controlled nat- ural language (CNL) to support richer forms of human-machine interaction. The approach uses a conversational protocol to facilitate a flow of collaborative messages from NL to CNL and back again in support of interactions such as: turning eyewitness reports from human observers into actionable information (from both soldier and civilian sources); fusing information from humans and physical sensors (with associated quality metadata); and assisting human analysts to make the best use of available sensing assets in an area of interest (governed by man- agement and security policies). CNL is used as a common formal knowledge representation for both machine and human agents to support reasoning, semantic information fusion and generation of rationale for inferences, in ways that remain transparent to human users. Examples are provided of various alternative styles for user feedback, including NL, CNL and graphical feedback. A pilot experiment with human subjects shows that a prototype conversational agent is able to gather usable CNL information from untrained human subjects.' as abstract.

there is a document named 'doc-2570' that
  has 'We describe a system architecture aimed at supporting Intelligence, Surveillance, and Reconnaissance (ISR) activities in a Company Intelligence Support Team (CoIST) using natural language-based knowledge representation and reasoning, and semantic matching of mission tasks to ISR assets. We illustrate an application of the architecture using a High Value Target (HVT) surveillance scenario which demonstrates semi-automated matching and assignment of appropriate ISR assets based on information coming in from existing sensors and human patrols operating in an area of interest and encountering a potential HVT vehicle. We highlight a number of key components of the system but focus mainly on the human/machine conversational interaction involving soldiers on the field providing input in natural language via spoken voice to a mobile device, which is then processed to machine-processable Controlled Natural Language (CNL) and confirmed with the soldier. The system also supports CoIST analysts obtaining real-time situation awareness on the unfolding events through fused CNL information via tools available at the Command and Control (C2). The system demonstrates various modes of operation including: automatic task assignment following inference of new high- importance information, as well as semi-automatic processing, providing the CoIST analyst with situation awareness information relevant to the area of operation.' as abstract.

there is a document named 'doc-2571' that
  has 'Intelligence, Surveillance, and Reconnaissance (ISR) has been called the "... \'hub\' of 21st century (military) operations." Military doctrine provides guidelines and protocols for ISR, but little is known about Soldier decision-making for the allocation of ISR platforms. To determine if technology may be useful for augmenting Soldier performance with ISR, we assessed the accuracy of decision-making using simulated allocation tasks. Soldiers made decisions by assigning ISR platform sensors to simplified target detection and identification tasks. The objective, or algorithmic accuracy of the decisions were based on the National Imagery Interpretability Reconnaissance Scale (NIIRS), which consists of normative ratings of imagery interpretability by intelligence analysts across varying sensor capabilities (i.e., pixels on the sensor). Algorithmic accuracy was derived from unclassified/open-source information on sensor capabilities based on NIIRS. Soldiers performed the same set of decision-making tasks twice. First, using their own knowledge and experience with ISR and, second, with complete information on sensor capabilities. Decision accuracy was slightly lower in the first set of assignments compared with the second. However, both were below algorithmic accuracy. Results indicate technology for decision aids with ISR allocation may enhance human decision-making.' as abstract.

there is a document named 'doc-2572' that
  has 'At TCC 2013, Choi et al. introduced the notion of multi-client verifiable computation in which a set of clients outsource to an untrusted server the computation of a function f over their collective inputs in a sequence of time periods. In that work, the authors defined and realized multi-client verifiable computation satisfying soundness against a malicious server and privacy against the semi-honest corruption of a single client. We explore the possibility of achieving stronger security guarantees in this setting, in several respects. We begin by introducing a simulation-based notion of security in the universal composability framework, which provides a clean way of defining soundness and privacy in a single definition. We show the notion is impossible to achieve, even in the semi-honest case, if clientserver collusion is allowed. Faced with this result, we explore several meaningful relaxations and give constructions realizing them.' as abstract.

there is a document named 'doc-2574' that
  has 'Agents may cooperate by communicating their opinions about various phenomenon. These opinions are then fused by agents and used for informed decision-making. However, fusing opinions from diverse sources is not trivial - especially in open multiagent systems - where it is not possible to ensure that the sources are honest and their opinions are not misleading. In this paper, we propose a novel approach that exploits consistencies and conflicts between personal observations and shared information to derive trust evidence for information sources. Based on the derived evidence, we describe how opinions from diverse sources can be fused. We have evaluated our approach using two scenarios: one service selection setting and a general information fusion setting. Through extensive simulations, we have shown that our approach significantly outperforms the existing trustbased information fusion approaches.' as abstract.

there is a document named 'doc-2576' that
  has 'Smart environments are ecosystems, which seam- lessly embed IT assets into physical world\'s objects and hold promise for improving the services we receive from our social and economic ecosystems. The management of smart environment assets in multi-partner, dynamic collaboration scenarios where different sets of assets are owned and operated by different partners is a non-trivial problem, due to restrictive asset sharing policies applied by collaborating partners. In this work we formalize, evaluate and compare two asset sharing policies, investigating their impact on MSTA-P, a policy-regulated version of an existing asset-task assignment protocol. The first sharing policy is based on a traditional asset ownership model while the second is based on an edge model allowing asset sharing among collaborating partnes through cross-partner team formations. We find that while the traditional ownership model allows slightly better performance, the difference is only marginal, so a team- sharing model offers a viable alternative sharing approach.' as abstract.

there is a document named 'doc-2584' that
  has 'Big data updated by intensive write stream is becoming increasingly prevalent in our daily life (e.g., social event streams). To serve such large dataset, modern data owners usually outsource it to a third-party Cloud, which is designed to scale out to large number of data users. Due to the limited trust placed on a third-party Cloud, data owners sign the data stream so that data users can verify the query result from Cloud. In this paper, we tackle the problem of verifiable freshness in the case of multi-version stream model. We propose a memoryresident digest structure that can lead to efficient verification performance; the structure is named INCBM-TREE as it INCrementally builds a Bloom filter-embedded Merkle TREE. We have demonstrated the superior performance of verification under small memory footprint for signing, which lends the INCBM-TREE to the case of small owner and data users with limited resources.' as abstract.

there is a document named 'doc-2585' that
  has 'Nowadays, intensive data streams become more and more prevalent in our daily life, such as scalable system logs, network monitoring, social event stream, among many others. To digest such intensive stream, modern data providers usually outsource the data storage and data serving to a third-party Cloud which can not be fully untrusted. Given a query result from the Cloud, a data consumer may need to verify the data correctness and result freshness. In this paper, we address the problem of authenticating the freshness and correctness in an outsourced streaming database. To sign the intensive data stream, we propose an INCBM-TREE that INCrementally builds a Bloom filter- embedded Merkle TREE. INCBM-TREE uses Merkle tree to verify data correctness and uses BLOOM FILTER\'s to verify the data (non)- membership for freshness. Given N memory size, the incremental construction of INCBM-TREE allows for building a data batch of size 2N for signing. Comparing to prior work, the main contribution of this work includes: _ The first work to address the query result freshness in a multi- version data stream. _ A lightweight data signing scheme, which requires small memory footprint and lends itself to a scenario with intensive streams and low-power providers.' as abstract.

there is a document named 'doc-2586' that
  has 'This paper describes a multitiered architecture for realizing an inference management firewall (IMF) that employs context-aware information masking techniques for systematic management of risk-vs-value trade-off of sensor data. Previously we have demonstrated an initial implementation of the IMF running as messaging services on the Information Fabric, which is a middleware asset developed under the International Technology Alliance (ITA) research program. Furthermore, we have presented an additional asset, recently implemented on a commercially-available mobile device running the Android operating system, which is intended to operate as an information source and first-line inference management capability at the edge of the network. The low-cost and widespread use of Androidbased mobile devices offers a popular platform for crowdsourced participatory sensing. The focus of our current work is on the integration of these two technology assets in support of policymanaged, sensor-driven workflows in coalition scenarios.' as abstract.

there is a document named 'doc-2587' that
  has 'Smart phones are used to collect and share personal sensor data with third-party apps. Often these apps are from untrustworthy providers leading to data misuse and privacy violations. To mitigate privacy threats Android enforces explicit user permission for a select set of resources, traditionally perceived to be privacyprone. However, research in recent years have successfully demonstrated the inadequacy of this mechanism by using various combinations of the so-called innocuous sensors to draw inferences private to the user. In addition, the options available to the user are to either allow complete access to all requested resources or not install the app at all. We present ipShield, a framework which provides users with greater control over the shared data at runtime. ipShield performs monitoring of every sensor used by an app and publishes this information to the user. It uses the sensors accessed by an app to perform a privacy risk assessment and enumerates the list of possible inferences to the user. The choice of inferences as the alphabet for conveying privacy risk enables the user to better appreciate the implications of sharing data. A recommendation of possible privacy actions on sensor data is generated based on user preferences. Finally, the user is provided with an option to override the generated actions, and manually configure context-aware fine-grained privacy rules. We implemented ipShield by modifying AOSP version 4.2.2 r 1 and tested it on a Nexus 4. Our results indicate that running ipShield adds negligible overhead in terms of performance, has a low energy footprint, and can be used to control data sharing with a wide variety of apps.' as abstract.

there is a document named 'doc-2613' that
  has 'Functional encryption (FE) is a powerful primitive enabling fine-grained access to encrypted data. In an FE scheme, secret keys ("tokens") correspond to functions; a user in possession of a ciphertext ct = Enc(x) and a token TKf for the function f can compute f(x) but learn nothing else about x. An active area of research over the past few years has focused on the development of ever more expressive FE schemes. In this work we introduce the notion of multi-input functional encryption. Here, informally, a user in possession of a token TKf for an n-ary function f and multiple ciphertexts ct1 = Enc(x1), . . . , ctn = Enc(xn) can compute f(x1, . . . , xn) but nothing else about the {xi}. Besides introducing the notion, we explore the feasibility of multi-input FE in the public-key and symmetric-key settings, with respect to both indistinguishability-based and simulation-based definitions of security.' as abstract.

there is a document named 'doc-2618' that
  has 'Cognitive social simulation is a computer simulation technique that aims to improve our understanding of the dynamics of socially-situated and socially-distributed cognition. Cognitive architectures are typically used to support cognitive social simulation; however, the most widely used cognitive architecture - ACT-R - has, to date, been the focus of relatively few cognitive social simulation studies. The current paper reports on the results of an ongoing effort to develop an experimental simulation capability that can be used to undertake studies into sociallydistributed cognition using the ACT-R cognitive architecture. An ACT-R cognitive model is first presented that demonstrates one approach to solving a task previously used to investigate sensemaking performance within teams of human subjects. An approach to the implementation of an ACT-R cognitive social simulation capability is then described. The approach relies on the use of a variety of custom ACT-R modules and memoryresident Lisp databases. The custom modules enable ACT-R agents to exchange information with each other during the course of their sensemaking activities. The Lisp databases, in contrast, are used to store information about communicative transactions, the experimental setup and the structure of the communication network. The proposed solution provides the basic elements required to run cognitive social simulation experiments into collective sensemaking using the ACT-R architecture; however, further work needs to be undertaken in order to address a number of limitations associated with agent communication capabilities and the ability of agents to interact with the task environment.' as abstract.

there is a document named 'doc-2619' that
  has 'The aim of cognitive social simulation is to improve our understanding of the complex inter-play between factors that are spread across the cognitive, social and technological domains. This makes cognitive social simulation techniques particularly appealing as a means to undertake experiments into socially-distributed cognition. The current paper reports on the results of an ongoing effort to develop a cognitive social simulation capability that can be used to undertake studies into team cognition using the ACT-R cognitive architecture. The focus of the cognitive modeling effort associated with the development effort is a particular team-based problem solving task that forms part of the Experimental Laboratory for Investigating Collaboration, Information-sharing, and Trust (ELICIT) experimentation framework. This task has been used with human subjects to investigate the effect of different command and control organizational structures on collective problem solving performance. The results of the cognitive modeling effort are presented and future work to extend both the simulation capability and the cognitive model are outlined. By comparing the results obtained with the ACT-R simulation capability with those obtained from previous experiments using the ELICIT experimentation framework, it should be possible to evaluate the extent to which ACT-R agents exhibit performance profiles similar to those of their human counterparts. This will support the effort to evaluate the extent to which cognitive social simulation experiments with ACT-R can be used to generate findings of predictive relevance to future studies using the ELICIT experimentation framework.' as abstract.

there is a document named 'doc-2624' that
  has 'Enforcing privacy in location based services is very crucial in the current mobile world. Past literature has examined both location and identity obfuscation techniques in order to optimally tradeoff security/privacy with utility _ this primarily addresses the \'how to enforce location privacy problem\'; however, it does not address the \'where to enforce location privacy problem\'. This paper examines the \'where\' problem and in particular, examines tradeoffs between enforcing location privacy at a device vs. enforcing location privacy at an edge location server. This paper also sketches an implementation of location privacy solutions at both the device and the edge location server and presents detailed experiments using real mobility and user profile data sets collected from various data sources (taxicabs, Smartphones). Our results show that while device-based solutions do not require trust in the edge location server, they either suffer from high false positive rate (about 25% probability of not meeting the desired privacy requirement) or low utility (about 600 meters higher error in obfuscated location data).' as abstract.

there is a document named 'doc-2626' that
  has 'In this poster, we present a novel, robust, and effective de-anonymization attack on mobility trace data and social data. The experimental results demonstrate that our proposed de-anonymization attack is very effective and robust to noise.' as abstract.

there is a document named 'doc-2627' that
  has 'While expensive cryptographically verifiable computation aims at defeating malicious agents, many civil purposes of outsourced computation tolerate a weaker notion of security, i.e., "lazy-but-honest" contractors. Targeting this type of agents, we develop optimal contracts for outsourcing of computational tasks via appropriate use of rewards, punishments, auditing rate, and "redundancy". Our contracts provably minimize the expense of the outsourcer (principal) while guaranteeing correct computation. Furthermore, we incorporate practical restrictions of the maximum enforceable fine, limited and/or costly auditing, and bounded budget of the outsourcer. By examining the optimal contracts, we provide insights on how resources should be utilized when auditing capacity and enforceability are limited. Finally, we present a light-weight cryptographic implementation of the contracts and discuss a comparison across different implementations of auditing in outsourced computation.' as abstract.

there is a document named 'doc-2629' that
  has 'Intelligence analysis is the process of reasoning about information in order to produce hypothetical explanations for a situation. In this process, it is fundamental to assess how, when, and where this information has been elaborated. A model of provenance can capture this contextual information. Provenance data inevitably affects the identification of plausible conclusions, thus, it must be introduced in the reasoning process. In this paper, we propose a model of argument schemes that allows software agents to explore provenance for improving the information assessment. Argument schemes present the essential elements of provenance that warrant the credibility of the information. Schemes are also used to establish preferences between pieces of information according to different provenance criteria, such as timeliness and reliability. The introduction of schemes about provenance facilitates the decision-making process by providing a rational method to assess the credibility of a piece of information and to resolve conflicting information.' as abstract.

there is a document named 'doc-2631' that
  has 'Homomorphic MACs, introduced by Gennaro and Wichs in 2013, allow anyone to validate computations on authenticated data without knowledge of the secret key. Moreover, the secret-key owner can verify the validity of the computation without needing to know the original (authenticated) inputs. Beyond security, homomorphic MAC are required to produce short tags (succinctness) and to allow for composability (i.e. outputs of authenticated computations should be re-usable as inputs for new computations). At Eurocrypt 2013 Catalano and Fiore proposed two realizations of homomorphic MACs that support a restricted class of computations (arithmetic circuits of polynomial degree), are practically efficient, but fail to achieve both succinctness and composability at the same time. In this paper we generalize the work of Catalano and Fiore in several ways. First, we abstract away their results using the notion of encodings with limited malleability. Next, we generalize their constructions to work with graded encodings, and more abstractly with k-linear groups. The main advantage of this latter approach is that it allows for homomorphic MACs which are (somewhat) composable while retaining succinctness. Interestingly, our construction uses graded encodings in a generic way. Thus, all its limitations (limited composability and non-constant size of the tags) solely depend on the fact that currently known multilinear maps share similar constraints. This means, for instance, that our scheme would support arbitrary circuits (polynomial depth) if we had compact multilinear maps with an exponential number of levels.' as abstract.

there is a document named 'doc-2635' that
  has 'We propose a new approach to integrating probabilities and argumentation, based on Markov Random Fields, and building on the connection between conditional independence and the labelling status of arguments. Such a system overcomes the main limitation of Markov logic networks, namely that only consistent theory can be ascribed non-zero probability. Our approach provides a principled technique for the merger of probabilities and argumentation, and holds promise in allowing for the learning of an argumentation system.' as abstract.

there is a document named 'doc-2637' that
  has 'The ubiquity of smartphones has led to the emergence of mobile crowdsourcing tasks such as the detection of spatial events when smartphone users move around in their daily lives. However, the credibility of those detected events can be negatively impacted by unreliable participants with low-quality data. Consequently, a major challenge in quality control is to discover true events from diverse and noisy participants\' reports. This truth discovery problem is uniquely distinct from its online counterpart in that it involves uncertainties in both participants\' mobility and reliability. Decoupling these two types of uncertainties through location tracking will raise severe privacy and energy issues, whereas simply ignoring missing reports or treating them as negative reports will signifi- cantly degrade the accuracy of the discovered truth. In this paper, we propose a new method to tackle this truth discovery problem through principled probabilistic modeling. In particular, we integrate the modeling of location popularity, location visit indicators, truth of events and three-way participant reliability in a uni- fied framework. The proposed model is thus capable of efficiently handling various types of uncertainties and automatically discovering truth without any supervision or the need of location tracking. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art truth discovery approaches in the mobile crowdsourcing environment.' as abstract.

there is a document named 'doc-2645a' that
  has 'File uploads, even those on the order of a few hundred kilobytes, commonly fail when attempted over lossy, low- bandwidth, and highly-latent connections. These conditions, which are common for mobile networks in rural, resource- poor areas, result in the inability for residents of these areas to fully participate in the modern Internet. Without the ability to contribute culturally-relevant content, users in de- veloping regions run the risk of further marginalization or cultural loss, as the Internet becomes increasingly irrelevant to their way of life. We study Internet traffic collected from a rural African village and find that, despite good WiFi and LAN connectivity, the quality of experience for local net- work users when viewing Internet media is quite poor due to the bottlenecked backhaul link. We also find that users often abort uploads due to network performance. Guided by research findings that media content produced by local users is often heavily consumed by local users, we design VillageCache, a system that leverages locality of interest by transforming and redistributing copies of locally uploaded content at the network edge. Our prototype system focuses on mobile Facebook and YouTube traffic as our trace anal- ysis shows those services to be popular among users and TCP flows related to them to be both bandwidth-intensive and poor-performing. We evaluate our prototype in a lab environment and find the system saves bandwidth and pro- vides a drastic improvement in user quality of experience. We propose placing systems such as VillageCache at the edge of poorly-connected networks to enable mobile users to produce and consume media-rich content while mitigating the constraints present in under-resourced networks.' as abstract.

there is a document named 'doc-2660' that
  has 'Cyber security attacks are becoming ever more fre- quent and sophisticated. Enterprises often deploy several security protection mechanisms, such as anti-virus software, intrusion detection/prevention systems, and firewalls, to protect their critical assets against emerging threats. Unfortunately, these protection systems are typically "noisy", e.g., regularly generating thousands of alerts every day. Plagued by false positives and irrelevant events, it is often neither practical nor cost-effective to analyze and respond to every single alert. The main challenge faced by enterprises is to extract important information from the plethora of alerts and to infer potential risks to their critical assets. A better understanding of risks will facilitate effective resource allocation and prioritization of further investigation. In this paper, we present MUSE, a system that analyzes a large number of alerts and derives risk scores by correlating diverse entities in an enterprise network. Instead of considering a risk as an isolated and static property, MUSE models the dynamics of a risk based on the mutual reinforcement principle. We evaluate MUSE with real- world network traces and alerts from a large enterprise network, and demonstrate its efficacy in risk assessment and flexibility in incorporating a wide variety of data sets.' as abstract.

there is a document named 'doc-2669a' that
  has 'Mobile micro-cloud is an emerging technology in distributed computing, which is aimed at providing seamless computing/data access to the edge of the network when a centralized service may suffer from poor connectivity and long latency. Different from the traditional cloud, a mobile micro-cloud is smaller and deployed closer to users, typically attached to a cellular basestation or wireless network access point. Due to the relatively small coverage area of each basestation or access point, when a user moves across areas covered by different basestations or access points which are attached to different micro-clouds, issues of service performance and service migration become important. In this paper, we consider such migration issues. We model the general problem as a Markov decision process (MDP), and show that, in the special case where the mobile user follows a one-dimensional asymmetric random walk mobility model, the optimal policy for service migration is a threshold policy. We obtain the analytical solution for the cost resulting from arbitrary thresholds, and then propose an algorithm for finding the optimal thresholds. The proposed algorithm is more efficient than standard mechanisms for solving MDPs.' as abstract.

there is a document named 'doc-2670' that
  has 'This paper investigates how to restrict the spread of sensitive information. This work is situated in a military context, and provides a tractable method to decide what semantic information to share, with whom, and how. The latter decision is supported by obfuscation, where a related concept or fact may be shared instead of the original. We consider uncertain information, and utilise Subjective Logic as an underlying formalism to further obfuscate concepts, and reason about obfuscated concepts.' as abstract.

there is a document named 'doc-2675' that
  has 'This paper describes the research undertaken under the International Technology Alliance (ITA) [1] to explore the ELICIT identification task [2] as an example of a complex problem that simultaneously involves many cognitive tasks which would have to be modelled in order to be achieved by a computer agent. This task requires the identification of the participants in a possible attack, using information contained in short Natural Language (NL) sentences. Successful problem solving in this task requires the analysis of NL sentences based on common sense knowledge, the representation of the domain, the representation of a problem solving strategy, the construction of suitable reasoning rules, the running of the rules, the making of assumptions, and detection of inconsistencies and the presentation of rationale. All of these problems are those faced by an analyst in their task, and an understanding of how these problems could be addressed could allow the development of support mechanisms to facilitate their performance. The paper describes research into how the use of a Controlled Natural Language (ITA Controlled English, or CE) [3] can contribute to the solution of these problems. Knowledge of the domain was represented in a CE conceptual model, and this model defined the concepts and the logical rules to perform reasoning. The full task of interpreting the NL sentences was considered at this stage to be too complex, since much common sense knowledge would have to be extracted and represented. It was therefore decided initially to have a human convert the original sentences into other more simple English sentences thus temporarily avoiding the more complex problems of common-sense interpretation, and it was also decided to focus on one aspect of the identification task (WHO were the responsible agents). The sentences were analysed by the ERG system [4] into an MRS [5] linguistic semantic representation, which was further analysed by a CE-based linguistic-to-domain semantic mapping system, resulting in the extraction of domain CE facts from the sentences. However some of the NL sentences expressed rules rather than facts, and research was undertaken to determine how the CE rules for processing the sentence could themselves generate other CE rules that could be added to the domain model in order to solve the ELICIT identification problem. The paper describes an extension of the CE meta-model and CE system to allow rules to be objects generated by other rules. The domain model and extracted rules were run to identify the "WHO", and this provided a set of CE facts expressing the rationale graph of the reasoning leading to the conclusion. This reasoning could be seen to be based upon two assumptions in the interpretation of the sentences, and these assumptions could be explored in the rationale graph, allowing the analyst to understand the reasoning and the sources of uncertainty in the conclusions.' as abstract.

there is a document named 'doc-2677' that
  has 'Providing analysts and decision makers with a means of assessing how certain extracted information is, and what the sources of uncertainty are, is an important part of the provenance of a piece of intelligence for decision-making. A major source of (un)certainty derives from the text in reports that analysts and decision makers rely on. We outline an analysis of uncertainty expressions used in English. We show how a controlled English can be used to represent the uncertainty expressions and infer important information, and how this information might be represented to analysts in support of decision-making for military operations.' as abstract.

there is a document named 'doc-2685' that
  has 'In graph databases, a given graph query can be executed in a large variety of semantically equivalent ways. Each such execution plan produces the same results, but at different computation costs. The query planning problem consists of finding, for a given query, an execution plan with the minimum cost. The traditional greedy or heuristic cost-based approaches addressing the query planning problem do not guarantee by design the optimality of the chosen execution plan. In this paper, we present a principled framework to solve the query planning problem by casting it into an Integer Linear Programming problem, and discuss its applications to testing and improving heuristic-based query planners.' as abstract.

there is a document named 'doc-2687' that
  has 'Online debating tools enable collaboration in analysing and addressing a wide range of problems. In contexts such as e-participation in legislation, scientific inquiry or open source intelligence analysis, structured and well-formulated arguments are essential for the collaborative formation of justified viewpoints. This type of debate requires analysis of information, construction and sharing of arguments, and opinion polling. Existing tools often provide limited support for these different interaction modes. In this paper we propose CISpaces, an initial set of tools for collaborative virtual spaces of analysis of arguments and debate. CISpaces provides a uniform way of constructing and exchanging arguments based upon argumentation schemes. This structure permits autonomous reasoning to be applied for supporting the identification of plausible conclusions. In CISpaces users can access a crowd-sourcing service to gather responses from a wide audience. Moreover, the context of retrieval of information and arguments is collected in CISpaces using provenance, and this provenance data is elaborated by the system to evaluate claims according to their credibility.' as abstract.

there is a document named 'doc-2688' that
  has 'RAM-model secure computation addresses the inherent limitations of circuit-model secure computation considered in almost all previous work. Here, we describe the first automated approach for RAM-model secure computation in the semihonest model. We define an intermediate representation called SCVM and a corresponding type system suited for RAM-model secure computation. Leveraging compile-time optimizations, our approach achieves order-of-magnitude speedups compared to both circuit-model secure computation and the state-of-art RAMmodel secure computation.' as abstract.

there is a document named 'doc-2689' that
  has 'In this paper, we present a novel and effective de-anonymization attack on mobility trace data and social data. First, we design an Unified Similarity (US) measurement. By analyzing US on real data sets, we find that some data can potentially be de-anonymized accurately and precisely while the other data can be de-anonymized with a coarse granularity. Utilizing this property, we present a US based De-Anonymization (DA) framework which iteratively de-anonymizes data with an accuracy guarantee. Then, to de-anonymize data without the knowledge of the overlap size between the anonymized data and the auxiliary data, we generalize DA to an Adaptive De-Anonymization (ADA) framework. By methodically working on two core matching subgraphs, ADA achieves high deanonymization accuracy with relatively low computational overhead. Finally, we examine DA/ADA on mobility traces and social data sets. The experimental results demonstrate that our proposed de-anonymization attack is very effective and robust to noise.' as abstract.

there is a document named 'doc-2694' that
  has 'We consider a scenario in which a data owner outsources storage of a large graph to an untrusted server; the server performs computations on this graph in response to queries from a client (whether the data owner or others), and the goal is to ensure verifiability of the returned results. Existing work on verifiable computation (VC) would compile each graph computation to a circuit or a RAM program and then use generic techniques to produce a cryptographic proof of correctness for the result. Unfortunately, such an approach will incur large overhead, especially in the proof-computation time. In this work we address the above by designing, building, and evaluating ALITHEIA, a nearly practical VC system tailored for graph queries such as computing shortest paths, longest paths, and maximum flow. The underlying principle of ALITHEIA is to minimize the use of generic VC systems by leveraging various algorithmic techniques specifically for graphs. This leads to both theoretical and practical improvements. Asymptotically, it improves the complexity of proof computation by at least a logarithmic factor. On the practical side, we show that ALITHEIA achieves signifi- cant performance improvements over current state-of-the-art (up to a 108_ improvement in proof-computation time, and a 99.9% reduction in server storage), while scaling to 200,000-node graphs.' as abstract.

there is a document named 'doc-2697' that
  has 'Rapid but informed decision-making capabilities at lower echelons are fast becoming a necessity in many coalition operations due to the dynamism associated with such environments. In this paper we investigate technologies to assist CoIST (Company Intelligence Support Team) users operating at the network edge in support of military operations. Through an integration experiment we illustrate the impact of such technologies in rapid decision-making situations. The paper describes the technology integration experiment in the context of a vignette and shows how a natural language conversational interface between human and machine agents in a hybrid team is used. The system can capture local information reporting, infer high value information based on background knowledge, automatically raise intelligence tracking tasks and match, rank and propose appropriate assets to tasks, taking into account contextual factors such as environmental and the distributed network conditions. The approach utilizes ontology-based resource matching capabilities and uses a Controlled Natural Language as a human-friendly - but machine processable - language that is expressive enough to serve as a single common format for both human and machine processing. This capability is designed to operate in a lightweight distributed environment at the edge of the network.' as abstract.

there is a document named 'doc-2702' that
  has 'It has been claimed that computational models of argumentation provide support for complex decision making activities in part due to the close alignment between their semantics and human intuition. In this paper we assess this claim by means of an experiment: people\'s evaluation of formal arguments - presented in plain English - is compared to the conclusions obtained from argumentation semantics. Our results show a correspondence between the acceptability of arguments by human subjects and the justification status prescribed by the formal theory in the majority of the cases. However, posthoc analyses show that there are some significant deviations, which appear to arise from implicit knowledge regarding the domains in which evaluation took place. We argue that in order to create argumentation systems, designers must take implicit domain specific knowledge into account.' as abstract.

there is a document named 'doc-2703' that
  has 'We present the CISpaces framework, a collaborative virtual space for intelligence analysis. In delivering intelligence, analysts are required to elaborate information about a situation and form hypothetical explanations. CISpaces supports the analysis of ambiguous information exploiting structured argumentation-based reasoning and collaboration. Here we demonstrate an initial set of tools that delivers support to analysts in constructing and evaluating hypotheses. Argumentation schemes are used as a uniform way to exchange and critically analyse evidence in collaboration. The system enables analysts to canvas a large group of collectors to gather information relevant to the analysis. The context of retrieval of information and analysis is collected in CISpaces using provenance elaborated by the system to establish the credibility of the claims.' as abstract.

there is a document named 'doc-2706' that
  has 'In this paper, we study the problem of selecting paths to improve the performance of network tomography applications in the presence of network element failures. We model the robustness of paths in network tomography by a metric called expected rank. We formulate an optimization problem to cover two complementary performance metrics: robustness and probing cost. The problem aims at maximizing the expected rank under a budget constraint on the probing cost. We prove that the problem is NP-Hard. Under the assumption that the failure distribution is known, we propose an algorithm called RoMe with guaranteed approximation ratio. Moreover, since evaluating the expected rank is generally hard, we provide a bound which can be evaluated efficiently. We also consider the case in which the failure distribution is not known, and propose a reinforcement learning algorithm to solve our optimization problem, using RoMe as a subroutine. We run a wide range of simulations under realistic network topologies and link failure models to evaluate our solution against a state-of-art path selection algorithm. Results show that our approaches provide significant improvements in the performance of network tomography applications under failures.' as abstract.

there is a document named 'doc-2715' that
  has 'The Virtual Information Exchange (VIE) strives to enable coalition service providers to share services and inter- operate with other shared services in a controlled and secure environment. Services may dynamically and selectively be made available from behind a VIE gateway which then exposes those services to other gateways participating in the coalition according to policy constraints. Services are then only accessible across gateways in a secure, controlled and monitored fashion. Services are registered with semantic metadata to enable match making between service consumers and providers. Further, services can be composed and recomposed using a service template that describes the services it requires. The gateways form an overlay network to create a distributed registry of services that can accommodate dynamic changes to the network topology. Further, the VIE provides a generic framework that can be easily configured to support different domains, including the mobile ad- hoc coalition scenarios we envision. We will describe the details of the proposed architecture, use case scenarios and some early experience with the VIE.' as abstract.

there is a document named 'doc-2716' that
  has 'With the rapid deployment of cellular networks, modern mobile devices are now equipped with at least two interfaces (WiFi and 3G/4G). As multi-path TCP (MPTCP) has been standardized by the IETF, mobile users running MPTCP can access the Internet via multiple interfaces simultaneously to provide robust data transport and better throughput. However, as cellular networks exhibit large RTTs compared to WiFi, for small data transfers, the delayed startup of additional flows in the current MPTCP design can limit the use of MPTCP. For large data transfers, when exploiting both the WiFi and cellular networks, the inflated and varying RTTs of the cellular flow together with the small and stable RTTs of the WiFi flow can lead to performance degradation. In this paper, we seek to investigate the causes of MPTCP performance issues in wireless environments and will provide analyses and a solution for better performance.' as abstract.

there is a document named 'doc-2719' that
  has 'Multi-Path TCP is a new transport protocol that enables systems to exploit available paths through multiple network interfaces. MPTCP is particularly useful for mobile devices, which frequently have multiple wireless interfaces. However, these devices have limited power capacity and thus judicious use of these interfaces is required. In this work, we develop a model for MPTCP energy consumption derived from experimental measurements using MPTCP on a mobile device with both cellular and WiFi interfaces. Using our MPTCP energy model, we identify the operating region where MPTCP can be more power efficient than either standard TCP or MPTCP. Based on our findings, we also design and implement an improved energy-efficient MPTCP that reduces power consumption by up to 8% in our experiments, while preserving the availability and robustness benefits of MPTCP.' as abstract.

there is a document named 'doc-2720' that
  has 'In this paper we show that the logical framework proposed by Becker et al. to reason about security policy behavior in a trust management context can be captured by an operational framework that is based on the language proposed by Miller to deal with scoping and/or modules in logic programming in 1989. The framework of Becker et al. uses propositional Horn clauses to represent both policies and credentials, implications in clauses are interpreted in counterfactual logic, a Hilbert-style proof is defined and a system based on SAT is used to proof whether properties about credentials, permissions and policies are valid in trust management systems, i.e. formulas that are true for all possible policies. Our contribution is to show that instead of using a SAT system, this kind of validation can rely on the operational semantics (derivability relation) of Miller\'s language, which is very close to derivability in logic programs, opening up the possibility to extend Becker et al.\'s framework to the more practical first order case since Miller\'s language is first order.' as abstract.

there is a document named 'doc-2721' that
  has 'Service Oriented Sensor Networks consist of various assets and host variety of services, some of which are composed of other services. Policies are widely used for regulating access to assets and services specially when these assets are owned by different parties in a coalition environment. In this paper, we present a novel mechanism for policy implementation to provide or restrict access to resources using policies. We present "Restriction Set Theoretic Expressions (RSTE)" to represent assets and policies in the form of sets at system level, therefore RSTE is independent of high-level representation of policies and assets. High-level representation of network assets and policies can be easily translated to semantically defined RSTE sets and then different RSTE operations are applied to restrict or release access to resources. RSTE defines sets and operations that can be performed on the sets to implement policies. We describe semantics of RSTE sets and operations for assets in service configuration in WSNs and show how the services and policies can be represented as sets. We then leverage the capabilities of relational databases by representing sets as tables and applying policies as set operations executed as SQL queries. Operations performed on the database tables yield restricted sets of policy enforced services. Such services can then be provided to the user or used by service configuration to compose complex services. If service composition cannot be performed due to policy restrictions, the restricting conditions are reported to user through presentation layer for policy negotiation and relaxation.' as abstract.

there is a document named 'doc-2722' that
  has 'There is a need for robust networks in all environments including austere ones. The prime example is Delay Tolerant Networks (DTN) that are subject to a growing body of research. These networks support applications used by the military and first responders, especially in emergency situations. Securing a DTN is not a trivial undertaking due to node mobility and the ADHOC method in which nodes communicate. Here, we propose a distributed trust management scheme to secure this class of networks with many of the underlying principles applicable to the larger class of Mobile ADHOC Networks (MANET). The scheme employs erasure coding not only to increase delivery rate in a DTN but also to infer the trustworthiness of the nodes along all paths that deliver a message segments to the destination. The proposed approach enables us to decide when the destination node should stop waiting for additional segments and instead request message retransmission. Moreover, even after the message is recreated successfully, additional segments received enable the destination to collect precious trust information about the nodes involved in delivery of these segments. We show how distributing this trust information identifies compromised nodes.' as abstract.

there is a document named 'doc-2724' that
  has 'Hybrid networks consisting of MANET nodes and cellular infrastructure have been recently proposed to improve the performance of military networks. Prior work has demonstrated the benefits of in-network content caching in a wired, Internet context. We investigate the problem of developing optimal routing and caching policies in a hybrid network supporting in-network caching with the goal of minimizing overall content-access delay. Here, needed content may always be accessed at a back-end server via the cellular infrastructure; alternatively, content may also be accessed via cache-equipped "cluster" nodes within the MANET. To access content, MANET nodes must thus decide whether to route to in-MANET cluster nodes or to back-end servers via the cellular infrastructure; the in-MANET cluster nodes must additionally decide which content to cache. We model the cellular path as either i) a congestion-insensitive fixed-delay path or ii) a congestion-sensitive path modeled as an M/M/1 queue. We demonstrate that under the assumption of stationary, independent requests, it is optimal to adopt static caching (i.e., to keep a cache\'s content fixed over time) based on content popularity. We also show that it is optimal to route to inMANET caches for content cached there, but to route requests for remaining content via the cellular infrastructure for the congestion-insensitive case and to split traffic between the inMANET caches and cellular infrastructure for the congestionsensitive case. We develop a simple distributed algorithm for the joint routing/caching problem and demonstrate its efficacy via simulation.' as abstract.

there is a document named 'doc-2726' that
  has 'We investigate the problem of localizing node failures in a communication network from end-to-end path measurements, under the assumption that a path behaves normally if and only if it does not contain any failed nodes. To uniquely localize node failures, the measured paths must show different symptoms under different failure events, i.e., for any two distinct sets of failed nodes, there must be a measurable path traversing one and only one of them. This condition is, however, impractical to test for large networks due to the combinatorial numbers of paths and failure sets. Our first contribution is a characterization of this condition in terms of easily verifiable conditions of the network topology and the placement of monitors under three families of probing mechanisms, which differ in whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cycle-free, or (iii) uncontrollable (i.e., determined by the default routing protocol). Our second contribution is a characterization of the maximum identifiability of node failures, measured by the maximum number of simultaneous failures that can always be uniquely localized. Specifically, we bound the maximal identifiability from both sides, with the upper and the lower bounds differing by at most one, and show that these bounds can be evaluated in polynomial time. Finally, we quantify the impact of the probing mechanism on the capability of node failure localization by numerically comparing the maximum identifiability under different probing mechanisms on both random and real network topologies. We observe that despite a higher implementation cost, probing along controllable paths can significantly improve a network\'s capability to localize simultaneous node failures.' as abstract.

there is a document named 'doc-2730' that
  has 'We study the task of efficient verifiable delegation of computation on encrypted data. First, we improve previous definitions in order to tolerate adversaries that learn whether or not clients accept the result of a delegated computation. Then, in this strong model, we show a scheme for arbitrary computations, and we propose highly efficient schemes for delegation of various classes of functions, such as linear combinations, high-degree univariate polynomials, and multivariate quadratic polynomials. Notably, the latter class includes many useful statistics. Using our solution, a client can store a large encrypted dataset with a server, query statistics over this data, and receive encrypted results that can be efficiently verified and decrypted. As a key contribution for the efficiency of our schemes, we develop a novel homomorphic hashing technique that allows us to efficiently authenticate computations, at the same cost as if the data were in the clear, avoiding a 104 overhead, which would occur with a naive approach. We confirm our theoretical analysis with extensive implementation tests that show the practical feasibility of our schemes.' as abstract.

there is a document named 'doc-2743' that
  has 'In many diverse settings, aggregated opinions of others play an increasingly dominant role in shaping individual decision making. One key prerequisite of harnessing the "crowd wisdom"is the independency of individuals\' opinions, yet in real settings collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective opinions distorts individuals\' decision making as well as their perceptions of quality and value, highlighting a fundamental disconnect from current modeling efforts: How to model social influence and its impact on systems that are constantly evolving? In this paper, we develop a mechanistic framework to model social influence of prior collective opinions (e.g., online product ratings) on subsequent individual decision making. We find our method successfully captures the dynamics of rating growth, helping us separate social influence bias from inherent values. Using large-scale longitudinal customer rating datasets, we demonstrate that our model not only effectively assesses social influence bias, but also accurately predicts long-term cumulative growth of ratings solely based on early rating trajectories. We believe our framework will play an increasingly important role as our understanding of social processes deepens. It promotes strategies to untangle manipulations and social biases and provides insights towards a more reliable and effective design of social platforms.' as abstract.

there is a document named 'doc-2744' that
  has 'In this paper, we study the quantification, practice, and implications of structural data (e.g., social data, mobility traces, etc.) De-Anonymization (DA). First, we address several open problems in structural data DA by quantifying perfect and (1 _ _)-perfect structural data DA, where _ is the error tolerated by a DA scheme. To the best of our knowledge, this is the first work on quantifying structural data DA under a general data model, which remedies the gap between structural data DA practice and theory. Second, we conduct the first large-scale study on the de-anonymizability of 26 real world structural datasets, including Social Networks (SNs), Collaborations Networks, Communication Networks, Autonomous Systems, Peer-to-Peer networks, etc. We also quantitatively show the conditions for perfect and (1 _ _)-perfect DA of the 26 datasets. Third, following our quantification, we design a practical and novel single-phase cold start Optimization based DA (ODA) algorithm. Experimental analysis of ODA shows that about 77.7% _ 83.3% of the users in Gowalla (.2M users and 1M edges) and 86.9% _ 95.5% of the users in Google+ (4.7M users and 90.8M edges) are de-anonymizable in different scenarios, which implies optimization based DA is implementable and powerful in practice. Finally, we discuss the implications of our DA quantification and ODA and provide some general suggestions for future secure data publishing.' as abstract.

there is a document named 'doc-2758' that
  has 'We investigate the problem of placing monitors to localize node failures in a communication network from endto-end path measurements, under the assumption that a path measurement is normal if and only if it does not contain any failed nodes. To uniquely localize failed nodes, the measurement paths must show different symptoms (path states) under different failure events. Our goal is to deploy the minimum set of monitors needed to satisfy this condition for a given probing mechanism. We consider three families of probing mechanisms, according to whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cycle-free, or (iii) uncontrollable (i.e., determined by the default routing protocol). Considering the fact that multiple simultaneous node failures are unlikely, we focus on single-node failures, for which we develop efficient monitor placement algorithms that guarantee unique localization with the minimum number of monitors. Using these algorithms, we study the impact of probing mechanism on node failure localization by comparing the required numbers of monitors on real network topologies across probing families. We observe that despite higher implementation costs, probing mechanisms with controllable paths can significantly reduce the number of monitors required for localizing node failures.' as abstract.

there is a document named 'doc-2766' that
  has 'Metrics for quantifying information leakage assume that an adversary\'s gain is the defender\'s loss. We demonstrate that this assumption does not always hold via a class of scenarios. We describe how to extend quantification to account for a defender with goals distinct from adversary failure. We implement the extension and experimentally explore the impact on the measured information leakage of the motivating scenario.' as abstract.

there is a document named 'doc-2768' that
  has 'Online video streaming through mobile devices has become extremely popular nowadays. Youtube, for example, reported that the percentage of its traffic streaming to mobile devices has soared from 6% to more than 40% for the past two years. Moreover, people are constantly seeking to stream high quality videos for better experience while often suffering from limited bandwidth. Thanks to the rapid deployment of content delivery networks (CDNs), popular videos now have multiple replicas at different sites, and users can stream videos from a close-by location with low latency. As mobile devices nowadays are equipped with multiple wireless interfaces (e.g., WiFi and 3G/4G), aggregating bandwidth for high definition video streaming has become possible. We propose a client-based video streaming solution, called MSPlayer, that takes advantage of multiple video sources as well as network paths through different interfaces. MSPlayer reduces start-up latency, and aims to provide high quality video streaming and robust data transport in mobile scenarios. We experimentally demonstrate our solution on a testbed and through the Youtube video service.' as abstract.

there is a document named 'doc-2791' that
  has 'Routing in information-centric networking remains an open problem. The main issue is scalability. Traditional IP routing can be used with name prefixes, but it is believed that the number of prefixes will grow too large. A related problem is the use of per-packet in-network state (to cut loops and return data to consumers). We develop a routing scheme that solves these problems. The service model of our information-centric network supports information pull and push using tag sets as information descriptors. Within this service model, we propose a routing scheme that supports forwarding along multiple loop-free paths, aggregates addresses for scalability, does not require per-packet network state, and leads to near-optimal paths on average. We evaluate the scalability of our routing scheme, both in terms of memory and computational complexity, on the full Internet AS-level topology and on the internal networks of representative ASes using realistic distributions of content and users extrapolated from traces of popular applications. For example, a population of 500 million users requires a routing information base of 3.8GB with an almost flat growth and, in this case, a routing update (one content descriptor) can be processed in 2ms on commodity hardware. We conclude that information-centric networking is feasible, even with (or perhaps thanks to) addresses consisting of expressive content descriptors.' as abstract.

there is a document named 'doc-2793a' that
  has 'We study the problem of maximizing the multicast throughput in a dense multi-channel multi-radio (MC-MR) wireless network with multiple multicast sessions. Specifically, we consider a fully connected network topology where all nodes are within transmission range of each other. In spite of its simplicity, this topology is practically important since it is encountered in several real-world settings. Further, a solution to this network can serve as a building block for more general scenarios that are otherwise intractable. For this network, we show that the problem of maximizing the uniform multicast throughput across multiple sessions is NP-hard. However, its special structure allows us to derive useful upper bounds on the uniform multicast throughput across multiple sessions. We show that an intuitive class of algorithms that maximally exploit the wireless broadcast feature can have very poor worst case performance. Using a novel group splitting idea, we then design two polynomial time approximation algorithms that are guaranteed to achieve a constant factor of the capacity bound under arbitrary multicast group memberships. These algorithms are simple to implement and provide interesting trade-offs between the achievable throughput and the total number of transmissions used.' as abstract.

there is a document named 'doc-2812' that
  has 'We investigate the problem of optimal request routing and content caching in a heterogeneous network supporting in-network content caching with the goal of minimizing average content access delay. Here, content can either be accessed directly from a back-end server (where content resides permanently) or be obtained from one of multiple in-network caches. To access a piece of content, a user must decide whether to route its request to a cache or to the back-end server. Additionally, caches must decide which content to cache. We investigate the problem complexity of two problem formulations, where the direct path to the back-end server is modeled as i) a congestionsensitive or ii) a congestion-insensitive path, reflecting whether or not the delay of the uncached path to the back-end server depends on the user request load, respectively. We show that the problem is NP-complete in both cases. We prove that under the congestion-insensitive model the problem can be solved optimally in polynomial time if each piece of content is requested by only one user, or when there are at most two caches in the network. We also identify a structural property of the user-cache graph that potentially makes the problem NP-complete. For the congestionsensitive model, we prove that the problem remains NP-complete even if there is only one cache in the network and each content is requested by only one user. We show that approximate solutions can be found for both models within a (1 _ 1/e) factor of the optimal solution, and demonstrate a greedy algorithm that is found to be within 1% of optimal for small problem sizes. Through trace-driven simulations we evaluate the performance of our greedy algorithms, which show up to a 50% reduction in average delay over solutions based on LRU content caching.' as abstract.

there is a document named 'doc-2815' that
  has 'The Internet routing system faces serious scalability challenges, due to the growing number of IP prefixes it needs to propagate throughout the network. Although these pre-fixes are assigned hierarchically, and roughly align with geographic regions, today\'s Border Gateway Protocol (BGP) and operational practices do not exploit opportunities to aggregate routes. We present a distributed route-aggregation technique (called DRAGON) where nodes analyze BGP routes across different prefixes to determine which of them can be filtered while respecting the routing policies for forwarding data-packets. DRAGON works with BGP, can be deployed incrementally, and offers incentives for ASs to upgrade their router software. We present a theoretical model of routeaggregation, and the design and analysis of DRAGON. Our experiments with realistic assignments of IP prefixes, network topologies, and routing policies show that DRAGON reduces the number prefixes in each AS by about 80% and significantly curtails the number of routes exchanged during transient periods of convergence.' as abstract.

there is a document named 'doc-2830' that
  has 'Information about quantitative characteristics in local businesses and services, such as the number of people waiting in line in a cafe and the percent of available fitness equipment in a gym, is important for informed decision, crowd management and event detection. However, such information is not available in current information services. In this paper, we investigate the potential of leveraging crowds as sensors to report such quantitative characteristics and investigate how accurately we can recover the true quantity values from noisy crowdsourced information. Through experiments, we find that crowd sensors have both bias and variance in quantity sensing, and task difficulty impacts the sensing accuracy. Based on these findings, we propose an unsupervised, probabilistic model to automatically find true quantity values from noisy crowdsourced information. Our model differs from existing categorical truth finding models as ours is specifically designed to tackle quantitative truth. In addition to devising an efficient model inference algorithm in a batch mode, we also design an even faster online version that can efficiently handle streaming data. Experimental results demonstrate that our method is able to accurately recover true quantitative characteristics in various scenarios.' as abstract.

there is a document named 'doc-2841' that
  has 'Seamless computing and data access is enabled by the emerging technology of mobile micro-clouds (MMCs). Different from traditional centralized clouds, an MMC is typically connected directly to a wireless base-station and provides services to a small group of users, which allows users to have instantaneous access to cloud services. Due to the limited coverage area of base-stations and the dynamic nature of mobile users, network background traffic, etc., the question of where to place the services to cope with these dynamics arises. In this paper, we focus on dynamic service placement for MMCs. We consider the case where there is an underlying mechanism to predict the future costs of service hosting and migration, and the prediction error is assumed to be bounded. Our goal is to find the optimal service placement sequence which minimizes the average cost over a given time. To solve this problem, we first propose a method which solves for the optimal placement sequence for a specific look-ahead time-window, based on the predicted costs in this time-window. We show that this problem is equivalent to a shortest-path problem and propose an algorithm with polynomial time-complexity to find its solution. Then, we propose a method to find the optimal look-ahead window size, which minimizes an upper bound of the average cost. Finally, we evaluate the effectiveness of the proposed approach by simulations with realworld user-mobility traces.' as abstract.

there is a document named 'doc-2847' that
  has 'Principal Agent Theory (PAT) seeks to identify incentives and sanctions that a consumer should offer a producer as part of a contract in order to maximise the former\'s utility. However, identifying optimal contracts in large systems is difficult, particularly when little information is available about producer competencies. In this work we propose that a global contract be used to govern such interactions, derived from the properties of a representative agent. After describing how such a contract can be obtained, we analyse the contract utility space and its properties. Finally, we suggest how our work can be integrated with existing work on multi-agent systems.' as abstract.

there is a document named 'doc-2848' that
  has 'This paper summarises recent progress towards the initial realisation of a multitiered inference management architecture. This architecture enables systematic control to be exercised over the risk and utility trade-off when sharing information in a mobile, ad-hoc network. Our previous work proposed a set of requirements for, and challenges facing, the development of an end-to-end inference management capability which has guided the latest design and implementation activities. Newly introduced features include the ability to distribute the command and control of the inference management process within the network. This facilitates the separation of phases in the policy specification and enforcement process thereby enabling diverse scenarios involving multiple parties where authority is distributed. We have also developed a messaging schema for expressing inference management goals which we are currently evaluating in the context of an information sharing scenario.' as abstract.

there is a document named 'doc-2849a' that
  has 'Recent advances in natural language question-answering systems and context-aware mobile apps create oppor- tunities for improved sensemaking in a tactical setting. Users equipped with mobile devices act as both sensors (able to acquire information) and effectors (able to act in situ), operating alone or in collectives. The currently- dominant technical approaches follow either a pull model (e.g. Apple\'s Siri or IBM\'s Watson which respond to users\' natural language queries) or a push model (e.g. Google\'s Now which sends notifications to a user based on their context). There is growing recognition that users need more flexible styles of conversational interaction, where they are able to freely ask or tell, be asked or told, seek explanations and clarifications. Ideally such conversations should involve a mix of human and machine agents, able to collaborate in collective sensemaking activities with as few barriers as possible. Desirable capabilities include adding new knowledge, collaboratively building models, invoking specific services, and drawing inferences. As a step towards this goal, we collect evidence from a number of recent pilot studies including natural experiments (e.g. situation awareness in the context of organised protests) and synthetic experiments (e.g. human and machine agents collaborating in infor- mation seeking and spot reporting). We identify some principles and areas of future research for "conversational sensemaking".' as abstract.

there is a document named 'doc-2850' that
  has 'Social media sources such as Twitter have proven to be a valuable medium for obtaining real-time information on breaking events, as well as a tool for campaigning. When tweeters can be characterised in terms of location (e.g., because they geotag their updates, or mention known places) or topic (e.g., because they refer to thematic terms in an ontology or lexicon) their posts can provide actionable information. Such information can be obtained in a passive mode, by collecting data from Twitter\'s APIs, but even greater value can be gained from an active mode of operation, by engaging with particular tweeters and asking for clarifications or amplifications. Doing so requires knowledge of individual tweeters as "sensing assets". In this paper we show how the use of social media as a kind of sensor can be accommodated within an existing framework for sensor-task matching, by extending existing ontologies of sensors and mission tasks, and accounting for variable information quality. An integrated approach allows tweeters to be "accessed" and "tasked" in the same way as physical sensors (unmanned aerial and ground systems) and, indeed, combined with these more traditional kinds of source. We illustrate the approach using a number of case studies, including field trials (obtaining eyewitness reports from the scene of organised protests) and synthetic experiments (crowdsourced situational awareness).' as abstract.

there is a document named 'doc-2852' that
  has 'A key aspect of an analyst\'s task in providing relevant information from data is the reasoning about the implications of that data, in order to build a picture of the real world situation. This requires human cognition, based upon domain knowledge about individuals, events and environmental conditions. For a computer system to collaborate with an analyst, it must be capable of following a similar reasoning process to that of the analyst. We describe ITA Controlled English (CE), a subset of English to represent analyst\'s domain knowledge and reasoning, in a form that it is understandable by both analyst and machine. CE can be used to express domain rules, background data, assumptions and inferred conclusions, thus supporting human-machine interaction. A CE reasoning and modeling system can perform inferences from the data and provide the user with conclusions together with their rationale. We present a logical problem called the "Analysis Game", used for training analysts, which presents "analytic pitfalls" inherent in many problems. We explore an iterative approach to its representation in CE, where a person can develop an understanding of the problem solution by incremental construction of relevant concepts and rules. We discuss how such interactions might occur, and propose that such techniques could lead to better collaborative tools to assist the analyst and avoid the "pitfalls".' as abstract.

there is a document named 'doc-2857b' that
  has 'Signature-based network intrusion detection systems (S-IDS) have become an important security tool in the protection of an organization\'s infrastructure against external intruders. By analyzing network traffic, S-IDS\' detect network intrusions. An organization may deploy one or multiple S-IDS\', and each of them works independently with the assumption that it can monitor all packets of a given flow to detect intrusion signatures. However, emerging technologies (e.g., Multi-Path TCP) violate this assumption, as traffic can be concurrently sent across all available paths (e.g., WiFi, Cellular, etc.) to boost the end-users\' network performances. Attackers may exploit this capability and split malicious payloads across multiple paths to evade traditional signature-based network intrusion detection systems. Although multiple monitors may be deployed, none of them has the full coverage of the network traffic to detect the intrusion signature. In this paper, we formalise this distributed signature-based intrusion detection problem as an asynchronous online exact string matching problem, and propose an algorithm for it. To demonstrate the effectiveness of our proposal we have implemented our algorithm and conducted comprehensive experiments. Our experimental results show that the behaviour of our algorithm depends only on the packet arrival rate: delay in detecting the signature grows linearly with respect to the packet arrival rate and with small overhead for monitor communications.' as abstract.

there is a document named 'doc-2858' that
  has 'Network Function Virtualization (NFV) is bringing closer the possibility to truly migrate enterprise datacenters into the cloud. However, moving applications from private datacenters to cloud centers is complicated because many of these applications require network-based services: firewalls, IDS, load balancers, etc. Furthermore, network centered applications such as the 3G/4G IP Multimedia Subsystem (IMS) have been excluded from services typically provided by datacenters. NFV makes network functions a first-class citizen, and therefore holds strong promise for both enterprise datacenters and more complex network services (e.g., IMS). However, for a Cloud Service Provider to offer such services, many research questions still need to be addressed: e.g., in order to scale up/down resources to satisfy traffic demands and guarantee QoS, when and where should new virtual network functions be instantiated? How can network configuration be updated on-demand to guarantee service chaining, especially in the events of virtual network function creation and deletion? To enable on-demand management of the datacenter network, traditional Cloud Computing Management must be rethought. In this paper, we introduce the concept of a Network Function Center (NFC): we discuss the expected functionality, the meaning of management in this new context, and present a prototype system that uses genetic algorithms to dynamically distribute server and network resources.' as abstract.

there is a document named 'doc-2860' that
  has 'We present a decision-theory based approach for efficiently sampling information sources in resource-constrained environments, where there is uncertainty regarding source trustworthiness. We exploit diversity among sources to stratify the population into homogeneous subgroups to both minimise redundant sampling and mitigate the effect of certain biases (e.g., source collusion). After presenting our formal framework, we show empirically that our approach performs as well as existing truth discovery approaches with respect to accuracy, while significantly reducing sampling cost.' as abstract.

there is a document named 'doc-2861' that
  has 'In intelligence analysis we seek to make sense of information that is often conflicting or incomplete, and weigh up competing hypotheses that may explain a situation. This imposes a high cognitive load on analysts, and there are few automated tools to aid them in their task. In this paper, we present an agent-based tool to help analysts in acquiring, evaluating and interpreting information in collaboration with others. Agents assist analysts in reasoning with different types of evidence to identify what happened and why, what is credible, and how to obtain further evidence. Argument schemes, as defeasible patterns of reasoning, lie at the heart of the tool, and sensemaking agents assist analysts in structuring evidence and identifying plausible hypotheses. A crowdsourcing agent is used to reason about structured information explicitly obtained from groups of contributors, and provenance is used to assess the credibility of hypotheses based on the origin of the supporting information.' as abstract.

there is a document named 'doc-2862' that
  has 'Access control has been applied in various scenarios in the past for negotiating the best policy. Solutions with XACML for access control has been very well explored by research and have resulted in significant contributions to various sectors including healthcare. In controlling access to the sensitive data such as medical records, it is important to guarantee that the data is accessed by the right person for the right reason. Location of access requestor can be a good indication for his/her eligibility and reasons for accessing the data. To reason with geospatial information for access control, Geospatial XACML (eXtensible Access Control Markup Language) is proposed as a standard. However, there is no available implementation and architecture for reasoning with Geospatial XACML policies. This paper propose to extend XACML with geohashing to implement geospatial policies. It also proposes an architecture for checking reliability of the geospatial information provided by clients. With a case study, we demonstrate how our framework can be used to control the privacy and data access of health service data in handheld devices.' as abstract.

there is a document named 'doc-2863' that
  has 'This paper presents a new compiler and architecture called GhostRider for supporting privacy preserving computation in the cloud. The architecture employs Oblivious RAM (ORAM) which our compiler uses to emit programs that satisfy memory-trace obliviousness (MTO): Even an adversary that observes memory, bus traffic, and access times while the program executes can learn nothing about the program\'s sensitive inputs and outputs. A naive way to achieve MTO is to allocate all code and data in a single ORAM, and disable caches or fix the rate of memory traffic to avoid timing channels. GhostRider\'s compiler instead allocates data to non-oblivious, encrypted RAM (ERAM) and employs a scratchpad when the memory trace, and timing differences, are not a source of leaks, respectively. The compiler can also allocate to multiple ORAM banks when safe, which sometimes significantly reduces access times. We have formalized our approach and proved it enjoys MTO. Our FPGA based hardware prototype and simulation results show that GhostRider significantly outperforms the naive strategy.' as abstract.

there is a document named 'doc-2866' that
  has 'The combination of software-as-a-service and the increasing use of mobile devices gives rise to a considerable difference in computational power between servers and clients. Thus, there is a desire for clients to outsource the evaluation of complex functions to an external server. Servers providing such a service may be rewarded per computation, and as such have an incentive to cheat by returning garbage rather than devoting resources and time to compute a valid result. In this work, we introduce the notion of Revocable Publicly Verifiable Computation (RPVC), where a cheating server is revoked and may not perform future computations (thus incurring a financial penalty). We introduce a Key Distribution Center (KDC) to efficiently handle the generation and distribution of the keys required to support RPVC. The KDC is an authority over entities in the system and enables revocation. We also introduce a notion of blind verification such that results are verifiable (and hence servers can be rewarded or punished) without learning the value. We present a rigorous definitional framework, define a number of new security models and present a construction of such a scheme built upon Key-Policy Attribute-based Encryption.' as abstract.

there is a document named 'doc-2872' that
  has 'Data outsourcing offers cost-effective computing power to manage massive data streams and reliable access to data. Data owners can forward their data to clouds, and the clouds provide data mirroring, backup, and online access services to end users. However, outsourcing data to untrusted clouds requires data authenticity and query integrity to remain in the control of the data owners and users. In this paper, we address the authenticated data-outsourcing problem specifically for multi-version key-value data that is subject to continuous updates under the constraints of data integrity, data authenticity, and "freshness" (i.e., ensuring that the value returned for a key is the latest version). We detail this problem and propose INCBM-TREE, a novel construct delivering freshness and authenticity. Compared to existing work, we provide a solution that offers (i) lightweight signing and verification on massive data update streams for data owners and users (e.g., allowing for small memory footprint and CPU usage for a low-budget IT department), (ii) immediate authentication of data freshness, (iii) support of authentication in the presence of both real-time and historical data accesses. Extensive benchmark evaluations demonstrate that INCBM-TREE achieves higher throughput (in an order of magnitude) for data stream authentication than existing work. For data owners and end users that have limited computing power, INCBM-TREE can be a practical solution to authenticate the freshness of outsourced data while reaping the benefits of broadly available cloud services.' as abstract.

there is a document named 'doc-2889' that
  has 'Network tomography aims to infer the individual performance of networked elements (e.g., links) using aggregate measurements on end-to-end paths. Previous work on network tomography focuses primarily on developing estimators using the given measurements, while the design of measurements is often neglected. We fill this gap by proposing a framework to design probing experiments with focus on probe allocation, and applying it to two concrete problems: packet loss tomography and packet delay variation (PDV) tomography. Based on the Fisher Information Matrix (FIM), we design the distribution of probes across paths to maximize the best accuracy of unbiased estimators, asymptotically achievable by the maximum likelihood estimator. We consider two widely-adopted objective functions: determinant of the inverse FIM (D-optimality) and trace of the inverse FIM (A-optimality). We also extend the A-optimal criterion to incorporate heterogeneity in link weights. Under certain conditions on the FIM, satisfied by both loss and PDV tomography, we derive explicit expressions for both objective functions. When the number of probing paths equals the number of links, these lead to closed-form solutions for the optimal design; when there are more paths, we develop a heuristic to select a subset of paths and optimally allocate probes within the subset. Observing the dependency of the optimal design on unknown parameters, we further propose an algorithm that iteratively updates the design based on parameter estimates, which converges to the design based on true parameters as the number of probes increases. Using packet-level simulations on real datasets, we verify that the proposed design effectively reduces estimation error compared with the common approach of uniformly distributing probes.' as abstract.

there is a document named 'doc-2891' that
  has 'Publicly Verifiable Outsourced Computation (PVC) allows devices with restricted resources to delegate expensive computations to more powerful external servers, and to verify the correctness of results. Whilst highlybeneficial in many situations, this increases the visibility and availability of potentially sensitive data, so we may wish to limit the sets of entities that can view input data and results. Additionally, it is highly unlikely that all users have identical and uncontrolled access to all functionality within an organization. Thus there is a need for access control mechanisms in PVC environments. In this work, we define a new framework for Publicly Verifiable Outsourced Computation with Access Control (PVC-AC). We formally define algorithms to provide different PVC functionality for each entity within a large outsourced computation environment, and discuss the forms of access control policies that are applicable, and necessary, in such environments, as well as formally modelling the resulting security properties. Finally, we give an example instantiation that (in a black-box and generic fashion) combines existing PVC schemes with symmetric Key Assignment Schemes to cryptographically enforce the policies of interest.' as abstract.

there is a document named 'doc-2893' that
  has 'Choosing a hard-to-guess secret is a prerequisite in many security applications. Whether it is a password for user authentication or a secret key for a cryptographic primitive, picking it requires the user to trade-off costs for resistance against an adversary: a simple password is easier to remember but is also easier to guess; likewise, a shorter cryptographic key may require fewer computational and storage resources but it also is easier to attack. It is therefore crucial for the security of computer systems that designers and users understand how one can (or should) resolve this trade-off. In this paper we propose a game-theoretic framework to tackle this problem, and analyze several models applicable to a wide range of secret picking vs. secret guessing scenarios. We characterize the trade-off policies for choosing a secret that, in the face of a strategic reaction of adversaries, are optimal with respect to a number of different measures. Namely, we derive the mutually-optimal decisions as Nash Equilibria, the strategically pessimistic decisions as Maximin, and optimal randomizations to commit to as Stackelberg Equilibria of the game. Our analyses show the crucial role that the nature of the adversaries play in designing of secret-picking plans, specifically, whether they are limited in their number of tries or are ruled by the cost of making individual guesses. When the adversaries are faced with a capped number of guesses, we establish that the optimal trade-off policy of a user is in general still a uniform randomization, but over a subset of the secret domain. On the other hand, when the attacker strategy is ruled by the cost of making individual guesses, we show how Nash Equilibria may completely fail to provide the user with any level of security, signifying the vital role of credible commitment for such cases. We illustrate our results using numerical examples based on realworld samples and discuss some policy implications of our work.' as abstract.

there is a document named 'doc-2899' that
  has 'We describe the results of an experiment applying human-information interaction technologies to support situation understanding in the context of the NATO Summit in Newport, South Wales, September 2014. The experiment centered on the application of (1) controlled natural language (CNL) for knowledge representation and reasoning, and (2) a conversational natural language interface for information capture, querying, and sense-making. The main research question addressed was whether these technologies assist subject-matter experts - social scientists with expertise in community policing - to make sense of a complex, evolving situation in real time, drawing on open source intelligence including social media.' as abstract.

there is a document named 'doc-2900' that
  has 'Restricting the spread of sensitive information is important in domains ranging from commerce to military operations. In this position paper, we propose research aimed at exploring techniques for privacy enforcement when humans are the recipient of - possibly obfuscated - information. Such obfuscation could be considered to be a white lie, and we argue that determining what information to share and whether it should be obfuscated must be done through controlled query evaluation, which depends on each agent\'s risk/benefit evaluation. We concentrate on machine-human interactions, and note that appropriate specific natural language interfaces need to be developed to handle obfuscation. We propose a solution for creating controlled query evaluation mechanisms based on robust approaches for data representation under uncertainty, viz. SDL-Lite, and propose using ITA Controlled English for natural language generation so as to handle subjectivity. We present the general architecture for our approach, with a focus on the relationship between formal ontology, controlled query evaluation, and natural language.' as abstract.

there is a document named 'doc-2901' that
  has 'Principal Agent Theory (PAT) seeks to identify incentives and sanctions that a consumer should apply when entering into a contract with a provider in order to maximise their own utility. However, identifying suitable contracts - maximising utility while minimising regret - is difficult, particularly when little information is available about provider competencies. In this paper we show that a global contract can be used to govern such interactions, derived from the properties of a representative agent. After describing how such a contract can be obtained, we analyse the contract utility space and its properties. Then, we show how this contract can be used to address the cold start problem and that it significantly outperforms other approaches. Finally, we discuss how our work can be integrated with existing research into multi-agent systems.' as abstract.

there is a document named 'doc-2903' that
  has 'We propose a new approach to integrating probabilities and argumentation, based on Markov Random Fields, and building on the connection between conditional independence and the labelling status of arguments. Such a system utilizes con- flicting patterns in the knowledge which is ignored by Markov Logic Networks, where only consistent boolean interpretations can be ascribed non-zero probability. Using argumentation as a theory underpinning non-monotonic reasoning, our approach provides a principled technique for the merger of probabilistic graphical models and non-monotonic logics, linking the latter\'s semantics (via stable, preferred and grounded extensions) and inference in Markov Random Fields.' as abstract.

there is a document named 'doc-2904' that
  has 'We study the dynamic service migration problem in mobile edge-clouds that host cloud-based services at the network edge. This offers the benefits of reduction in network overhead and latency but requires service migrations as user locations change. It is challenging to make these decisions in an optimal manner because of the uncertainty in node mobility as well as possible non-linearity of the migration and transmission costs. In this paper, we formulate a sequential decision making problem for service migration using the framework of Markov Decision Process (MDP). Our formulation captures general cost models and provides a mathematical framework to design optimal service migration policies. In order to overcome the complexity associated with computing the optimal policy, we approximate the underlying state space by the distance between the user and service locations. We show that the resulting MDP is exact for uniform one-dimensional mobility while it provides a close approximation for uniform two-dimensional mobility with a constant additive error term. We also propose a new algorithm and a numerical technique for computing the optimal solution which is significantly faster in computation than traditional methods based on value or policy iteration. We illustrate the effectiveness of our approach by simulation using real-world mobility traces of taxis in San Francisco.' as abstract.

there is a document named 'doc-2906' that
  has 'Network monitoring is an essential component of network operation and, as the network size increases, it usually generates a significant overhead in large scale networks such as sensor and data center networks. In this paper, we show that measurement correlation often exhibited in real networks can be successfully exploited to reduce the network monitoring overhead. In particular, we propose an online adaptive measurement technique with which a subset of nodes are dynamically chosen as monitors while the measurements of the remaining nodes are estimated using the computed correlations. We propose an estimation framework based on jointly Gaussian distributed random variables, and formulate an optimization problem to select the monitors which minimize the estimation error under a total cost constraint. We show that the problem is NP-Hard and propose three efficient heuristics. In order to apply our framework to real-world networks, in which measurement distribution and correlation may significantly change over time, we also develop a learning based approach that automatically switches between learning and estimation phases using a change detection algorithm. Simulations carried out on two real traces from sensor networks and data centers show that our algorithms outperforms previous solutions based on compressed sensing and it is able to reduce the monitoring overhead by 50% while incurring a low estimation error. The results further demonstrate that applying the change detection algorithm reduces the estimation error up to two orders of magnitude.' as abstract.

there is a document named 'doc-2909' that
  has 'In modern military operations the emphasis is on smaller teams and more ad hoc teamwork. This requires greater agility both in terms of capturing actionable intelligence as well as appropriate dissemination and fusion of that information to coalition team members based on their tasks and need to know. In previous research and development we have explored the potential for a controlled natural language (CNL), acting as the sole knowledge representation language to facilitate cooperative working among human and machine agents. In this paper we envision a "living database" to support situation understanding, enable intelligence reporting, fusion and dissemination based on context. The human and machine users of the system have the ability to add, remove, or edit existing information using the CNL, including extensions to the model (or schema) in real-time. In this paper we describe various examples undertaken using this approach along with initial experiments using a conversational software agent to enable the field user to interact with such information in full natural English language.' as abstract.

there is a document named 'doc-2910' that
  has 'In order to support the use of cognitive architectures in computer simulation studies involving virtual environments, an integration solution is proposed that enables ACT-R cognitive models to communicate with the Unity game engine. The integration solution is tested using virtual robots that are able to move around a 3D virtual environment and interact with various objects. The robots are equipped with visual, tactile and proprioceptive sensor systems, which yield information about the current sensory state of the robot. This information is communicated to an ACT-R model that subsequently controls the behaviour of the robot by making decisions about motor output. The performance of the model and the integrity of the integration solution are evaluated using a task that requires a virtual robot to retrieve target objects and deposit them in a specified goal location.' as abstract.

there is a document named 'doc-2912' that
  has 'We introduce an experiment designed to study trade-offs in collaborative decision making environments such as the ability to accumulate information and its impact on the fluctuation of decisions. Two models of the experiment are presented: a cognitive model using the ACT-R cognitive architecture and a probabilistic argumentation model using Markov Random Fields. Representative results from the experiment are presented and compared to the results of the two models. Implications of the results and avenues for future work are discussed.' as abstract.

there is a document named 'doc-2915a' that
  has 'Consider the problem of reliable multicast over a network in the presence of adversarial errors. In contrast to traditional network error correction codes designed for a given network capacity and a given number of errors, we study an arguably more realistic setting that prior knowledge on the network and adversary parameters is not available. For this setting we propose efficient and throughput-optimal error correction schemes, provided that the source and terminals share randomness that is secret form the adversary. We discuss an application of cryptographic pseudorandom generators to efficiently produce the secret randomness, provided that a short key is shared between the source and terminals. Finally we present a secure key distribution scheme for our network setting.' as abstract.

there is a document named 'doc-2915b' that
  has 'We discuss crowdsourcing as a means to evaluate the benefits of employing argument schemes for making sense of information in intelligence analysis. In this position paper, we propose two sets of crowdsourcing experiments to measure the effectiveness of argument schemes in forming valid arguments, and critical questions to prevent cognitive biases.' as abstract.

there is a document named 'doc-805' that
  has 'The application of complex network theory to communication systems has led to several important results. Nonetheless, previous research has often neglected to take into account their temporal properties, which in many real scenarios play a pivotal role. Mainly because of mobility, transmission delays or protocol design, a communication network should not be considered only as a static entity. At the same time, network robustness has come extensively under scrutiny. Understanding whether networked systems can undergo structural damage and yet perform efficiently is crucial to both their protection against failures and to the design of new applications. In spite of this, it is still unclear what type of resilience we may expect in a network that continuously changes over time. In this work we present the first attempt to define the concept of temporal network robustness: we describe a measure of network robustness for time-varying networks and we show how it performs on different classes of random models by means of analytical and numerical evaluation. Particularly, we show how static approximation can wrongly indicate high robustness of fragile networks when adopted in mobile time-varying networks, while a temporal approach captures more accurately the system performance.' as abstract.

there is a document named 'doc-806a' that
  has 'In this paper, we propose a novel approach that combines logic programming with ontological reasoning. The proposed approach enables the use of ontological terms directly within logic programs. We demonstrate the usefulness of the proposed approach using a case-study of sensor-task matchmaking.' as abstract.

there is a document named 'doc-806b' that
  has 'In this paper, we propose Ontological Logic Programming (OLP), a novel approach that combines logic programming with ontological reasoning. The proposed approach enables the use of ontological terms (i.e., individuals, classes and properties) directly within logic programs. The interpretation of these terms are delegated to an ontology reasoner during the interpretation of the program. Unlike similar approaches, OLP makes use of the full capacity of both the ontological reasoning and logic programming. We evaluate the computational properties of OLP in different settings and show that its performance can be significantly improved using caching mechanisms. Furthermore, using a case-study, we demonstrate the usefulness of OLP in real-life settings.' as abstract.

there is a document named 'doc-806c' that
  has 'This is OWL ontology file for the ISTAR ontology' as abstract.

there is a document named 'doc-807a' that
  has 'In this paper, we present a software assistant agent that can proactively manage information on behalf of cognitively overloaded users. We develop an agent architecture, known here as ANTicipatory Information and Planning Agent (ANTIPA), for recognizing user plan in an unobtrusive manner and reasoning about time constraints to provide relevant information at a right time. ANTIPA integrates probabilistic plan recognition with constraint-based information gathering. This paper focuses on our probabilistic plan prediction algorithm inspired by a decision theory that human decision making is based on long-term outcomes. A proof of concept user study shows a promising result.' as abstract.

there is a document named 'doc-808' that
  has 'Trust and reputation are vital concepts in multi-agent systems where diverse, self-interested agents interact in pursuit of their own goals. While much work to date has focussed on enabling agents to form beliefs about the trustworthiness of potential delegation partners, the problem of deciding to trust on the basis of such beliefs has received relatively little attention. Without a model of trust decision, agents cannot take into account the risks involved in delegation. For example, different candidates may incur different costs, and tasks may offer varying payoffs in case of success or failure. In this paper, we present a decision theoretic model of trust decision which employs probabilistic trust beliefs within a Principal-Agent framework. We show how both agents in a two-party delegation relationship can make use of trust and reputation, as well as implicit reputational incentives, to make decisions about whom (and indeed whether) to trust.' as abstract.

there is a document named 'doc-809' that
  has 'Activity and plan recognition are well-developed research fields, where much work has been done. There has been, however, almost no effort in combining both to create intelligent software that can effectively support the decision-making process faced by human teams enacting a joint plan. In this paper, we present an architecture to integrate activity recognition and plan recognition using a team-oriented, incremental and distributed approach. In this way intelligent software assistants will be able to interact effectively in human-agent teams, and offer helpful and meaningful support to humans during plan enactment.' as abstract.

there is a document named 'doc-811' that
  has 'Pseudo-random Channel Hopping (PCH) enables nodes to avoid jamming attacks by hopping over multiple channels. However, to share hopping sequence information among nodes, PCH requires a pre-key establishment process, which takes considerable time. To address the issue, we propose a KeyLess Channel Hopping (KLCH) scheme that initiates communication between nodes randomly hopping over multiple channels without pre-key while guaranteeing rendezvous using a quorum system. We verify performance of KLCH via simulation study.' as abstract.

there is a document named 'doc-812' that
  has 'Mobility models have traditionally been tailored to specific application domains such as human, military, or ad hoc transportation scenarios. This tailored approach often renders a mobility model useless when the application domain changes. Furthermore, the failure to adapt the mobility model to accurately match the new domain naturally leads to wrong conclusions about the performance of protocols and applications running atop. In this paper, we propose a mobility modeling framework based on the observation that the mobility characteristics of most mobility-based applications can be captured in terms of a few fundamental factors: (1) Targets; (2) Obstacles; (3) Dynamic Events; (4) Navigation; (5) Steering behaviors; and (6) Dynamic Behaviors. We have designed and implemented a Universal Mobility Modeling Framework (UMMF), which enables the instantiation of a mobility model from a wide universe of possibilities defined by the aforementioned factors. We describe the mapping from application-domain-specifics to UMMF elements, demonstrating the power and flexibility of our approach by capturing representative mobility models with good accuracy in terms of a large number of topological metrics. We also describe several specific mobility scenarios and their UMMFbased model representations.' as abstract.

there is a document named 'doc-814a' that
  has 'Policies are declarations of constraints on the behaviour of components within distributed systems, and are often used to capture norms within agent-based systems. A few machine-processable representations for policies have been proposed, but they tend to be either limited in the types of policies that can be expressed or limited by the complexity of associated reasoning mechanisms. In this paper, we argue for a language that sufficiently expresses the types of policies essential in practical systems, and which enables both policy-governed decisionmaking and policy analysis within the bounds of decidability. We then propose an OWL-based representation of policies that meets these criteria using and a reasoning mechanism that uses a novel combination of ontology consistency checking and query answering. In this way, agent-based systems can be developed that operate flexibly and effectively in policy-constrainted environments.' as abstract.

there is a document named 'doc-815' that
  has 'We investigate the use of biologically inspired routing heuristics in the field of inter-domain routing in sensor networks. Instead of relying on classical topology control techniques for routing in sensor networks, the use of geographical coordinates has been investigated for self-organized and fully distributed message forwarding. However, the identification of the nodes\' positions is either expensive in terms of necessary equipment or message exchange. Therefore, the use of virtual coordinates has been investigated in this domain. The key advantage is that these virtual identifiers can also be used for data management similar as in a Distributed Hash Table (DHT). It is, however, extremely challenging to provide routing functionality between multiple independent networks or network domains. In previous work, we developed the Virtual Cord Protocol (VCP) that provides all the means for creating and maintaining such virtual identifiers and that is even able to route between neighboring network domains. This paper extends VCP by providing a generalized inter-domain routing framework using Ant Colony Optimization (ACO) for optimizing routes between multiple network domains. In extensive simulations, we evaluated this routing bio-inspired heuristic. The obtained results clearly demonstrate that ACO is very efficient even in highly mobile scenarios.' as abstract.

there is a document named 'doc-816' that
  has 'Researches about multi-radio mesh networks have mostly focused on channel allocation under internal interference. However, the deployment of WMNs in unlicensed bands of dense urban areas imposes many challenges regarding co-existence with residential access points. In this paper, we propose Urban-X, which is a first attempt towards a new architecture for MultiRadio Cognitive Mesh Networks. We develop a novel channel assignment that reflects channel and residential traffic state of external users to maximize network throughput. We evaluate our approach using an enhancement of the ns-2 simulator. Urban-X demonstrate the feasibility of our approach and show robustness to variation of channel environment and external user traffic.' as abstract.

there is a document named 'doc-817' that
  has 'A wireless mesh network (WMN) has been popularly researched as a wireless backbone for Internet access with off-the-shelf and inexpensive equipments. Nowadays, several applications like contents sharing, multicast video streaming, vehicular networks encourage to build mesh networks in urban areas. However, the deployment of WMNs in unlicensed bands of dense urban areas imposes many challenges. While previous research has mostly focused on optimal channel allocation under inter or intra-flow interferences within mesh nodes, the practical deployment of WMNs also requires to consider the interference caused by external entities such as residential access points that do not belong to the WMN. To address this issue, we propose Urban-X which is a first attempt towards a new architecture for Multi-Radio Cognitive Mesh Networks. Based on a crosslayer scheme, we develop novel routing and forwarding schemes reflecting the residential traffic state of external users. Through an extensive simulation analysis using the ns-2 simulator, we demonstrate the feasibility of our routing scheme and we show its robustness to the variations of channel environment and external traffics.' as abstract.

there is a document named 'doc-819' that
  has 'Reliable and secure content distribution in a disruptive environment is a critical challenge due to high mobile and lossy channels. Traditional IP networking and wireless protocols tend to perform poorly. In this paper, we propose Content Centric Networking (CCN) for emergency wireless ad hoc environments. CCN is a novel communication architecture capable to access and retrieve content by name. This new approach achieves scalability, security, and efficient network resource management in large scale disaster recovery and battlefield networks. Existing Internet CCN schemes cannot be directly applied to wireless mobile ad hoc networks due to different environments and specific limitations. Thus, we must extend the CCN architecture by introducing features and requirements especially designed for disruptive networks. We prove feasibility and performance gain of the new design via implementation and experimentation.' as abstract.

there is a document named 'doc-820a' that
  has 'Abductive inference has many known applications in multiagent systems including planning, scheduling, policy analysis and sensing data interpretation. However, most abductive frameworks rely on a centrally executed proof procedure whereas many of the application problems are distributed by nature. Confidentiality and communication overhead concerns often preclude agents\' knowledge from being centralised. We present in this paper a distributed abductive reasoning framework with a flexible and extensible proof procedure that permits collaborative abductive reasoning between agents over decentralised knowledge. The proof procedure is sound and complete upon termination, and can perform concurrent computation. To the best of our knowledge, this is the first distributed abductive system that can compute non-ground conditional proofs and handle arithmetic constraints.' as abstract.

there is a document named 'doc-820b' that
  has 'Abductive inference has many known applications in multiagent systems including planning, scheduling, policy analysis and sensing data interpretation. However, most abductive frameworks rely on a centrally executed proof procedure whereas many of the application problems are distributed by nature. Confidentiality and communication overhead concerns often preclude agents\' knowledge from being centralised. We present in this paper a distributed abductive reasoning framework with a flexible and extensible proof procedure that permits collaborative abductive reasoning between agents over decentralised knowledge. The proof procedure is sound and complete upon termination, and can perform concurrent computation. To the best of our knowledge, this is the first distributed abductive system that can compute non-ground conditional proofs and handle arithmetic constraints.' as abstract.

there is a document named 'doc-820c' that
  has 'Abductive inference has many known applications in multiagent systems including planning, scheduling, policy analysis and sensing data interpretation. However, most abductive frameworks rely on a centrally executed proof procedure whereas many of the application problems are distributed by nature. Confidentiality and communication overhead concerns often preclude agents\' knowledge from being centralised. We present in this paper a distributed abductive reasoning framework with a flexible and extensible proof procedure that permits collaborative abductive reasoning between agents over decentralised knowledge. The proof procedure is sound and complete upon termination, and can perform concurrent computation. To the best of our knowledge, this is the first distributed abductive system that can compute non-ground conditional proofs and handle arithmetic constraints.' as abstract.

there is a document named 'doc-821' that
  has 'The allocation of resources to tasks in an efficient manner is a key problem in computer science. One important application domain for solutions to this class of problem is the allocation of sensor resources for environmental monitoring, surveillance, or similar sensing tasks. In real-world problem domains, the problem is compounded by the fact that the number of tasks and resources change over time, the number of available resources is limited and tasks compete for resources. Thus, it is necessary for a practical allocation mechanism to have the flexibility to cope with dynamic environments, and to ensure that unfair advantages are not given to a subset of the tasks (say, because they arrived first). Typical contemporary approaches use agents to manage individual resources, and the allocation problem is modelled as a coordination problem. In existing approaches, however, the successful allocation of resources to a new task is strongly dependent upon the allocation of resources to existing tasks. In this paper we propose a novel negotiation mechanism for exchanging resources to accommodate the arrival of new tasks, dynamically re-arranging the resource allocation. We have shown, via a set of experiments, that our approach offers significantly better results when compared with an agentbased approach without resource re-allocation through concurrent negotiation.' as abstract.

there is a document named 'doc-822' that
  has 'Sensing resources play a crucial role in the success of critical tasks such as surveillance. Therefore, it is important to assigning appropriate sensing resources to tasks such that the selected resources fully cater the needs of the tasks. However, selecting the right resources to tasks is a computationally hard problem to solve. Most of the existing approaches address the efficiency aspect of the resource selection by considering the physical aspects of the sensor network (e.g., range, power, etc.) but have ignored important domain related properties such as capabilities of assets, environmental conditions, policies and so on which makes the selection effective. In this paper we present a knowledge rich mechanism to intelligently select resources for tasks such that the selected resources sufficiently cover the needs of the tasks. Ontologies are used to capture the crucial domain knowledge and semantic matchmaking is used to perform sensor-task matching. A combination of ontological and first-order-logic reasoning is considered for the solution architecture.' as abstract.

there is a document named 'doc-849' that
  has 'The immensity and variety of personal information (e.g., profile, photo, and microblog) on social sites require access control policies tailored to individuals\' privacy needs. Today such policies are still mainly specified manually by ordinary users, which is usually coarse-grained, tedious, and error-prone. This paper1 presents the design, implementation, and evaluation of an automated access control policy specification tool, XACCESS, that helps non-expert users effectively specify who should have access to which part of their data. A series of key features distinguish XACCESS from prior work: 1) it adopts a role-based access control model (instead of the conventional rule-based paradigm [10]) to capture the implicit privacy preference of social site users; 2) it employs a novel hybrid mining method to extract a set of semantically interpretable, functional "social roles", from static network structures and dynamic historical activities; 3) based on the identified social roles, confidentiality setting of personal data, and (optional and possibly inconsistent) predefined user-permission assignments, it recommends a set of high-quality privacy settings; 4) it allows user feedback in every phase of the process to further improve the quality of the suggested privacy policies. A comprehensive experimental evaluation is conducted over real social network data to validate the efficacy of XACCESS.' as abstract.

there is a document named 'doc-915' that
  has 'We investigate the scalability of a class of algorithms that exploit the dynamics of wireless fading channels to achieve secret communication in a large wireless network of n randomly located nodes. We describe a construction in which nodes transmit artificial noise to suppress eavesdroppers whose locations are unknown and ensure secrecy of messages transported across the network. Under a model in which eavesdroppers operate independently and under appropriate conditions on the achievable per-node throughput _(n), we show that the network can tolerate _ _" _ 1 n _(n) "2c _ eavesdroppers while ensuring that the aggregate rate at which eavesdroppers intercept packets goes to 0, where c is a constant such that 0 < c < 1. The result clearly establishes a trade-off between the achievable throughput and the allowable number of eavesdroppers. Under a collaborating eavesdropper model and a similar constraint on the eavesdropper throughput, we show that the network can tolerate a single eavesdropper with _ _" ln " _ 1 n _(n) ""1__ _ antennas, __ > 0. We also establish sufficient conditions on the number of eavesdroppers to achieve a non-zero throughput in our construction.' as abstract.

there is a document named 'doc-916' that
  has 'The secure transmission of information in wireless networks without knowledge of the eavesdroppers\' locations is considered. Two key mechanisms are employed: artificial noise generation from system nodes other than the transmitter and receiver, and a form of multi-user diversity that allows message reception in the presence of the artificial noise. We consider the maximum number of independently-operating and uniformly distributed eavesdroppers that can be present while the desired secrecy is achieved with high probability in the limit of a large number of system nodes. While our main motivation is considering eavesdroppers of unknown location, we first consider the case where the path-loss is identical between all pairs of nodes. In this case, a number of eavesdroppers that is exponential in the number of systems nodes can be tolerated. In the case of uniformly distributed eavesdroppers of unknown location, any number of eavesdroppers whose growth is sub-linear in the number of system nodes can be tolerated. The proposed approach significantly outperforms a power control approach based on standard multi-user diversity' as abstract.

there is a document named 'doc-917' that
  has 'Jamming attacks have been recently studied as wireless security threats disrupting reliable RF communication in a wireless network. Existing countermeasures often make use of spread-spectrum techniques such as Frequency Hopping Spread Spectrum and Direct Sequence Spread Spectrum [3]. Communication parties rely on a pre-shared common key (hopping sequence or spread code) which is unknown to the jammer, thus making the system robust against jamming attacks. This, however, raises another issue: namely establishing the initial secure key pairing under jamming attacks. This demands another secure communication channel and this introduces circular dependency.' as abstract.

there is a document named 'doc-918' that
  has 'In tactical scenarios, different organizations form their own networks and use different protocols based on their specific objectives. This increases network diversity and makes interoperation more intricate. However, the interoperability of multiple tactical MANETs is an important issue when the MANETs are engaged in a common mission. In order to handle the diversity and interconnect different protocols we leverage a special type of node called gateway. The Gateway translates protocols and formats. It bridges any technology gaps and transparently interconnects multiple MANETs. Only a subset of the nodes qualifies for the Gateway role. Yet, it is not costeffective to use them all simultaneously as gateways because they would generate excessive network overhead and might deplete the already scarce wireless network resources. If we pre-assign a subset of nodes as gateways, we may not be able to handle mobile topology changes. Moreover, we may not be able to satisfy in a cost-effective manner the evolving requirements of the mission. Thus, we need to dynamically elect gateways as network topology and mission requirements change. To this end, the election must be aware of the policy requirements of MANETs and mission. In this paper, we design and implement an election algorithm that can account for various metrics (e.g., network connectivity, energy balancing, secure routing, etc.). Simulation results show that the tuning of the election algorithm parameters plays an important role in the performance under a specific metric.' as abstract.

there is a document named 'doc-919' that
  has 'Inter-domain routing for MANETs (Mobile Ad Hoc Networks) draws increasing attention because of military and vehicular applications. Most of the proposed inter-domain routing protocols are using clustering to manage the connection between intra-domain nodes and inter-domain nodes. The distributed clustering algorithm elects within each domain gateways that connect to different domains. Also many inter-domain routing protocols assume that all nodes are equipped with GPS devices, which is common for nodes operating in military and vehicular environments. The geographical information can be used to assist the routing protocol. In this paper, we propose I-GIDR (improved geographical inter-domain routing protocol) which has several techniques to improve geo-based inter-domain routing protocol for MANETs. One key innovation is a new proposed gateway election algorithm that accounts for neighbor\'s number and distribution to select a gateway that can adapt to multiple domain scenarios. Another key innovation of this paper is neighbor priority, a concept based on the geographical distribution and density of neighbors. Using neighbor priority, we can optimize the geo-based routing protocol, and reduce control overhead. Simulation experiments show significant improvements in connectivity, Packet Delivery Ratio and Control Overhead.' as abstract.

there is a document named 'doc-920' that
  has 'The advancements of diverse radio technologies and emerging applications have spawned increasing heterogeneity in mobile ad hoc networks (MANETs). But the collaborative nature of communications and operations often requires that these heterogeneous MANETs to be interoperable. Nonetheless, the existing interconnection protocols designed for the Internet (namely inter-domain routing protocol such as BGP) are not adequate for handling the unique challenges in MANETs. In this paper, we present a novel Inter-MANET Routing protocol called InterMR that can handle the heterogeneity and dynamics of MANETs. Our first contribution is an Inter-MANET address scheme based on a variety of node attributes (e.g., symbolic name, property, etc.); this allows dynamic merging/split of network topologies without a separate Name Server. Our second contribution is to provide a seamless routing mechanism across heterogeneous MANETs without modifying the internal routing mechanisms in each MANET. The proposed scheme can transparently adapt to topological changes due to node mobility in MANETs by dynamically assigning the gateway functionalities. We show, by packet-level simulation, that the performance of InterMR can be improved by up to 112% by adaptive assigning gateway functionalities. We also show that InterMR is scalable with only modest overhead by analysis.' as abstract.

there is a document named 'doc-921' that
  has 'In this paper, we propose a novel packet forwarding scheme based on network coding that is resilient to jamming attack in a tactical area. Wireless communication is necessary in a battlefield, but it is fragile to jamming attacks from an adversary because of the wireless shared medium. Jamming attack is easily achieved by emitting continuous radio signals and it can interfere with other radio communications. Channel switching over multiple channels or route detouring have been proposed to restore communication from jamming attacks, but they require a special radio system or knowledge of the network topology. Our new scheme exploits packet redundancy of network coding. It dynamically changes the level of redundancy adapting to local jamming conditions and thus injects redundant encoded packets when and where a jamming attack occurs. In absence of jamming, it decreases forwarding rate to save resources so that our protocol efficiently manages the network resources. We provide performance evaluations of resiliency and efficiency of the new scheme via simulation study.' as abstract.

there is a document named 'doc-924' that
  has 'The design of the Internet protocol stack, with IP at the waist of the hourglass, mandates that packet delivery is governed by the destination IP address. This design has enabled explosive growth of the wired Internet, but faces two basic issues when applied to mobile environments. First, many mobile networks, such as mobile ad-hoc networks, are infrastructure-free, while Internet protocols are generally built with infrastructure support in mind (e.g., DHCP servers to handle IP address assignments). Second, node mobility introduces a high degree of dynamics in node interconnectivity, which defeats conventional routing protocols, originally designed for wired networks. In this paper, we argue that mobile networks can be made more effective and efficient through Named Data Networking (NDN) [4] (aka CCN).' as abstract.

there is a document named 'doc-925' that
  has 'Answer sharing is a key element in multi-agent systems as it allows agents to collaborate towards achieving a global goal. However exogenous knowledge of the world can influence each agent\'s local computation, and communication channels may introduce delays, creating multiple partial answers at different times. Agent\'s answers may, therefore, be incomplete and revisable, giving rise to the concept of speculative reasoning, which provides a framework for managing multiple revisable answers within the context of multi-agent systems. This paper extends existing work on speculative reasoning by introducing a new abductive framework to hierarchical speculative reasoning. This allows speculative reasoning in the presence of both negation and constraints, enables agents to receive conditional answers and to continue their local reasoning using default answers, thus increasing the parallelism of agents collaboration. The paper describes the framework and its operational model, illustrates the main features with an example and states soundness and completeness results.' as abstract.

there is a document named 'doc-926' that
  has 'The automation of policy refinement, whilst promising great benefits for policy-based management, has hitherto received relatively little treatment in the literature, with few concrete approaches emerging. In this paper we present initial steps towards a framework for automated distributed policy refinement for both obligation and authorization policies. We present examples drawn from military scenarios, describe details of our formalism and methods for action decomposition, and discuss directions for future research.' as abstract.

there is a document named 'doc-927b' that
  has 'This paper introduces and explores the new concept of Time-Specific Encryption (TSE). In (Plain) TSE, a Time Server broadcasts a key at the beginning of each time unit, a Time Instant Key (TIK). The sender of a message can specify any time interval during the encryption process; the receiver can decrypt to recover the message only if it has a TIK that corresponds to a time in that interval. We extend Plain TSE to the public-key and identity-based settings, where receivers are additionally equipped with private keys and either public keys or identities, and where decryption now requires the use of the private key as well as an appropriate TIK. We introduce security models for the plain, public-key and identitybased settings. We also provide constructions for schemes in the different settings, showing how to obtain Plain TSE using identity-based techniques, how to combine Plain TSE with public-key and identity-based encryption schemes, and how to build schemes that are chosen-ciphertext secure from schemes that are chosen-plaintext secure. Finally, we suggest applications for our new primitive, and discuss its relationships with existing primitives, such as Timed-Release Encryption and Broadcast Encryption.' as abstract.

there is a document named 'doc-928' that
  has 'Decision makers (humans or software agents alike) are faced with the challenge of examining large volumes of information originating from heterogeneous sources with the goal of ascertaining trust in various pieces of information. In this paper we argue (using examples) that traditional trust models are limited in their data model by assuming a pair-wise numeric rating between two entities (e.g., eBay recommendations, Netflix movie rating, etc). We present a novel trust computational model for rich, complex and uncertain information encoded using Bayesian Description Logics. We present security and scalability tradeoffs that arise in the new model, and the results of an evaluation of the first prototype implementation under a variety attack scenarios.' as abstract.

there is a document named 'doc-930' that
  has 'On the Semantic Web, decision makers (humans or software agents alike) are faced with the challenge of examining large volumes of information originating from heterogeneous sources with the goal of ascertaining trust in various pieces of information. While previous work has focused on simple models for review and rating systems, we introduce a new trust model for rich, complex and uncertain information.We present the challenges raised by the new model, and the results of an evaluation of the first prototype implementation under a variety of scenarios.' as abstract.

there is a document named 'doc-931' that
  has 'In both the commercial and defense sectors a compelling need is emerging for rapid, yet secure, dissemination of information. In this paper we address the threat of information leakage that often accompanies such information flows. We focus on domains with one information source (sender) and many information sinks (recipients) where: (i) sharing is mutually beneficial for the sender and the recipients, (ii) leaking a shared information is beneficial to the recipients but undesirable to the sender, and (iii) information sharing decisions of the sender are determined using imperfect monitoring of the (un)intended information leakage by the recipients. We make two key contributions in this context: First, we formulate data leakage prevention problems as Partially Observable Markov Decision Processes; we show how to encode one sample monitoring mechanism-digital watermarking-into our model. Second, we derive optimal information sharing strategies for the sender and optimal information leakage strategies for a rational-malicious recipient as a function of the efficacy of the monitoring mechanism. We believe that our approach offers a first of a kind solution for addressing complex information sharing problems under uncertainty.' as abstract.

there is a document named 'doc-932b' that
  has 'In a masquerade attack, an adversary who has stolen a legitimate user\'s credentials attempts to impersonate him to carry out malicious actions. Automatic detection of such attacks is often undertaken constructing models of normal behaviour of each user and then measuring significant departures from them. One potential vulnerability of this approach is that anomaly detection algorithms are generally susceptible of being deceived. In this paper, we first investigate how a resourceful masquerader can successfully evade detection while still accomplishing his goals. We then propose an algorithm based on the Kullback-Leibler divergence which attempts to identify if a sufficiently anomalous attack is present within an apparently normal request. Our experimental results indicate that the proposed scheme achieves considerably better detection quality than adversarial-unaware approaches.' as abstract.

there is a document named 'doc-933' that
  has 'This paper reports ongoing work in creating a logical foundation for reasoning about trust and trustworthiness in networks of individuals that may recommend one another as trustworthy or untrustworthy. One solution is to adopt some form of voting or counting scheme as in commonly done in reputations systems [10]. But in many circumstances, when the stakes are sufficiently high, e.g. deciding to trust a root certificate or disclose confidential information, weight of numbers does not constitute a good argument. As Plato puts it ". . . good decision is based on knowledge and not on numbers".' as abstract.

there is a document named 'doc-934' that
  has 'We consider the problem of tracking multiple moving targets in a continuous field using proximity sensors, which are binary sensors that can sense target presence by performing local energy detection subject to noise. Compared with more sophisticated sensors, proximity sensors have the advantage of having lower costs and lower energy consumption, but also the disadvantage of being less accurate. In this paper, we propose a hybrid tracking scheme where a coarse-scale tracking is first performed by proximity sensors to narrow down the areas of interest, and then a fine-scale tracking is performed by high-end sensors to estimate the exact target locations, with our focus on the former. In contrast to classic multi-target tracking which assumes 1-1 association between measurements and targets, we show that proximity measurements do not have such association and thus require a different objective. Formulating the coarse-scale tracking as a problem of tracking the histograms of targets in a cell-partitioned field, we develop both an optimal and two approximate solutions via Bayesian Filtering (BF). In particular, one of our approximate solutions decouples the tracking of different targets and thus reduces the dimensionality of BF by relaxing the likelihood function, and the other further reduces the problem into discrete space by quantizing the target mobility model and the relaxed likelihood function. Together with the optimal solution, they provide flexible tradeoffs between accuracy and complexity. Simulations show that the proposed solutions can effectively track targets to the accuracy of a cell and thus reduce uncertainty for the fine-scale tracking.' as abstract.

there is a document named 'doc-935' that
  has 'Extending the lifetime of wireless sensor networks requires energy-conserving operations such as duty-cycling. However, such operations may impact the effectiveness of high- fidelity real-time sensing tasks, such as object tracking, which require high accuracy and short response times. In this paper, we quantify the influence of different duty-cycle schemes on the efficiency of bearings-only object tracking. Specifically, we use the Maximum Likelihood localization technique to analyze the accuracy limits of object location estimates under different response latencies considering variable network density and dutycycle parameters. Moreover, we study the tradeoffs between accuracy and response latency under various scenarios and motion patterns of the object. We have also investigated the effects of different duty-cycled schedules on the tracking accuracy using acoustic sensor data collected at Aberdeen Proving Ground, Maryland, by the U.S. Army Research Laboratory (ARL).' as abstract.

there is a document named 'doc-936' that
  has 'The sensor tasking problem in sensor networks involves the representation of users\' tasks (user-level tasking) in a form which a sensor network needs to perform required operations (sensor-level tasking) leading to satisfaction of the tasks. We analysed four approaches to task representation (TR) in sensor networks: the Open Geospatial Consortium\'s Sensor Web Enablement, Goal Lattices, Semantic Streams, and Sensor Assignment to Missions. Each approach considers distinct aspect of the sensor tasking problem. We used the Web Ontology Language, OWL, to define the features of each TR, which enabled us then to identify mappings between them. These mappings allow us to combine the TRs into one hybrid task representation (HTR) that addresses both userlevel and sensor-level tasking, thus providing a more complete, integrated approach to the sensor tasking problem.' as abstract.

there is a document named 'doc-937' that
  has 'We study a problem in which a single sensor is scheduled to observe sites periodically, motivated by applications in which the goal is to maintain up-to-date readings for all the observed sites. In the existing literature, it is typically assumed that the time for a sensor switching from one site to another is negligible. This may not be the case in applications such as camera surveillance of a border, however, in which the camera takes time to pan and tilt to refocus itself to a new geographical location. We formulate a problem with refocusing delay constraints. We prove the problem to be NP-hard and then study a special case in which refocusing is proportional to some Euclidian metric. We give a lower bound on the optimal cost for the scheduling problem. Finally, we provide and experimentally evaluate several heuristic algorithms, some of them based on this computed lower bound.' as abstract.

there is a document named 'doc-938' that
  has 'This paper demonstrates the feasibility and attractiveness of NUM-INP, an extension of the Network Utility Maximization (NUM) framework that considers the integration of in-network processing. The NUM-INP protocol provides for distributed convergence of source transmission rate and compression ratio to near-optimal values, while respecting user-defined prioritization through the use of mission-specific utility functions. Specifically, we present an implementation of the NUM-INP protocol, and discuss our results, in contrast with our expectation from eventbased simulation.' as abstract.

there is a document named 'doc-939' that
  has 'The challenge of ensuring that the right information is available to the right personnel at the right time and in the right format is becoming increasingly important as military forces adopt a vision of Network Centric Warfare and Operations. This is particularly true in a dynamic environment with prevalent uncertainty, where various types and sources of information must be consulted while the information requirements change during planing and re-planning. The heterogeneity in doctrine and information systems inherent to coalition operations compounds this problem, particularly from the point of view of human users that must not only plan and execute missions but also keep in mind all potentially relevant sources of information, leading to cognitive overload. We have developed the ANTicipatory Information and Planning Agent (ANTIPA) [11] to manage information adaptively in order to mitigate user cognitive overload. To this end, the agent brings information to the user as a result of user requests, but, most crucially, it proactively predicts the user\'s prospective information needs by recognizing the user\'s plan; prefetches information that is likely to be used in the future; and offers the information when it is relevant to the current or future planning decisions. In this paper we review the ANTIPA architecture and describe potential applications of ANTIPA in coalition operations.' as abstract.

there is a document named 'doc-940' that
  has 'The effect of dynamic networks on distributed problem-solving was examined using a multi-agent simulation environment. Synthetic agents were tasked with the problem of finding optimal solutions to a specific design problem, and they were allowed to communicate the results of their search efforts to other agents via a dynamicallyevolving communication network structure. The growth of the network was determined by two parameters. One parameter determined the rate at which the network structure emerged, while the second determined the point at which the first network link was formed. Together, these parameters produced a reliable effect on collective problem-solving performance. Firstly, performance was negatively affected by the rate of network growth, with faster growth rates producing poorer performance. Secondly, performance was improved by introducing longer initial delay periods into the network formation process, a manipulation which also served to attenuate the decline in performance seen with increasing network growth rates. Of particular interest, the study found that networks with dynamic, constructive topologies delivered a better profile of performance relative to networks with fixed, static topologies. The results are discussed in relation to our understanding of how military coalition communication networks may affect performance outcomes in distributed problem-solving environments.' as abstract.

there is a document named 'doc-941' that
  has 'Over the years, researchers have expended considerable effort in attempts to improve military planning, most notably via the provision of automated planning support tools. While there have been some successes (e.g. the DART system which was used for movement planning during Gulf war), planning still remains a very human-orientated activity with little technical support. Why? A possible reason for this predicament is that researchers have not fully conceptualized the problem that planners face. For instance, a common approach has been to consider planning as a single process or a homogenous set of problems to be solved. Unfortunately, military planning is instead a set of heterogeneous and interrelated activities carried out by different sets of planners working at different times and locations. In addition, these sets of activities may be conceptually quite different from each other. It is therefore proposed that military planning should be viewed more appropriately as a capability, which consists of a set of diverse activities which are collectively aimed at producing a set of coordinated plans to achieve given high-level mission objectives. This perspective, while essentially humancentered, suggests where it is possible to provide beneficial automated support. This paper thus proposes a conceptual framework for providing automated support for aspects of the planning capability. It will describe the complex nature of military planning and proposes a pragmatic approach to providing planning support tools. This work is one part of the International Technology Alliance (ITA) research on collaborative shared understanding and problem solving over a network, where military planning is an example of distributed collaborative problem solving [1].' as abstract.

there is a document named 'doc-942' that
  has 'Service discovery is an essential step in deploying many wireless network applications. The design of service discovery protocols is particularly challenging for mobile wireless networks because of their dynamic and unstructured nature. Most of the previously proposed protocols are based on the assumption that there exists an end-to-end connection from the query source node to the destination node, the assumption that rarely holds for mobile wireless networks. In this paper, we propose a novel service discovery protocol for delay tolerant networks in which all node connections are intermittent. We use Bloom filter to describe services through a fixed size string and provide efficient service announcement and search. The service queries are spread in the network via random walk and forwarded using node meeting history as a hint. Our simulation results show that the proposed protocol achieves good performance as measured by the service discovery success rate, delay and overhead.' as abstract.

there is a document named 'doc-943' that
  has 'In a delay tolerant network (DTN), nodes are connected intermittently and the future node connections are mostly unknown. Since in these networks, a fully connected path from source to destination is unlikely to exist, message delivery relies on opportunistic routing. However, effective forwarding based on a limited knowledge of contact behavior of nodes is challenging. Most of the previous studies looked at only the pairwise node relations to decide routing. In contrast, in this paper, we analyze the correlation between the meetings of each node with other nodes and focus on the utilization of this correlation for efficient routing of messages. We introduce a new metric called conditional intermeeting time, which computes the average intermeeting time between two nodes relative to a meeting with a third node using only the local knowledge of the past contacts. Then, we show how we can utilize the proposed metric on the existing DTN routing protocols to improve their performance. For shortest-path based routing protocols in DTNs, we propose to route messages over conditional shortest paths in which the link cost between nodes are defined by conditional intermeeting times. Moreover, for metric-based forwarding protocols, we propose to use conditional intermeeting time as an additional delivery metric while making forwarding decisions of messages. Our trace-driven simulations on three different datasets show that the modified algorithms perform better than the original ones.' as abstract.

there is a document named 'doc-944' that
  has 'There have been many proposed approaches to performing negotiation in terms of the negotiation procedure, the implementation of agreement, the interactions of software agents representing the different organizations, cooperation among agents, etc. However, one cannot determine a best single approach as it highly depends on the specific application and usage scenario, as well as the needs and goals of the participants. For instance, in some situations, reaching a near pareto-optimal solution is desirable even though it requires that an exhaustive search on all attributes must be performed. In other situations, time might be more valuable and therefore reaching an agreement in a timely manner might have a higher priority. In order to address many different types of negotiation goals and scenarios, there is a need for a flexible negotiation system that can incorporate various alternatives and that is easily extensible and configurable. In this paper, we provide a generic negotiation system that can support many types of negotiation protocols. The proposed system acts as a third party that facilitates the negotiation process between multiple entities and allows them to choose a common negotiation goal and a desired negotiation protocol. We will provide a demonstration of the tool at the conference.' as abstract.

there is a document named 'doc-945' that
  has 'Identifying the behavioral patterns in a social network setting is beneficial to understand how people behave in certain application domains. Such patterns can also be utilized to characterize social signals such as social roles from interactions. In this work, we examine how probabilistic context free grammars (PCFGs) can be utilized to model interactions and role taking in a social network. We describe how to automatically build a PCFG given a set of interactions as the training data. Our experiments on the Mission Survival Corpus 1 (MSC-1) dataset show that PCFGs are a concise way of modeling social entity behaviors and are useful in understanding the probability distribution of interactions as well as the behavior types that are observed.' as abstract.

there is a document named 'doc-946' that
  has 'Service modeling and composition is a fundamental method for offering advanced functionality by combining a set of primitive services provided by the system. Unlike in the case of web services for which there is an abundance of reliable resources, in sensor networks, the resources are constrained and communication among nodes is error-prone and unreliable. Such a dynamic environment requires a continuous adaptation of the composition of services. In this paper, we first propose a graph-based model of sensor services that maps to the operational model of sensor networks and is amenable to analysis. Based on this model, we formulate the process of sensor service composition as a cost-optimization problem, which is NP-complete. We then propose two heuristic methods for its solution, the top-down and the bottom-up, and discuss their centralized and distributed implementations. Using simulations, we evaluate their performance.' as abstract.

there is a document named 'doc-947b' that
  has 'There is a significant challenge in designing, optimizing, deploying and managing complex sensor networks over heterogeneous communications infrastructures. The ITA Sensor Fabric addresses these challenges in the areas of sensor identification and discovery, sensor access and control, and sensor data consumability, by extending the message bus model commonly found in commercial IT infrastructures out to the edge of the network. In this paper we take the message bus model further into a semantically rich, model-based design and analysis approach that considers the sensor network and its contained services as a Service Oriented Architecture. We present an application of a hierarchic schema for nested service definitions together with an initial ontology that describes the assets and services deployed in a sensor network infrastructure.' as abstract.

there is a document named 'doc-948' that
  has 'In traditional multi-level security (MLS) models, object labels are fixed assessments of sensitivity. In practice there will inevitably be some uncertainty about the damage that might be caused if a document falls into the wrong hands. Furthermore, unless specific management action is taken to regrade the label on an object, it does not change. This does not reflect the operational reality of many modern systems where there is clearly a temporal element to the actual sensitivity of information. Tactical information may be highly sensitive right now but comparatively irrelevant tomorrow whilst strategic secrets may need to be maintained for many years, decades, or even longer. In this paper we propose to model both security labels and clearances as probability distributions. We provide practical templates to model both uncertainty and temporally characterised dependencies, and show how these features can be naturally integrated into a recently proposed access control framework based on quantified risk.' as abstract.

there is a document named 'doc-949' that
  has 'Ciphertext-policy attribute-based encryption (CPABE) provides an encrypted access control mechanism for broadcasting messages. Basically, a sender encrypts a message with an access control policy tree which is logically composed of attributes; receivers are able to decrypt the message when their attributes satisfy the policy tree. A user\'s attributes stand for the properties that he currently owns. A user should keep his attributes up-to-date. However, this is not easy in CP-ABE because whenever one attribute changes, the entire private key, which is based on all the attributes, must be changed. In this paper, we introduce fading function, which renders attributes "dynamic" and allows users to update each attribute separately. We study how choosing fading rate for fading function affects the efficiency and security. We also compare our design with CP-ABE and find our scheme performs significantly better under certain circumstance.' as abstract.

there is a document named 'doc-950' that
  has 'Due to technology or policy constraints, communications across network domains usually require the intervention of gateways, and their proper deployment is crucial to the overall performance. In this paper, we study the problem of placing static gateways in mobile DTNs consisting of multiple domains. Given a limited gateway budget, the problem is to select deployment locations to optimize certain performance. The challenge is that different domains may possess heterogeneous properties. To ensure general applicability of solution, we propose a unified framework based on utility optimization, and solve utility computation and placement optimization separately. To handle heterogeneity, we decompose utility computation into individual domains and derive closed-form solutions based on key domain characteristics with focus on the routing scheme. Moreover, we develop quadratic-complexity algorithms to solve the optimization efficiently, which has guaranteed performance under certain uniformity conditions. Although certain assumptions have been made in developing the solutions, evaluations based on synthetic data and real DTN traces both show that the proposed solutions can achieve near-optimal (within 5%) performance at much lower complexities, and the results are robust with respect to the routing schemes and the mobility patterns. Compared with utility-agnostic deployments, our solutions significantly improve the end-to-end performance (by up to 50%).' as abstract.

there is a document named 'doc-952' that
  has 'In a number of network scenarios (including military settings), mobile nodes are clustered into groups, with nodes within the same group exhibiting significant correlation in their movements. Mobility models for such networks should reflect this group structure. In this paper, we consider the problem of identifying the number of groups, and the membership of mobile nodes within groups, from a trace of mobile nodes. We present two clustering algorithms to determine the number of groups and their identities: k-means chain and spectral clustering. Different from traditional k-means clustering, k-means chain identifies the number of groups in a dynamic graph, using a chaining process to keep track of group trajectories over the entire trace. The second approach uses spectral clustering, which uses similarities between node pairs to cluster nodes into groups. We show that the number of groups and node membership can be accurately extracted from traces, particularly when the number of groups is small.' as abstract.

there is a document named 'doc-953' that
  has 'Opportunistic forwarding is a simple scheme for packet routing in ad hoc wireless networks such as duty cycling sensor networks in which reducing energy consumption is a principal goal. While it is simple and can be analytically characterized, it suffers from a high end-to-end latency. In this paper we show how this latency can be drastically reduced if nodes have limited knowledge of network topology (that can be achieved by scoped dissemination of link state information), and hence deriving a hybrid routing protocol. We give an analytical formulation of end-to-end latency between any pair of nodes in such duty cycling networks as the scope of topology dissemination is varied. We borrow from our prior results derived from spectral graph theory to derive exact expressions for mean latency as a function of various network and protocol parameters such as size, duty cycle probability, and scope of link state dissemination. These analytical expressions agree very well with simulation results. We also show how this latency analysis can be coupled with overhead analysis to determine good values of topology dissemination scope.' as abstract.

there is a document named 'doc-954a' that
  has 'The Diffie-Hellman protocol (DHP) is one of the most studied protocols in cryptography. Much work has been dedicated to armor the original protocol against active attacks while incurring a minimal performance overhead relative to the basic (unauthenticated) DHP. This line of work has resulted in some remarkable protocols, e.g., MQV, where the protocol\'s communication cost is identical to that of the basic DHP and the computation overhead is small. Unfortunately, MQV and similar 2-message "implicitly authenticated" protocols do not achieve full security against active attacks since they cannot provide forward secrecy (PFS), a major security goal of DHP, against active attackers. In this paper we investigate the question of whether one can push the limits of authenticated DHPs even further, namely, to achieve communication complexity as in the original DHP (two messages with a single group element per message), maintain low computational overhead, and yet achieve full PFS against active attackers in a provable way. We answer this question in the affirmative by resorting to an old and elegant key agreement protocol: the Okamoto-Tanaka protocol [32]. We present a variant of the protocol (denoted mOT) which achieves the above minimal communication, incurs a computational overhead relative to the basic DHP that is practically negligible, and yet achieves full provable key agreement security, including PFS, against active attackers. Moreover, due to the identity-based properties of mOT, even the sending of certificates (typical for authenticated DHPs) can be avoided in the protocol. As additional contributions, we apply our analysis to prove the security of a recent multi-domain extension of the Okamoto-Tanaka protocol by Schridde et al. and show how to adapt mOT to the (non id-based) certificate-based setting.' as abstract.

there is a document named 'doc-955b' that
  has 'We explore the idea of applying machine learning techniques to automatically infer risk-adaptive policies whose purpose is to reconfigure a network\'s security architecture when the context in which it operates changes. To illustrate this approach, we consider the case of a MANET where certain nodes carrying sensitive services (e.g., web servers, key repositories, firewalls, etc.) should consider relocating themselves into a different node to guarantee a proper functioning. We use simulation to derive properties from a candidate policy, and then apply Genetic Programming and Multi-Objective Optimisation techniques to search for optimal candidates. The inferred policies take the form of risk-aware service relocation algorithms that autonomously dictate when and how to relocate services with the aim of keeping risk to a minimum. Since security policies often have implications in dimensions other than security, we force the learning process to consider also the consequences (performance, usability) of a given policy.' as abstract.

there is a document named 'doc-956' that
  has 'We present Virtual High-resolution Time (VHT), a powerproportional time-keeping service that offers a baseline power draw of a low-speed clock (e.g. 32 kHz crystal), but provides the time resolution that only a higher frequency clock could offer (e.g. 8 MHz crystal), and scales essentially linearly with access (i.e. the "reading" and "writing" of the clock). We achieve this performance by revisiting a basic assumption in the design of time-keeping systems - that to achieve a given time-stamping resolution, a free-running timebase of equivalent frequency is needed. We show that this assumption is false and argue that the dependence is not on usage (i.e. whether on or off) but rather on access (i.e. reading and writing). Therefore, it is possible to duty cycle the free-running timebase itself, and augment it with a lower-frequency, temperature-compensated one, which achieves comparable resolution, at a fraction of the power, for typical workloads. The key technical challenge lies in duty cycling the fast clock and synchronizing the fast and slow clocks. To assess the viability of the approach, we explore how VHT could be implemented on several different platform architectures, and to study the power/performance tradeoff, we characterize VHT on one particular architecture in detail. Our results show power-proportional operation with a 10_ improvement in average power and a synchronization accuracy exceeding 1 _s at duty cycles below 0.1%.' as abstract.

there is a document named 'doc-957' that
  has 'In this paper, we investigate how packet delays and losses affect the quality of target tracking. Specifically, we use Bayesian information of the posterior distribution of target locations to quantify the quality of target tracking and investigate how network quality and measurement quality affect the value of Bayesian information. We show that improving measurement quality provides diminishing gain on tracking quality, while the gain from improving network quality does not diminish. We obtain the condition under which a user obtains information gain on the target location from a tracking process. We further use Bayesian information as the metric for the gateway to select the sensor for taking measurements and determine the measurement time to control the tracking quality' as abstract.

there is a document named 'doc-958' that
  has 'Assessing the quality of sensor-originated information is key for the effective and predictable operation sensor-enabled computerised applications. However, with increasing uncertainties due to alternative deployment scenarios, operational realities, and sensing resource use, it becomes very challenging in designing replicable software solutions that can easily be reused in a number of occasions with minimal customisation. Leveraging semantic sensor web technologies, this paper presents an ontology-based design framework for organising a library of quality of information (QoI) analysis algorithms specific to a data source, and interfacing to a library containing computational algorithms assessing quality of information.  The ontology-based framework is brand enough to allow easy accommodation of new computational algorithms that domain experts may provide to the library as needed to reflect specific deployment, operational, sensing realities.' as abstract.

there is a document named 'doc-959' that
  has 'Data loss in wireless sensing applications is inevitable and while there have been many attempts at coping with this issue, recent developments in the area of Compressive Sensing (CS) provide a new and attractive perspective. Since many physical signals of interest are known to be sparse or compressible, employing CS, not only compresses the data and reduces effective transmission rate, but also improves the robustness of the system to channel erasures. This is possible because reconstruction algorithms for compressively sampled signals are not hampered by the stochastic nature of wireless link disturbances, which has traditionally plagued attempts at proactively handling the effects of these errors. In this paper, we propose that if CS is employed for source compression, then CS can further be exploited as an application layer erasure coding strategy for recovering missing data. We show that CS erasure encoding (CSEC) with random sampling is robust for handling missing data in erasure channels, paralleling the performance of BCH codes, with the added benefit of graceful degradation of the reconstruction error even when the amount of missing data far exceeds the designed redundancy. Further, since CSEC is equivalent to nominal oversampling in the incoherent measurement basis, it is computationally cheaper than conventional erasure coding. We support our proposal through extensive performance studies.' as abstract.

there is a document named 'doc-960d' that
  has 'In many applications, tasks can be delegated to intelligent agents. In order to carry out a task, an agent should reason about what types of resources the task requires. However, determining the right resource types requires extensive expertise and domain knowledge. In this paper, we propose means to automate the selection of resource types that are required to fulfill tasks. Our approach combines ontological reasoning and logic programming for a flexible matchmaking of resources to tasks. Using the proposed approach, intelligent agents can autonomously reason about the resources and tasks in various real-life settings. Using a case-study, we describe and evaluate how agents can use the proposed approach to promote resource sharing. Our evaluations show that the proposed approach is efficient and very useful for multi-agent systems.' as abstract.

there is a document named 'doc-961' that
  has 'Timely dissemination of information to mobile users is vital in many applications. In a critical situation, no network infrastructure may be available for use in dissemination, over and above the on-board storage capability of the mobile users themselves. We consider the following specialized content distribution application: a group of users equipped with wireless devices build an ad hoc network in order cooperatively to retrieve information from certain regions (the mission sites). Each user requires access to some set of information items originating from sources lying within a region. Each user desires low-latency access to its desired data items, upon request (i.e., when pulled). In order to minimize average response time, we allow users to pull data either directly from sources or, when possible, from other nearby users who have already pulled, and continue to carry, the desired data items. That is, we allow for data to be pushed to one user and then pulled by one or more additional users. The total latency experienced by a user vis-vis a certain data item is then in general a combination of the push delay and the pull delay. We assume each delay time is a function of the hop distance between the pair of points in question. Our goal in this paper is to assign data to mobile users, in order to minimize the total cost and the average latency experienced by all the users. In a static setting, we solve this problem in two different schemes, one of which is easy to solve but wasteful, one of which relates to NP-hard problems but is less so. Then in a dynamic setting, we adapt the algorithm for the static setting and develop a new algorithm with respect to users\' gradual arrival. In the end we show a trade-off can be made between minimizing the cost and latency.' as abstract.

there is a document named 'doc-962' that
  has 'Utility-based cross-layer optimization is a valuable tool for resource management in mission-oriented wireless sensor networks (WSN). The benefits of this technique include the ability to take application- or mission-level utilities into account and to dynamically adapt to the highly variable environment of tactical WSNs. Recently, we developed a family of distributed protocols which adapts the bandwidth and energy usage in mission-oriented WSN in order to optimally allocate resources among multiple missions, that may have specific demands depending on their priority, and also variable schedules, entering and leaving the network at different times.9-12 In this paper, we illustrate the practical applicability of this family of protocols in tactical networks by implementing one of the protocols, which ensures optimal rate adaptation for congestion control in mission-oriented networks,9 on a real-time 802.11b network using the ITA Sensor Fabric.13 The ITA Sensor Fabric is a middleware infrastructure, developed as part of the International Technology Alliance (ITA) in Network and Information Science,14 to address the challenges in the areas of sensor identification, classification, interoperability and sensor data sharing, dissemination and consumability, commonly present in tactical WSNs.15 Through this implementation, we (i) study the practical challenges arising from the implementation and (ii) provide a proof of concept regarding the applicability of this family of protocols for efficient resource management in tactical WSNs amidst the heterogeneous and dynamic sets of sensors, missions and middle-ware.' as abstract.

there is a document named 'doc-963' that
  has 'Coverage redundancy problem is one of the significant problems in wireless sensor networks. To reduce the energy consumption that arises when the high number of sensors is active, various coverage control protocols (sleep scheduling algorithms) have been proposed. In these protocols, a subset of nodes necessary to maintain sufficient sensing coverage are kept active while the others are put into a sleep mode to reduce the energy consumption. In this paper, we study the coverage redundancy problem in a sensor network where the locations of nodes and the distances between nodes are neither known nor could be easily calculated. We define a neighbor graph as the graph formed by the neighbors of a node and analyze the effect of different levels of connectivity in neighbor graphs on the coverage redundancy of sensor nodes. Moreover, we apply our results to a lightweight deployment-aware scheduling algorithm and demonstrate the improvement in the performance of the algorithm.' as abstract.

there is a document named 'doc-964' that
  has 'Routing in delay tolerant networks (DTNs) in which most of the nodes are mobile and intermittently connected is a challenging problem because of unpredictable node movements and lack of knowledge of future node connections. To ensure reliability against failures and increase the success rate of delivery, erasure coding technique is used to route messages in DTNs. In this paper, we study how the cost of erasure coding based routing protocols can be reduced. Specifically, we analyze the effects of different spraying algorithms, right parameter selection and splitting spraying phase on the cost of message delivery. We also perform simulations to evaluate the proposed approaches and demonstrate that the cost of erasure coding based routing can be reduced considerably with the proposed strategies while maintaining the delivery rate and delay objectives.' as abstract.

there is a document named 'doc-965' that
  has 'In open, dynamic multi-agent systems, agents may form short-term ad-hoc groups, such as coalitions, in order to meet their goals. Trust and reputation are crucial concepts in these environments, as agents must rely on their peers to perform as expected, and learn to avoid untrustworthy partners. However, ad-hoc groups introduce issues which impede the formation of trust relationships. For example, they may be short-lived, precluding agents from gaining the necessary experiences to make an accurate trust evaluation. This paper describes a new approach, inspired by theories of human organisational behaviour, whereby agents generalise their experiences with known partners as stereotypes and apply these when evaluating new and unknown partners. We show how this approach can complement existing state of the art trust models, and enhance the confidence in the evaluations that can be made about trustees when direct and reputational information is lacking or limited.' as abstract.

there is a document named 'doc-966' that
  has 'We present an efficient approach for identifying, learning and modeling the policies of others during collaborative activities. In a set of experiments, we demonstrate that more accurate models of others\' policies (or norms) can be developed more rapidly using various forms of evidence from argumentation-based dialogue.' as abstract.

there is a document named 'doc-967' that
  has 'Imagine many small devices send data to a single receiver, encrypted using the receiver\'s public key. Assume an adversary that has the power to adaptively corrupt a subset of these devices. Given the information obtained from these corruptions, do the ciphertexts from uncorrupted devices remain secure? Recent results suggest that conventional security notions for encryption schemes (like IND-CCA security) do not suffice in this setting. To fill this gap, the notion of security against selective-opening attacks (SOA security) has been introduced. It has been shown that lossy encryption implies SOA security against a passive, i.e., only eavesdropping and corrupting, adversary (SO-CPA). However, the known results on SOA security against an active adversary (SO-CCA) are rather limited. Namely, while there exist feasibility results, the (time and space) complexity of currently known SO-CCA secure schemes depends on the number of devices in the setting above. In this contribution, we devise a new solution to the selective opening problem that does not build on lossy encryption. Instead, we combine techniques from non-committing encryption and hash proof systems with a new technique (dubbed "cross-authentication codes") to glue several ciphertext parts together. The result is a rather practical SO-CCA secure public-key encryption scheme that does not suffer from the efficiency drawbacks of known schemes. Since we build upon hash proof systems, our scheme can be instantiated using standard number-theoretic assumptions such as decisional Diffie-Hellman (DDH), decisional composite residuosity (DCR), and quadratic residuosity (QR). Besides, we construct a conceptually very simple and comparatively efficient SO-CPA secure scheme from (slightly enhanced) trapdoor one-way permutations. We stress that our schemes are completely independent of the number of challenge ciphertexts, and we do not make assumptions about the underlying message distribution (beyond being efficiently samplable). In particular, we do not assume efficient conditional re-samplability of the message distribution. Hence, our schemes are secure in arbitrary settings, even if it is not known in advance how many ciphertexts might be considered for corruptions.' as abstract.

there is a document named 'doc-968' that
  has 'In this work, we present a robust sensor fusion system for exploratory data collection, exploiting the spatial redundancy in sensor networks. Unlike prior work, our system design criteria considers a heterogeneous correlated noise model and packet loss, but no prior knowledge of signal characteristics. The former two assumptions are both common signal degradation sources in sensor networks, while the latter allows exploratory data collection of unknown signals. Through both a numerical example and an experimental study on a large military site, we show that our proposed system reduces the noise in an unknown signal by 58.2% better than a comparable algorithm.' as abstract.

there is a document named 'doc-969' that
  has 'Estimating the relative value of alternative tactics, techniques and procedures (TTP) and information systems requires measures of the costs and benefits of each, and methods for combining and comparing those measures. The NATO Code of Best Practice for Command and Control Assessment explains that decision making quality would ideally be best assessed on outcomes. Lessons learned in practice can be assessed statistically to support this, but experimentation with alternate measures in live conflict is undesirable. To this end, the development of practical experimentation to parameterize effective constructive simulation and analytic modelling for system utility prediction is desirable. The Land Battlespace Systems Department of Dstl has modeled human development of situational awareness to support constructive simulation by empirically discovering how evidence is weighed according to circumstance, personality, training and briefing. The human decision maker (DM) provides the backbone of the information processing activity associated with military engagements because of inherent uncertainty associated with combat operations. To develop methods for representing the process in order to assess equipment and non-technological interventions such as training and TTPs we are developing componentized or modularized timed analytic stochastic model components and instruments as part of a framework to support quantitative assessment of intelligence production and consumption methods in a human decision maker-centric mission space. In this paper, we formulate an abstraction of the human intelligence fusion process from the Defence Science and Technology Laboratory\'s (Dstl\'s) INCIDER model to include in our framework, and synthesize relevant cost and benefit characteristics.' as abstract.

there is a document named 'doc-970' that
  has 'Sensor coverage varies with location due to factors such as weather, terrain, and obstacles. If a field can be partitioned into zones of homogeneous sensing areas, then coverage by a random deployment of sensors can be optimized by controlling the number of sensors deployed in each zone. We derive expressions to directly calculate the optimal sensor partition in runtime asymptotically equal to the number of zones. We further derive expressions to determine the minimum sensor count required to achieve a specific coverage threshold. We bound the maximum increase in coverage over a strategy oblivious to differences in sensing areas, which our results show is no greater than 13% for a field with two zones. While the analytical solutions assume that each zone is covered independently, we allow sensors to affect neighboring zones in simulations. Nevertheless, the simulations support the optimality of our solution.' as abstract.

there is a document named 'doc-971' that
  has 'Delay tolerant networks are characterized by the sporadic connectivity between their nodes and therefore the lack of stable end-to-end paths from source to destination. Since the future node connections are mostly unknown in these networks, opportunistic forwarding is used to deliver messages. However, making effective forwarding decisions using only the network characteristics (i.e. average intermeeting time between nodes) extracted from contact history is a challenging problem. Based on the observations about human mobility traces and the findings of previous work, we introduce a new metric called conditional intermeeting time, which computes the average intermeeting time between two nodes relative to a meeting with a third node using only the local knowledge of the past contacts. We then look at the effects of the proposed metric on the shortest path based routing designed for delay tolerant networks. We propose Conditional Shortest Path Routing (CSPR) protocol that routes the messages over conditional shortest paths in which the cost of links between nodes is defined by conditional intermeeting times rather than the conventional intermeeting times. Through trace-driven simulations, we demonstrate that CSPR achieves higher delivery rate and lower end-to-end delay compared to the shortest path based routing protocols that use the conventional intermeeting time as the link metric.' as abstract.

there is a document named 'doc-972' that
  has 'Location based routing protocols are heavily dependent on location services which provide the position information of the desired destination node. Seldom location service schemes include energy efficiency metrics when evaluating their performance in forwarding location update and query packets. We propose a novel location service that aims at decreasing the distance traveled by the location update and query packets and, thus, at reducing the overall energy cost. Simulation results are presented to demonstrate that the new scheme achieves energy efficiency while maintaining all the other performance metrics comparable to the previously published algorithms.' as abstract.

there is a document named 'doc-974' that
  has 'As more data (especially scientific data) is digitized and put on the Web, the importance of tracking and sharing its provenance metadata grows. Besides capturing the annotation properties of data, provenance research also emphasizes interlinking relevant data. Therefore, it is desirable to make provenance metadata easy to access, share, reuse, integrate and reason with. To address these requirements, ontologies can be of use to encode expectations and agreements concerning provenance metadata reuse and integration. The Web is of use to support access and sharing. The Semantic Web, with its languages for representing terms and their descriptions,, such as RDFS and OWL, is of use for capturing expectations, agreements, and meaning. We are investigating best practices for providing Semantic Web encodings for provenance ontologies by analyzing a selection of popular Semantic Web provenance ontologies such as Open Provenance Model (OPM), Dublin Core (DC) Terms, and the Proof Markup Language (PML). In this paper, we will highlight a few findings which include: (i) similarities and differences among existing provenance ontologies; (ii) popular approaches used to model provenance concepts and lessons learned from the usage of Semantic Web language features in representing provenance concepts; (iii) expressivity and tractability of representative provenance ontologies. The outcome of our study provides not only guidance to provenance ontology users but also insights to promote better collaborative provenance ontology development and scalable processing of provenance ontologies.' as abstract.

there is a document named 'doc-975a' that
  has 'Verifiable Computation enables a computationally weak client to "outsource" the computation of a function F on various inputs x1,...,xk to one or more workers. The workers return the result of the function evaluation, e.g., yi = F(xi), as well as a proof that the computation of F was carried out correctly on the given value xi . The verification of the proof should require substantially less computational effort than computing F(xi) from scratch. We present a protocol that allows the worker to return a computationally-sound, non-interactive proof that can be verified in O(m) time, where m is the bit-length of the output of F. The protocol requires a one-time pre-processing stage by the client which takes O(' as abstract and
  has 'C' as abstract and
  has ') time, where C is the smallest Boolean circuit computing F. Our scheme also provides input and output privacy for the client, meaning that the workers do not learn any information about the xi or yi values.' as abstract.

there is a document named 'doc-976' that
  has 'In this work we study the activity span of MySpace accounts and its connection to the distribution of the number of friends. The activity span is the time elapsed since the creation of the account until the user\'s last login time. We observe exponentially distributed activity spans. We also observe that the distribution of the number of friends over accounts with the same activity span is well approximated by a lognormal with a fairly light tail. These two findings shed light into the puzzling (yet unexplained) inflection point (knee) in the distribution of friends in MySpace when plotted in log-log scale. We argue that the inflection point resembles the inflection point of Reed\'s (Double Pareto) Geometric Brownian Motion with Exponential Stopping Times model. We also present evidence against the Dunbar number hypothesis of online social networks, which argues, without proof, that the inflection point is due to the Dunbar number (a theoretical limit on the number of people that a human brain can sustain active social contact with). While we answer many questions, we leave many others open.' as abstract.

there is a document named 'doc-977' that
  has 'The security of access and information flow carries with it the risk that resources will be misused - intentionally or accidentally. Static Access Control (AC) policies based on qualitative judgements are insufficient for scenarios where roles and access requirements are subtle and change frequently. We propose using quantified risk and benefit estimates in the design and management of AC policies. The first step is to build models for estimating risk and benefit. Many factors may affect risk and benefit, and the relationship among them and their impacts are usually complex and hard to determine analytically. Therefore we decided to use machine learning techniques to learn the models from AC data. Due to the sensitive and evolving nature of real AC data, we must generate synthetic data to begin demonstrating the efficacy of our approach. Given that no sets of AC data could cover all possibilities, we must also show that the models learned can be applied to data outside the training sets. We start examining these problems by first creating a parametrised simulation model, designed to capture certain properties related to the risk and benefit of information transfer, including how risk aggregates over time. We then explore how different choices of the model parameters affect our ability to predict this risk, and describe some preliminary results of transferring learning between different instantiations of the model.' as abstract.

there is a document named 'doc-978' that
  has 'While scheduling the nodes in a wireless network to sleep periodically can save energy, it also incurs higher latency and lower throughput. We consider the problem of designing optimal sleep schedules in wireless networks, and show that finding sleep schedules that can minimize the latency over a given subset of source-destination pairs is NP-hard. We also derive a latency lower bound given by d   O(1/p) for any sleep schedule with a required active rate (i.e., the fraction of active slots of each node) p, and the shortest path length d. We offer a novel solution to optimal sleep scheduling using green-wave sleep scheduling (GWSS), inspired by coordinated traffic lights, which is shown to meet our latency lower bound (hence is latency-optimal) for topologies such as the line, grid, ring, torus and tree networks, under light traffic. For high traffic loads, we propose non-interfering GWSS, which can achieve the maximum throughput scaling law given by T(n, p) = _(p/_n) bits/sec on a grid network of size n, with a latency scaling law D(n, p) = O( _n) O(1/p). Finally, we extend GWSS to a random network with n Poisson-distributed nodes, for which we show an achievable throughput scaling law of T(n, p) = _(p/_n log n) bits/sec and a corresponding latency scaling law D(n, p) = O( !n/ log n)   O(1/p); hence meeting the well-known Gupta-Kumar achievable throughput rate _(1/ _n log n) when p _ 1.' as abstract.

there is a document named 'doc-979' that
  has 'Neighbor discovery is essential for the process of self-organization of a wireless network, where almost all routing and medium access protocols need knowledge of one-hop neighbors. In this paper we study the problem of neighbor discovery in a static and synchronous network, where time is divided into slots, each of duration equal to the time required to transmit a hello message, and potentially, some sort of feedback message. Our main contributions lie in detailing the physical layer mechanism for how nodes in receive mode detect the channel status, describing algorithms at higher layers that exploit such a knowledge, and characterizing the significant gain obtained. In particular, we describe one possible physical layer architecture that allows receivers to detect collisions, and then introduce a feedback mechanism that makes the collision information available to the transmitters. This allows nodes to stop transmitting packets as soon as they learn about the successful reception of their discovery messages by the other nodes in the network. Hence, the number of nodes that need to transmit packets decreases over time. These nodes transmit with a probability that is inversely proportional to the number of active nodes in their neighborhood, which is estimated using the collision information available at the nodes. We show through analysis and simulations that our algorithm allows nodes to discover their neighbors in a significantly smaller amount of time compared to the case where reception status feedback is not available to the transmitters.' as abstract.

there is a document named 'doc-980' that
  has 'In this paper, we study the application of cooperative diversity to wireless broadcast channels, a fundamental building block of wireless communication networks. Several cooperative broadcast protocols will be proposed, and information theoretic metrics are developed to facilitate performance evaluation. Provided that there is no direct S-D link, the proposed protocols can achieve a multiplexing gain close to one, whereas the traditional two-hop scheme can only achieve the diversity gain 1/2. Provided that there are direct S-D links, the proposed protocol can still outperform the comparable scheme, particularly at high multiplexing gains.' as abstract.

there is a document named 'doc-981' that
  has 'Recent advances in wireless technology have led to the proliferation of wireless devices, ranging from wireless LAN to cellular phones. There has been, and will be, increase in not only the number of wireless services, but also the number of types of wireless services, and greater demand for mobility, portability, and integrated services. For wireless success to continue, technologies must support a high degree of flexibility and interoperability of wireless devices to scale with such demand. Next Generation Wireless will depend highly upon flexible and interoperable devices in constructing a wireless network that consists of heterogeneous devices that can communicate with each other. Our work investigates the technical issues that must be overcome in bringing such flexibility and interoperability, one of which being device discovery. We propose Wireless Interrupt, an inter-device signaling mechanism that can be employed by software defined radios, such that a device can signal its neighbors of its existence or the services it provides without knowing the protocols or channels used by its neighbors a priori.' as abstract.

there is a document named 'doc-983' that
  has 'Controlled mobile helper nodes called data ferries have recently been proposed to bridge communications between disconnected nodes in a delay-tolerant manner. While existing work has explored various trajectory designs for the data ferry by assuming either static nodes or full observations at the data ferry, the problem remains open when the nodes are mobile and the ferry only has partial observations. In this paper, we investigate the problem of dynamic ferry mobility control under limitedrange sensing. Assuming the data ferries are capable of sensing node presence within certain range and adjust their movements dynamically, we aim to design control policies that maximize the number of effective contacts. We provide a comprehensive model of the control framework using Partially Observable Markov Decision Process (POMDP), based on which we study the structure of the optimal policy and propose an efficient heuristic policy which shows significant improvement over the predetermined benchmark. To the best of our knowledge, this is the first data ferry control mechanism that can handle both stochastic node mobility and incomplete ferry observations.' as abstract.

there is a document named 'doc-984' that
  has 'Morphogenesis is the process that gives shapes to organisms from an embryonic stage through a sequence of cell divisions. Starting from a simple embryonic cell, the controlled division and transformation of the cells into different types leads to the creation of a complex organism. The growth of complex organisms is completely autonomic, and is one of the best examples of self-organizing systems found in nature. In comparison, computer networks of today are much more static and require significant human intervention in order to take on the form that is desired. However, emerging technologies, such as server and network virtualization, enable an architecture where complex computer systems can also develop by following the morphogenesis paradigm. In this paper, we present the benefits of morphogenesis in a distributed data center based computing environment, and we describe the architecture of a computer network that self-organizes using the principles of morphogenesis.' as abstract.

there is a document named 'doc-985' that
  has 'Abductive reasoning is a powerful inference mechanism that can generate conditional proofs. The combination of Abduction and Logic Programming (ALP) [5] has many known applications, such as planning, scheduling, cognitive robotics, medical diagnosis and policy analysis [3]. However, most abductive frameworks [7, 4, 8] rely on a centrally executed proof procedure whereas many of the application problems are distributed by nature. Confidentiality and communication overhead concerns often preclude transmitting all the knowledge required for centralised reasoning. Recently, ALIAS [1] and DARE [9] have shown how to distribute abductive computation in a collaborative system. However, their distributed proof procedures, which are based on the well-known Kakas-Mancarella procedure [6], do not support constructive negation [11] and cannot compute non-ground conditional proofs. Hence they cannot be used for applications such as scheduling and planning involving time and cost, which require constraint processing [12].' as abstract.

there is a document named 'doc-986a' that
  has 'Network coding has received significant attention in the networking community for its potential to increase throughput and improve robustness without any centralized control. Unfortunately, network coding is highly susceptible to "pollution attacks" in which malicious nodes modify packets in a way that prevents the reconstruction of information at recipients; such attacks cannot be prevented using standard end-to-end cryptographic authentication because network coding requires that intermediate nodes modify data packets in transit. Specialized solutions to the problem have been developed in recent years based on homomorphic hashing and homomorphic signatures. The latter are more bandwidth-efficient but require more computation; in particular, the only known construction uses bilinear maps. We contribute to this area in several ways. We present the first homomorphic signature scheme based solely on the RSA assumption (in the random oracle model), and present a homomorphic hashing scheme based on composite moduli that is computationally more efficient than existing schemes (and which leads to secure network coding signatures based solely on the hardness of factoring in the standard model). Both schemes use shorter public keys than previous schemes. In addition, we show variants of existing schemes that reduce the communication overhead significantly for moderate-size networks, and which improve computational efficiency in some cases quite dramatically (e.g., we achieve a 20-fold speedup in the computation of intermediate nodes). At the core of our techniques is a modified approach to network coding where instead of working in a vector space over a field, we work directly over the integers (with small coefficients).' as abstract.

there is a document named 'doc-987' that
  has 'Partially Observable Markov Decision Process (POMDP) is a popular framework for planning under uncertainty in partially observable domains. Yet, the POMDP model is risk-neutral in that it assumes that the agent is maximizing the expected reward of its actions. In contrast, in domains like financial planning, it is often required that the agent decisions are risk-sensitive (maximize the utility of agent actions, for nonlinear utility functions). Unfortunately, existing POMDP solvers cannot solve such planning problems exactly. By considering piecewise linear approximations of utility functions, this paper addresses this shortcoming in three contributions: (i) It defines the Risk-Sensitive POMDP model; (ii) It derives the fundamental properties of the underlying value functions and provides a functional value iteration technique to compute them exactly and (c) It proposes an efficient procedure to determine the dominated value functions, to speed up the algorithm. Our experiments show that the proposed approach is feasible and applicable to realistic financial planning domains.' as abstract.

there is a document named 'doc-988' that
  has 'Recent work has applied game-theoretic models to real-world security problems at the Los Angeles International Airport (LAX) and Federal Air Marshals Service (FAMS). The analysis of these domains is based on input from domain experts intended to capture the best available intelligence information about potential terrorist activities and possible security countermeasures. Nevertheless, these models are subject to significant uncertaintyespecially in security domains where intelligence about adversary capabilities and preferences is very difficult to gather. This uncertainty presents significant challenges for applying game-theoretic analysis in these domains. Our experimental results show that standard solution methods based on perfect information assumptions are very sensitive to payoff uncertainty, resulting in low payoffs for the defender. We describe a model of Bayesian Stackelberg games that allows for general distributional uncertainty over the attackers payoffs. We conduct an experimental analysis of two algorithms for approximating equilibria of these games, and show that the resulting solutions give much better results than the standard approach when there is payoff uncertainty.' as abstract.

there is a document named 'doc-989' that
  has 'This paper investigates the fundamental limits on the gain of joint Network Coding and link-layer transmission Rate Diversity in wireless multicast applications. Network Coding has been shown to improve throughput of wireless multicast in various scenarios and applications. Those applications can further exploit link-layer Rate Diversity, whereby individual nodes can transmit at faster rates than the overall uniform rate at the expense of smaller coverage area. The benefits of Rate Diversity for wireless multicast have been demonstrated before, without considering the inter-connections between network coding and rate diversity. In this paper, we address the performance of Network Coding and Multi-Rate Diversity in wireless multicast networks and thereby derive formal bounds on the throughput gain for such networks.' as abstract.

there is a document named 'doc-990' that
  has 'Network coding was found to be useful for ad hoc wireless multicast in disruptive channel and connectivity conditions. In heterogeneous networks, comprising teams with different technical preparedness, it is possible that only the radios of the most advanced teams have sufficient resources to network encode/decode. In this case, an interesting solution is partial (hybrid) network coding - only a fraction of the nodes encodes, the balance simply forwarding the packets. Partial coding poses interesting problems and opportunities that require novel solutions. The first contribution of this paper is the control of redundant transmissions. Network coding detects duplicates using the "innovative packet" check; it drops non-innovative packets. Likewise, we require a new duplicate detection scheme for non-network coding (non-NC) nodes using encoding vectors since packet ID may be obliterated during packet mixing. The second contribution is the study of performance loss caused by partial coding. Through the analysis of representative topologies and scenarios, we identify densities and distributions (of noncoding nodes) that render network coding inefficient. The results are of practical importance because they help determine when one should switch from network coding to other forms of protection (e.g., erasure codes or fountain codes). The third contribution is in the protection from malicious packet corruption, i.e., pollution. There is the risk that nodes belonging to an untrusted team (a likely situation in tactical coalitions) can inject polluted packets into the network. Pollution is critical in network coding. If it goes unchecked, pollution makes it easy for an attacker to spoil an entire generation. An opportunity offered by hybrid network coding is to force the untrusted nodes to perform simple forwarding, without coding. We show that their behavior can be more easily and efficiently checked (with hash signatures instead of homomorphic hashes) while still profiting from their forwarding.' as abstract.

there is a document named 'doc-991' that
  has 'An understanding of the policy and resource availability constraints under which others operate is important for effectively developing and resourcing plans in a multi-agent context. Such constraints (or norms) are not necessarily public knowledge, even within a team of collaborating agents. What is required are mechanisms to enable agents to keep track of who might have and be willing to provide the resources required for enacting a plan by modeling the policies of others regarding resource use, information provision, etc. We propose a technique that combines machine learning and argumentation for identifying and modeling the policies of others. Furthermore, we demonstrate the utility of this novel combination of techniques through empirical evaluation.' as abstract.

there is a document named 'doc-992' that
  has 'When human teams collaborate they ensure that all team members understand their current context by communicating both verbally and nonverbally. Military teams are trained to work together and their behaviors often follow learned patterns. If these patterns were observed and recognized, they would provide evidence of a team\'s context, and context could be used to assist the team by anticipating needed support and by disambiguating requests for information. Soldiers from the United Kingdom participated in a series of simulated missions in which their speech and actions were recorded. Analyzing these behaviors revealed patterns related to location and movement, weapon use, and team communication. Both when stationary and when moving the soldiers often followed patterns that are described in field manuals. When suppressing an enemy location they fired in a controlled systematic pattern. There were many patterns observable in their spoken communication related to speech acts, use of metaphors, status reports, and other topics. Simple heuristics were defined for potentially recognizing most of the identified patterns. The heuristics for recognizing location and movement patterns depend primarily on speed of movement, dispersion, and linearity of positions. Measures for these three concepts were defined and calculated for one mission. The values of these concepts appear to vary systematically and accurately reflect the events that occurred in the simulation. Measurements of these concepts may provide a means of identifying and interpreting to some extent the changes in a team\'s context.' as abstract.

there is a document named 'doc-994' that
  has 'A group of acoustic arrays that provide direction of approach estimates also supports classification of vehicles using the beams formed during that estimation. Successful simultaneous tracking and classification has demonstrated the value of such a sensing resource as an unattended ground sensor (UGS) installation. We now consider potential attacks on the integrity of such an installation, describing the effect of compromised acoustic arrays in the data analysis and tracking and classification results. We indicate how these can be automatically recognized, and note that calibration methods intended for deployment time can be used for recovery during operation, which opens the door to methods for recovery from the compromise without re-configuring the equipment, using abductive reasoning to discover the necessary re-processing structure. By rotating an acoustic array, the tracking stability and implied path of an entity can be distorted while leaving the data and analysis from individual arrays self-consistent. Less structured modifications such as exchanging microphone connections impact the basic data analysis. We examine the effect of these classes of attack on the integrity of a set of unattended acoustic arrays, and consider the steps necessary for detection, diagnosis, and recovering an effective sensing system. Understaning these steps plays an important part in reasoning in support of balance of investment, planning, operation and post-hoc analysis.' as abstract.

there is a document named 'doc-995' that
  has 'Effective revision tracking is important to maintain and use Semantic Web data for both publishers and readers. Information related to revisions in this setting often contains basic context information, semantic difference summary, and rationale summary. In this work, we present a general architecture for modeling and publishing revision history of social semantic Web data. This model has been implemented as an extension to an existing infrastructure, namely Semantic MediaWiki. We show a variety of applications that can be built using the framework, including provenance tracking, statistics, temporal reasoning and explanation.' as abstract.

there is a document named 'doc-996' that
  has 'Wikis are a well-known Web 2.0 content management platform. The recent introduction of semantic wikis extends the capabilities of conventional wikis by allowing users to edit and query structured semantic annotations (e.g., categories and typed links) rather than plain wiki text. This new feature, as shown in this paper, supports the provision of a novel, transparent, and light-weight social Web application model. This model enables developers to collectively build Web applications using semantic wikis, supporting such capabilities as data modeling, data management, data processing and data presentation. The source scripts and data of such applications are transparent to Web users. Beyond a generic description for the Web application model, we show two proof-of-concept prototypes, namely RPI Map and CNL (Controlled Natural Language) Wiki, both of which are based on Semantic MediaWiki (SMW).' as abstract.

there is a document named 'doc-997' that
  has 'In this paper, we employ a stochastic geometry model to analyze the transmission capacity of the Decode-andForward (DAF) cooperation scheme in an overlaid wireless network where a primary (PR) network and a secondary (SR) network coexist together. The PR users employ DAF scheme and have a higher priority to access the channel, whereas the SR users use only direct transmission. Because of the fact of coexistence, the interference from SR network seriously affects the performance of PR network. Assuming that simultaneous transmitters in both networks are randomly located in space according to Poisson point processes, we develop outage probabilities for both DAF and direct transmission schemes in both deterministic and Rayleigh fading channels. By defining transmission capacity in terms of the outage probability, a desired data rate and the density of transmissions, we further quantify transmission capacities for both schemes. It shows that the use of cooperative transmission achieves much better reliability and a larger transmission capacity than the use of direct transmission in the PR network. Furthermore, such performance gain can be manipulated to increase the transmission capacity of the SR network without deteriorating the performance of the PR network. Numerical results also demonstrate the significant improvement on the transmission capacity by using cooperative transmission.' as abstract.

there is a document named 'doc-999a' that
  has 'Many multicast protocols have been proposed for ad hoc networks. ODMRP is one of the most popular protocols due to robustness in high mobile wireless networks. Since packet redundancy in a mesh structure, however, ODMRP has a scalability problem when the number of senders and receivers increases. We improve scalability using cognitive radio technology that diversi- fies channel usage for multicast members with cognitive multiple channels. We design a channel allocation and distribution scheme to build a multicast tree for each group. Also, we implemented cooperative sensing by multicast members using Join Query and Reply messages. Our protocol, CoCast, shows better performance in delivery ratio and throughput comparing to conventional ODMRP through simulation experiments.' as abstract.

there is a document named 'doc-x0002' that
  has 'Coalition Intelligence, Surveillance and Reconnaissance (ISR) networks provide an invaluable service to joint missions and operations provided they are installed and operated appropriately. The process of obtaining the benefits of an ISR network needs to begin long before the first shared ISR asset is deployed on the ground. The joint mission needs to be planned so as to satisfy any policy constraints and national objectives individual participants may have, and the right mechanisms for sharing information need to be developed. Policy conflicts that may prevent optimal operation of the network need to be resolved at the appropriate level of authority. In this paper, we present an end-to-end life-cycle for planning and deploying a coalition ISR network. This life-cycle model is targeted to address the requirements that arise due to the differences policies and national objectives of different partners in a coalition.' as abstract.

there is a document named 'doc-x0003' that
  has 'Coalition operations rely on the fusion, sharing and dissemination of information for a network of disparate Intelligence, Surveillance and Reconnaissance (ISR) assets such as sensors, sensing platforms, human intelligence, data fusion and networking elements. One prominent aspect of this research is the design of policy-aware fusion, that is, fusion that takes policy related to security, resource control, command-and-control, etc. into account. Processes are described for development of fusion algorithms and policy protocols that will enable rapid assembly/dynamic control of ISR assets and associated policy agreements that govern the sharing and dissemination of information to support multiple concurrent coalition missions.' as abstract.

there is a document named 'doc-x0004' that
  has 'Research into the retrieval and dissemination of mission-specific information across sensor networks is leading to the development of many novel new algorithms and the definition of new paradigms for configuring and enforcing communication flow policies amongst the various components. Testing the relative merits of such algorithms and policies, and exploring interoperability issues between them, is difficult unless they share a common test and validation framework. The wide variety of algorithms being developed (including ontological, mission scripting, resource allocation, network routing, data fusion algorithms, and policies) presents unique challenges to the development of such a framework and its subsequent instrumentation to provide experimental results. This work demonstrates a prototype of such framework (or "Fabric"), built on top of commercial off-the-shelf (COTS) components, used in a real-world sensor network deployment to experiment with the deployment of algorithms and gathering of live sensor data.' as abstract.

there is a document named 'doc-x0007' that
  has 'Tactical networks are highly constrained in communication and computation resources, particularly at the edge. This limitation can be effectively addressed by the emerging technology of mobile micro-clouds (MMCs) that is aimed at providing seamless computing/data access at the edge of such networks. Deployment of MMCs can enable the delivery of critical, timely, and mission relevant situational awareness to end users in highly dynamic environments. Different from traditional clouds, an MMC is smaller and deployed closer to users, typically attached to a fixed or mobile basestation that is deployed in the field. Due to the relatively small coverage area of each basestation, a mobile user may frequently switch across areas covered by different basestations. An important issue therefore is where to place the service so that acceptable service performance can be maintained, while coping with the user and network dynamics. Existing work has considered this problem mainly from a theoretical angle. In this paper, with the aim of pushing the theoretical results one step closer to practice, we study the performance of dynamic service placement using an emulation framework, namely the Common Open Research Emulator (CORE) which embeds the Extendable Mobile Ad-hoc Network Emulator (EMANE). We first present the system architecture used in the emulation. Then, we present the message exchange and control process between different network entities, as well as methods of deciding where to place the services. Finally, we perform emulation using realworld user mobility traces of San Francisco taxis and present the results. The results show several insightful observations in a realistic network setting, such as the impact of randomness and delay on the service placement performance.' as abstract.

there is a document named 'doc-x0008' that
  has 'Mobile micro-clouds (MMCs), which are small cloud-like infrastructures that can host services for mobile users, provide a promising approach to deliver data and computation to users in low bandwidth/high latency networks. In this work, we take a step towards security-aware service migration for multiple servers for MMC in tactical environments. As a fundamental model, we consider two users sharing a cluster of MMCs and moving between coverage areas of different MMCs according to random walk. We formulate the problem using a finite horizon Markov Decision Process (MDP). Due to the coupling between users due to security (and possibly interference) costs, we observe that the problem is significantly more complex compared with the non-security-aware single user service migration problem studied in the literature, even in the simplest case of two users. Accordingly, we consider efficient control algorithms which aim to minimize time average costs. Particularly, we propose a modified myopic policy which aims to reduce time average costs by steering operation towards settings which would reduce the likelihood of future security cost increases, by proactively keeping the services distant. Our numerical results demonstrate that our modified myopic policy outperforms other policies in terms of average cost for various settings.' as abstract.

there is a document named 'doc-x0011' that
  has 'We consider the problem of placing the minimum number of monitors in a communication network with possible topology changes to identify additive link metrics from path metrics. The core of our solution is a suite of robust monitor placement algorithms with different performance-complexity tradeoffs that guarantee network identifiability for the multiple possible topologies. In particular, we show that the optimal (i.e., minimum) monitor placement is the solution to a generalized hitting set problem, where we provide a polynomial-time algorithm to construct the input. Although the optimal placement is NP-hard in general, we identify non-trivial special cases that can be solved efficiently. We further demonstrate how the proposed algorithms can be augmented to handle unpredictable topology changes and tradeoffs between monitor cost and adaptation cost. Our evaluations on mobility-induced dynamic topologies verify the effectiveness and robustness of the proposed algorithms.' as abstract.

there is a document named 'doc-x0012' that
  has 'We propose a framework for optimizing in-network processing (INP) in wireless sensor networks. INP provides a platform for processing (e.g., fusing, aggregating or compressing) the data along the transmission routes in the sensor network. This can reduce the volume of transmitted data, therefore optimizing the utilization of energy and bandwidth. However, such data processing must ensure that the end result can meet given QoI requirements. We formulate the QoI-aware INP problem as a non-linear optimization problem to identify the optimal degree of data compression at each sensor node subject to satisfying a QoI requirement for the end-user. The formulation arranges all involved sensor nodes in a tree where data is transfered and processed from nodes to their parent nodes toward the root node of the tree. Under the assumption of uniform parameter setting, we show that the processing tree can be collapsed into a linear graph where the number of nodes represents the node levels of the original processing tree. This represents a significant reduction in complexity of the problem. Numerical example are provided to illustrate the performance of the proposed approach.' as abstract.

there is a document named 'doc-x0013' that
  has 'Loss tomography using multicast and unicast measurements have been investigated separately. In this paper we compared the performance of loss tomography using multicast and unicast on tree structures. We proved identifiability of unicast measurements on tree structures with no 2-degree nodes. To theoretically compare multicast and unicast, we built an observation model for multicast on trees and developed expressions for calculating the Cramer-Rao bound \' . We applied measurement design for unicast in trees and developed a simpler solution than our earlier work. Using a packet level simulator, we evaluated and compared the per link MSE of multicast and unicast under varying parameter settings include link weights, link loss rate distribution and size of the tree. The results show that in contrast to general belief that multicast outperforms unicast, unicast can outperform multicast under tight constraint on probing overhead, especially in terms of a weighted average of per-link MSEs. Meanwhile, multicast achieves more consistent performance wrt varying link success distribution or tree size.' as abstract.

there is a document named 'doc-x0014' that
  has 'The heterogeneous and dynamic nature of tactical coalition networks poses several challenges to common network management tasks, due to the lack of complete and accurate network information. In this paper, we consider the problem of redeploying services in mobile tactical networks. We propose M-iTop, an algorithm for inferring the network topology when only partial information is available. M-iTop initially constructs a virtual topology that overestimates the number of network components, and then repeatedly merges links in this topology to resolve it towards the structure of the true network. We perform extensive simulations and show that M-iTop enables an efficient redeployment of services over the network despite the limitation of partial information.' as abstract.

there is a document named 'doc-x0015' that
  has 'In-network processing (INP) is a promising method that allows aggregation of data while it is being transferred along the communication paths as a means to optimize the utilization of network resources without violating the quality of information (QoI) requirements. Given the large amount of data existing in dynamic environments, the optimization of INP requires a distributed framework that can adapt easily to network changes and user requirements. In this work, we develop the principle for designing a distributed mechanism in order to determine and control INP. Specifically, the proposed framework can decide, in a distributed way, which nodes along the communication paths optimally perform INP, with consideration of operational energy consumption and QoI requirements for achieving global optimal INP. The significance of the proposed distributed method is that it requires each node to make independent decisions locally for data aggregations, thus naturally enhance robustness and efficiency against network and data load dynamics. Extensive numerical results are presented to confirm the validity of the proposed approach.' as abstract.

there is a document named 'doc-x0016' that
  has 'We consider the problem of controlling mobile data ferries for message delivery among disconnected, scattered domains in a highly partitioned network. Existing work on data ferry control mostly focuses on predetermined ferry routes, assuming full observations at the ferry and no explicit Quality of Service (QoS) constraints on the resulting communications. In this paper, we aim at designing a QoS-enabled ferry control solution, which handles both partial observations and bounded message delays. To this end, we extend our previous work on data ferry control with partial observations into a comprehensive hierarchical framework called Switch-and-Navigate (SAN), which consists of a global switch policy for determining the best domain to visit and a local navigation policy per domain for searching for nodes within individual domains. Under the assumption of Markovian node mobility, both the global and the local control problems are formulated as Partially Observable Markov Decision Processes (POMDPs) to maximize the discounted effective throughput over all domains. Due to the fact that the optimal solution to POMDP is PSPACE-hard, we develop heuristic policies and further approximations for efficient computation. Simulation results show that the proposed policies can significantly improve the performance over predetermined alternatives.' as abstract.

there is a document named 'doc-x0017' that
  has 'We consider the problem of disseminating data from a base station to a sparse, partitioned mobile network by controllable data ferries with limited ferry-node and ferry-ferry communication ranges. Existing solutions to data ferry control mostly assume the nodes to be stationary, which reduces the problem to designing fixed ferry routes. In the more challenging scenario of mobile networks, existing solutions have focused on single-ferry control and left out an important issue of ferry cooperation in the presence of multiple ferries. In this paper, we jointly address the issues of ferry navigation and cooperation using the approach of stochastic control. Under the assumption that ferries can communicate within each partition, we propose a hierarchical control system called Dispatch-and-Search (DAS), consisting of a global controller that dispatches ferries to individual partitions and local controllers that coordinate the search for nodes within each partition. Formulating the global and the local control as Partially Observable Markov Decision Processes (POMDPs), we develop efficient control policies to optimize the (discounted) total throughput, which significantly improve the performance of their predetermined counterparts in cases of limited prior knowledge.' as abstract.

there is a document named 'doc-x0018' that
  has 'We investigate fundamental characteristics of cooperative transmission in terms of power efficiency. By introducing the concept of "cooperative region", we evaluate the average power efficiency which is defined as the ratio of total consumed transmit power with cooperation to that of direct transmission and show how the average performance depends upon the QoS requirement, distance between source and destination and on node density. Further, we propose a dynamic cooperation scheme that combines both cooperative and direct transmission. Analytical results are supplemented by simulation results to demonstrate the energy saving of cooperation transmission.' as abstract.

there is a document named 'doc-x0019' that
  has 'We seek to support communications in highly-partitioned mobile wireless networks via controllable data ferries. While existing ferry control techniques assume either stationary nodes or complete ferry observation of node locations, we address the more challenging scenario of highly mobile nodes and partial ferry observations. Using the tool of Partially Observable Markov Decision Processes (POMDP), we develop a comprehensive framework where we expand the solution space from predetermined trajectories to policies that can map ferry observations to navigation actions dynamically. Under this framework, we present an optimal and several efficient heuristic policies. We compare the proposed policies with predetermined control through analysis and simulations with respect to multiple node mobility parameters including speed, locality, activeness, and range of movement. The comparisons show a significant performance gain of up to twice the contact rate in cases of high uncertainty. In cases of low uncertainty, we give a sufficient condition under which predetermined control is optimal.' as abstract.

there is a document named 'doc-x0020' that
  has 'Because of their importance in military and other applications, Mobile Ad-Hoc Wirless Networks or MANETs have attracted significant attention in the research community.  However, almost virtually all of the literature has focused on analysing MANETs in an asymptotic case with a very large number of nodes under varying levels of node density and distribution.  While the asymptotic analysis is extremely valuable, practical usage of MANETs requires us to be able to analyse networks of finite size.  In this paper, we present an approach to analyse MANETs with a fixed number of nodes which can be used in many practical applications related to MANETs.  Our approach is based on simplifying the motion paths of a MANETs by applying a set of transformations, and decomposing the motion paths into a generalised Fourier series of simpler periodic motions.' as abstract.

there is a document named 'doc-x0021' that
  has 'This paper studies the scaling laws of the data gathering capacity of large scale multihop wireless networks. Unlike the data communication paradigms studied in previous research, for example, the many-to-many, many-to-one, broadcast, and multicast paradigms, the data gathering capacity concerns the per source node throughput in a network where a subset of nodes send data to some designated destinations while other nodes serve as relays. This someto-some communication paradigm is commonplace in many wireless networks, for example, wireless mesh networks and wireless sensor networks, and in some cases perhaps more prevalent than the other paradigms. We first derive the upper and constructive lower bounds for the data gathering capacity, and then examine their design and performance implications. Our results show that the data gathering capacity is constrained by different factors in several different scaling regimes of the number of source and destination nodes, exhibiting distinct scaling laws in those regimes. This work fills a gap in our understanding of the capacity of various communication paradigms, and can lead to better network planning and performance for data gathering wireless network applications.' as abstract.

there is a document named 'doc-x0026' that
  has 'Information is critical in almost all decision making processes. Therefore, it is important to get the right information at the right time from the right sources. However, information sources may behave differently while providing information- i.e., they may provide unreliable, erroneous, noisy, or misleading information deliberately or unintentionally. Motivated by this observation, in this paper, we propose a statistical information fusion approach based on behavior estimation. Our approach transforms the conveyed information into more useful form by tempering them with the estimated behaviors of sources. Through extensive simulations, we have shown that our approach has a lower computational complexity, and achieves significantly low behavior estimation and fusion errors.' as abstract.

there is a document named 'doc-x0027' that
  has 'Understanding the reasoning of others is a key aspect to achieving a shared understanding when collaboratively solving a problem, such as the generation of a plan, and recent observations of military planners suggest that it plays a key role in the planning process. An example of rationale is described where a misunderstanding is only resolved by the joint exploration and cross-challenging of the rationale. A prototype tool is described that permits the creation and visualization of the basic rationale via the use of a Controlled Natural Language derived from Common Logic Controlled English. Using the example, the paper explores mechanisms that could potentially make more effective use of rationale.' as abstract.

there is a document named 'doc-x0028' that
  has 'This paper presents the extension and evaluation of a formal representation that enables planners at different levels of command, and in different functional area, to jointly share, develop, and modify plans. Planning has moved from a co-located, concurrent, small team activity to an activity that involves a large, culturally diverse, hierarchical, globally-distributed team. However, significant benefits of distributed planning can only come if the team is able to communicate and maintain a shared understanding of the commander\'s intent, objectives, resources and constraints, as well as decisions made and justifications for planning options chosen or rejected. Effective automation must support the collaborative planning process itself, rather than just the artifacts it produces. The Collaborative Planning Model (CPM) is an ontology developed to support military planning by representing not only goals, plans, and constraints, but also the human rationale that support the decisions made, and alternatives rejected while creating the plan. Over the course three years, multiple evaluations of the CPM have been conducted culminating in a unifying evaluation of the CPM in a distributed, cross-UK-US hierarchical planning exercise. This evaluation has highlighted potential challenges that must be met when achieving shared understanding in more complex multi-level collaborative planning, including issues of representational semantics, rationale, configuration management, visualization utilizing context and filtering, plan interoperability, and interfaces.' as abstract.

there is a document named 'doc-x0035' that
  has 'We study the emergence of long-range connectivity in multilayer networks (also termed multiplex, composite, and overlay networks) obtained by merging the connectivity subgraphs of multiple sparsely activated instances of an underlying backbone network. Multilayer networks have applications ranging from studying communication or social networks formed with hybrid technologies, a transportation network connecting the same cities via rail, road and air, outbreaks of flu epidemics involving multiple viral strains, temporal flow of information in dynamic networks, and potentially conductivity properties of graphene-like stacked lattices. For a homogenous multilayer network- formed by merging M random site-percolating instances of the same graph G with single-layer site-occupation probability q-we show that when q exceeds a threshold qc(M) = (1= p M), a spanning cluster appears in the multilayer network [1]. Using the configuration model based approach [4], we find qc(M) exactly for random graphs with arbitrary vertex-degree distributions. For multilayer percolation in a general graph G, we show that qc= p M < qc(M) < p _ln(1 _ pc)= p M; 8M 2 Z+, where qc and pc are the site and bond percolation thresholds of G, respectively. The derivations of the above bounds show that multilayer percolation is a bridge between pure-site (single layer) to pure-bond (many layers) percolation. In fact, we show that multilayer percolation is very closely related to mixed (site-bond) percolation [2], where the former has slightly more nearest-neighbor bond correlations as compared to classical site-bond percolation. Using this connection, we find a close upper bound to the multilayer percolation thresholds. Furthermore, using an analytical approximation to the site-bond critical line [3], which depends only on the site and bond percolation thresholds of the underlying graph, we find an excellent approximation to the layered percolation thresholds for regular lattices. We report several exact results (via numerical simulations using variations of the Newmann-Ziff technique [5]). Finally, we find a rigorous analytical lower bound to the multilayer thresholds for the layered Kagome lattice [1] using a site-to-bond transformation technique developed by Scullard [6], which leverages the duality of the triangular and honeycomb lattices to compute the critical surface for any correlated bond percolation process on the triangular lattice where the correlations are limited to within each triangular face.' as abstract.

there is a document named 'doc-1002' that
  has 'Despite several research studies, the effective analy- sis of policy based systems remains a significant challenge. Policy analysis should at least (i) be expressive (ii) take account of obligations and authorizations, (iii) include a dynamic system model, and (iv) give useful diagnostic information. We present a logic-based policy analysis framework which satisfies these requirements, showing how many significant policy-related prop- erties can be analysed, and give details of an implementation.' as abstract.

there is a document named 'doc-1009' that
  has 'In wireless networks, the broadcast nature of wireless transmission enables cooperation by sharing the same transmissions with nearby receivers and thus can help improve spatial reuse and boost network throughput along a multi-hop routing. The performance of wireless networks can be further improved if prior information available at the receivers can be utilized to achieve perfect interference subtraction. In this paper, we investigate performance gain on network throughput for wireless cooperative networks by using a simple MUD scheme, called overlapped transmission, in which multiple transmissions are allowed only when the information in the interfering signal is known at the receiver. It is shown that the scheme of cooperative transmission with overlapping increases network throughput by 24% compared to that of direct transmission with overlapping. We then propose a new cooperation scheme called supplementary cooperation, which improves the performance gain of direct transmission with overlapping by 42%. Analytical results are developed to show that in a general network scenario, supplementary cooperation achieves bit error rate (BER) reduction of 34.87%, compared with the conventional cooperative transmission. Furthermore, we proposed a criterion for finding the best cooperative route to achieve maximum network throughput in a general network.' as abstract.

there is a document named 'doc-1010' that
  has 'Neighbor discovery1 is one of the first steps in the initialization of a wireless ad hoc network. In this paper, we design and analyze practical algorithms for neighbor discovery in wireless networks. We first consider an ALOHA-like neighbor discovery algorithm in a synchronous system, proposed in an earlier work. When nodes do not have a collision detection mechanism, we show that this algorithm reduces to the classical Coupon Collector\'s Problem. Consequently, we show that each node discovers all its n neighbors in an expected time equal to ne(ln n   c), for some constant c. When nodes have a collision detection mechanism, we propose an algorithm based on receiver status feedback which yields a ln n improvement over the ALOHA-like algorithm. Our algorithms do not require nodes to have any estimate of the number of neighbors. In particular, we show that not knowing n results in no more than a factor of two slowdown in the algorithm performance. In the absence of node synchronization, we develop asynchronous neighbor discovery algorithms that are only a factor of two slower than their synchronous counterparts. We show that our algorithms can achieve neighbor discovery despite allowing nodes to begin execution at different time instants. Furthermore, our algorithms allow each node to detect when to terminate the neighbor discovery phase.' as abstract.

there is a document named 'doc-1013' that
  has 'In this paper, we consider the problem of transmission delay in terms of finite coding length derived from the random coding bound for different cooperative protocols. Specifically, we first study the impact of cooperative transmission on the routing decision for wireless ad-hoc networks, where a routing optimization problem is formulated to minimize the endto-end delay that ensures a satisfactory error performance. The closed-expression of the optimal solution is developed through the optimization problem and later used as quantitative criterion of routing decision. Furthermore, consider the interference impact on system performance along a multi-hop routing, we then investigate performance gain on transmission delay for wireless cooperative networks by using a simple multi-user detection scheme, called overlapped transmission, in which multiple transmissions are allowed only when the information in the interfering signal is known at the receiver. As a result, both analytical and numerical results demonstrate the significant improvement on the system performance by using cooperative transmission with overlapping as well as the trade-off between the end-to-end delay and network throughput.' as abstract.

there is a document named 'doc-1014' that
  has 'In this correspondence, we study the application of network coding to opportunistic scheduling for wireless uplink channels. The key idea proposed in this paper is to always schedule the user with best channel gain for transmission, and meantime the use of network coding encourages the scheduled users to help the ones which have not been served previously. Analytical and numerical results have been developed to show that the proposed network coding schedulers can achieve better tradeoff of fairness and system throughput than comparable schemes.' as abstract.

there is a document named 'doc-1016' that
  has 'Due to technology or policy constraints, communications across domains usually require the intervention of gateways, and their proper deployment is crucial to the overall system. This paper addresses the placement of static gateways in mobile DTNs. Given a limited gateway budget, the problem is to allocate this budget among selected locations such that certain performance metrics are optimized. The challenge is that different domains may possess heterogeneous properties, which have to be considered jointly to optimize the end-to-end performance. To this end, a unified gateway deployment framework is proposed, which separates the domain-specific utility computation from the high-level gateway placement. To obtain an efficient solution, quadraticcomplexity suboptimal algorithms are proposed together with utility decomposition and analysis. Based on a classification of intra-domain routing schemes, closed-form analytical expressions are derived to calculate the utilities per domain, which are shown to approximate the actual performance metrics closely under Poisson contact processes while being robust against the contact distribution. In particular, evaluation based on the contact traces of a real DTN shows that the calculated utility is within 10% of the actual value after constant scaling, and the final deployment performs as well as that from a brute-force optimization based on simulated utilities. Compared with random deployment, the proposed strategies improve performance by up to 30%.' as abstract.

there is a document named 'doc-1017' that
  has 'With ever increasing number of diverse routing protocols proposed to deal with network dynamics in MANETs, the heterogeneity of MANETs has increased dramatically. While many of these proposals only concern enhancing routing in a single domain, little attention has been given to the interoperations among the heterogeneous MANETs. Moreover, the existing interdomain routing protocols (i.e., BGP) designed for the Internet cannot cope with the challenges of MANETs. In this paper, we propose a self-organizing inter-domain routing protocol for heterogeneous MANETs. The goal of this paper is to provide seamless inter-domain routing across different MANETs, which can adapt to the dynamic topology changes. We carefully examine the practical issues of inter-domain routing in MANETs and design our protocol to support opaque interoperations among heterogeneous MANET domains. In addition, our proposed protocol performs a fully distributed dynamic gateway assignment mechanism which significantly improves the performance (i.e., up to 200% improvement compared to a static mechanism in the sparse networks) by adaptively assigning gateways in response to topology changes. We also prototype the our protocol in a network simulator and extensively evaluate its performance under realistic MANET scenarios.' as abstract.

there is a document named 'doc-1019a' that
  has 'In this paper, we propose a novel packet forwarding scheme based on network coding that is resilience to jamming attack in a wireless ad hoc network. In a tactical field, wireless communication is prevailed between military agents and vehicles, but it is fragile by jamming attack from an adversary because of the wireless shared medium. Jamming attack is easily achieved by emitting continuous radio signal and it can interfere with other radio communications within the network. Channel switching over multiple channels or route detouring have been proposed to restore communication from jamming attacks, but they require a special radio system or knowledge of network topology. Our scheme exploits packet redundancy in network coding. It dynamically changes the level of redundancy adapting to channel condition locally and thus injects redundant encoded packets when and where jamming attack occurs. In normal situation, it decreases a forwarding rate to save resources so that our protocol efficiently manages the network resources. We provide performance evaluations of resiliency and efficiency of the new scheme via simulation study.' as abstract.

there is a document named 'doc-1020' that
  has 'We study one common class of processes that frequently take place on top of various types of networks, seen in application domains as diverse as biology, social sciences and computer science. We refer to this class of processes as collective behavior processes. In an abstract description, these processes dictate the state of a networked system, under the assumption that the state of each individual node is a function of its intrinsic behavior as well as a function of the state of its neighboring nodes. We model them as Markov processes and we derive approximate analytical results for the stationary distribution of states, both for Erdos-Renyi random graphs and for random geometric graphs. Inspired by the behavior characteristics of these processes, we have designed a novel duty-cycling protocol for wireless sensor networks and we evaluate its performance under various system parameters.' as abstract.

there is a document named 'doc-1021' that
  has 'The management and operations of distributed computer systems and networks is a complex labor-intensive task that accounts for the lion\'s share of costs in current computing environment [1]. A significant component of the complexity arises from the heterogeneity that exists in the computing infrastructure. The presence of a variety of different types of machines in a computing environment creates challenges related to the configuration, error diagnosis and repair of the infrastructure that is put into place. In many environments, e.g. in the case of military networks, a variety of instruments implies the need to carry batteries and power equipment, which increases dead weight and increase power consumption.' as abstract.

there is a document named 'doc-1023' that
  has 'We propose an efficient method to evaluate historybased policies written as logic programs. To achieve this, we dynamically compute, from a given policy set, a finite subset of the history required and sufficient to evaluate the policies. We maintain this history by monitoring rules and transform the policies into a non history-based form. We further formally prove that evaluating history-based policies can be reduced to an equivalent, but more efficient, evaluation of the non history-based policies together with the monitoring rules.' as abstract.

there is a document named 'doc-1024' that
  has 'Research in pervasive and autonomic computing focuses on supporting services for pervasive applications, but often ignores how such applications can be realised through the federation of autonomous entities. In this paper we present a methodology for designing collaborations between autonomous components, using the Self-Managed Cell (SMC) framework. We focus on the structural, task-allocation and communication aspects of management interactions between SMCs. This allows us to specify the management of large-scale systems by composing management functions as building block abstractions of an interaction.' as abstract.

there is a document named 'doc-1028' that
  has 'In both commercial and defense sectors a com- pelling need is emerging for rapid, yet secure, dissemination of information to the concerned actors. Traditional approaches to information sharing that rely on security labels (e.g., Multi- Level Security (MLS)) suffer from at least two major drawbacks. First, static security labels do not account for tactical information whose value decays over time. Second, MLS-like approaches have often ignored information transform semantics when deducing security labels (e.g., output security label = max over all input security labels). While MLS-like label deduction appears to be conservative, we argue that this approach can result in both underestimation and overestimation of security labels. We contend that overestimation may adversely throttle information flows, while underestimation incites information misuse and leakage. In this paper we present a novel calculus approach to securely share tactical information. We model security metadata as a vector half-space (as against a lattice in a MLS-like approach) that supports three operators: _,   and _. The value operator _ maps a metadata vector into a time sensitive scalar value. The operators   and _ support arithmetic on the metadata vector space that are homomorphic with the semantics of information transforms. We show that it is unfortunately impossible to achieve strong homomorphism without incurring exponential metadata expansion. We use B-splines (a class of compact parametric curves) to develop concrete realizations of our metadata calculus that satisfy weak homomorphism without suffering from meta- data expansion and quantify the tightness of values estimates in the proposed approach.' as abstract.

there is a document named 'doc-1029a' that
  has 'We demonstrate an extension of hierarchical identity-based encryption (HIBE) from the domain of a single Trusted Authority (TA) to a coalition of multiple independent Trusted Authorities with their own hierarchies. Coalitions formed under such schemes may be temporary or dynamic in membership without compromising security. In this paper we give an instantiation with formal security model and a proof of security against selective-identity chosen-plaintext attacks in the standard model based upon the difficulty of solving the Bilinear Decisional Diffie-Hellman (BDDH) problem.' as abstract.

there is a document named 'doc-1030' that
  has 'Wildcard identity-based encryption (IBE) provides an effective means of communicating among groups which do not have a well-defined membership or hierarchy pre-established, as may frequently be the case in dynamic coalition operations. The protection of group communication against compromised nodes is, however, expensive in that it typically requires frequent re-keying in the case of attribute-based IBE or voting-based revocation mechanisms, which can be problematic in multi-hop adhoc networks. In this paper we investigate the use of asymmetric communication links such as may be provided by unmanned aerial vehicles to provide efficient revocation mechanisms for small ad-hoc networks. Such link characteristics allow the efficient maintenance and propagation of blacklists as proposed by Saxena et al. and also enable the development of probability and plausibility metrics for revocation requests. We therefore propose a revocation scheme making use of both mobility and asymmetric communication channels to enhance both the robustness and efficiency of the scheme over the symmetrical case.' as abstract.

there is a document named 'doc-1031' that
  has 'Secure and un-jammed physical-layer communica- tion over a wireless link can be practically achieved using spread- spectrum techniques (eg: direct-sequence or frequency-hopping spread spectrum); however, these require a prior exchange of a spreading code. In many hostile and coalition military environments such a pre-exchange of codes between all the nodes may not be possible, thereby, requiring an on-demand exchange of a secret code (also referred as secret key) between two legitimate nodes seeking communication. In this paper, we study channel-reciprocity based secret-key exchange under an active jamming adversary. We analyze the performance of this frame- work and provide fundamental information-theoretic bounds on its efficiency, measured as the communication overhead per bit of generated secret-key.' as abstract.

there is a document named 'doc-1032' that
  has 'Key management is perhaps the most complex and most vulnerable part of any cryptographic implementation. To date key generation and activation have been extensively studied in the context of mobile ad hoc and wireless sensor networks. However, a dearth of research exists in designing techniques for key deactivation (revocation) and even less so for key reactivation. In this paper1 we study key-revocation schemes that are well-suited for the ad-hoc network environment. Specifically, we present a novel scheme with the following characteristics: Distributed: Our scheme requires no permanently available central authority. Active: A sufficient number of selfish honest nodes are incentivised to revoke malicious nodes. Secure: The scheme is secure against a large number of malicious nodes (30% of the network for an IDS-error rate of 15%). IDS-error tolerant: Revocation decisions are based on IDS. Our scheme is active for any meaningful IDS (IDS error rate < 0.5) and secure for an IDS error rate of up to 29%. Several schemes in the literature have 2 of the first 3 properties (property number 4 is typically not explored), but this work is the first to possess all four. This makes our revocation scheme well-suited for environments such as ad hoc networks, which are very dynamic, have significant bandwidth-constraints, and where many nodes are vulnerable to compromise.' as abstract.

there is a document named 'doc-1033' that
  has 'Key management is perhaps the most complex and most vulnerable part of any cryptographic implementation. To date key generation and activation have been extensively studied in the context of mobile ad hoc and wireless sensor networks. However, a dearth of research exists in designing techniques for key deactivation (revocation) and even less so for key reactivation. In this paper1 we study key-revocation schemes that are well-suited for the ad-hoc network environment. Specifically, we present a novel scheme with the following characteristics: Distributed: Our scheme requires no permanently available central authority. Active: A sufficient number of selfish honest nodes are incentivised to revoke malicious nodes. Secure: The scheme is secure against a large number of malicious nodes (30% of the network for an IDS-error rate of 15%). IDS-error tolerant: Revocation decisions are based on IDS. Our scheme is active for any meaningful IDS (IDS error rate < 0.5) and secure for an IDS error rate of up to 29%. Several schemes in the literature have 2 of the first 3 properties (property number 4 is typically not explored), but this work is the first to possess all four. This makes our revocation scheme well-suited for environments such as ad hoc networks, which are very dynamic, have significant bandwidth-constraints, and where many nodes are vulnerable to compromise.' as abstract.

there is a document named 'doc-1035' that
  has 'In this paper, we show how knowledge representation and reasoning techniques can support sensor-mission assignment, proceeding from a high-level specification of information requirements, to the allocation of assets such as sensors and platforms. In our previous work, we showed how assets can be matched to mission tasks by formalising the military missions and means framework in terms of an ontology, and using this ontology to drive a matchmaking process derived from the area of semantic Web services. The work reported here extends the earlier approach in two important ways: (1) by providing a richer and more realistic way for a user to specify their information requirements, and (2) by using the results of the semantic matchmaking process to define the search space for efficient asset allocation algorithms. We accomplish (1) by means of a rule-based representation of the NIIRS approach to relating sensed data to the tasks that data may support. We illustrate (2) by showing how the output of our matching process can drive a well-known efficient combinatorial auction algorithm (CASS). Finally, we summarise the status of our illustration-of-concept application, SAM (Sensor Assignment to Missions), and discuss various roles such an application can play in supporting sensormission assignment.' as abstract.

there is a document named 'doc-1043a' that
  has 'Developing and resourcing coalition plans require an understanding of the policy and resource availability constraints under which coalition members operate. Policies and resource availability information are not necessarily public knowledge within the coalition. Also, it is difficult to keep track of the policies and resource availabilities of others. What is required is agent support for keeping track of who might have and be willing to provide the resources required for enacting a plan and modeling the policies of others regarding resource use, information provision, etc. We propose a technique that combines machine learning and argumentation for identifying and modeling the policies of others and advising on how a plan may be resourced using this model. Also, we present the results of the initial evaluation of this model.' as abstract.

there is a document named 'doc-1044a' that
  has 'To support coalition teams, we are investigating the use of software agents that can help in planning team activities, thus reducing the cognitive burden on team members. Since the timely delivery of information can be crucial to team performance, we are especially interested in creating plans than explicitly include communications between team members. In previous work we developed a formal model of planning and communication that can support the creation of such plans. Here we describe how to implement this formal model, in particular how to map our model into the language of binary decision diagrams (BDDs), a representation of states and actions for which there are efficient open-source planners.' as abstract.

there is a document named 'doc-1046a' that
  has '"The best laid plans seldom survive contact with the enemy" is a well known military adage. The unpredictability of the physical environment (e.g. weather), opponent activity, and the uncertain and dynamic effects of this activity on friendly outcomes lead to deviations from planned activities during execution. This phenomenon may be exacerbated when warfighters are from different coalition partners and have not previously cotrained. We are conducting research towards the development of software agents that will track the activities of human teams and monitor their plan execution. The agents will be able to recognise deviations from pre-planned team activities and offer advice that would help humans to avoid or resolve problems, take advantage of presented opportunities, and in general maintain the synchronisation among the different plan components.' as abstract.

there is a document named 'doc-1047a' that
  has 'When human teams collaborate they ensure that all team members understand their current context by communicating both verbally and nonverbally. Military teams are trained to work together and their behaviors often follow learned patterns. If these patterns were observed and recognized, they would provide evidence of a team\'s context, and context could be used to assist the team by anticipating needed support and by disambiguating requests for information. Soldiers from the United Kingdom participated in a series of simulated missions in which their speech and actions were recorded. Analyzing these behaviors revealed patterns related to location and movement, weapon use, and team communication. Both when stationary and when moving the soldiers often followed patterns that are described in field manuals. When suppressing an enemy location they fired in a controlled systematic pattern. There were many patterns observable in their spoken communication related to speech acts, use of metaphors, status reports, and other topics. Simple heuristics were defined for potentially recognizing most of the identified patterns. The heuristics for recognizing location and movement patterns depend primarily on speed of movement, dispersion, and linearity of positions. Measures for these three concepts were defined and calculated for one mission. The values of these concepts appear to vary systematically and accurately reflect the events that occurred in the simulation. Measurements of these concepts may provide a means of identifying and interpreting to some extent the changes in a team\'s context.' as abstract.

there is a document named 'doc-1048a' that
  has 'Intelligent assistance to a team engaged in a military mission requires understanding the team\'s current context. A manual method for constructing models of mission context was constructed as a precursor for development of automatic methods. United Kingdom soldiers participated in a series of simulated missions while their voice communication, location, and related information were recorded. Independent analysts modeled two of the missions. These independently constructed models were compared to evaluate and improve the methods.' as abstract.

there is a document named 'doc-1049a' that
  has 'People working together can accomplish tasks and solve problems that far exceed the capabilities of a single individual. In this paper we consider how people collaborate when solving problems and the role of networks in supporting this collaboration. We begin by considering the cognitive processes and mental representations involved in problem solving by a single person. When people collaborate, these cognitive processes and representations extend across all participants, and we consider how these processes are replicated, shared, and/or allocated. Finally we consider the role of both human networks and the networked technologies that support collaborative problem solving. Network properties such as bandwidth and latency can influence how problems are solved and both the quality and timing of solutions. Our objectives are to provide a theoretical framework for network enabled collaborative problem solving and to suggest a research program for improved problem solving performance.' as abstract.

there is a document named 'doc-1052' that
  has 'Rationale is being explored by the ITA in planning, for shared understanding of plans, and for capturing dependencies between assumptions/decisions and parts of the plan. Informal rationale captured during evaluation of the Collaborative Planning Model has been explored in terms of alternative visual representations and questions that these raise.' as abstract.

there is a document named 'doc-1053' that
  has 'Controlled Natural Languages (CNL) have been proposed for representing information about a cognitive artifact, such as a plan, in a way that a human can more easily understand and communicate the artifact. This paper proposes the use of the flexible definition of layers of "high level" CNLs to evolve more expressive power specific to a domain but which can be transformed both logically and linguistically into a lower level generic CNL for semantic processing.' as abstract.

there is a document named 'doc-1054' that
  has 'Shared understanding is commonly seen as essential to the success of coalition operations, and current research efforts are attempting to develop techniques and technologies to improve shared understanding in military coalition contexts. In spite of this, our understanding of what the term \'shared understanding\' actually means is surprisingly poor. In part, this problem is attributable to the difficulty in comprehending the true nature of understanding itself, although confusions also arise about the precise nature of the differences between shared understanding and ostensibly similar constructs, such as shared mental models and shared situation awareness. In this paper, we attempt to improve our understanding of shared understanding by exploring the nature of understanding, situation awareness and mental models. Following Wittgenstein, we suggest that understanding is best conceived of as something akin to an ability,and shared understanding is, we suggest, best conceived of as the sharing of individual forms of understanding by multiple agents. We further suggest that mental models may provide a mechanistic realization for some of the performances that manifest understanding, and that situation awareness should best be seen as a particular kind of understanding, namely a dynamic form of situational understanding. In addition to discussing the nature of understanding and shared understanding, we also discuss their potential relevance to military coalition operations.We propose that shared understanding is important to coalition operations because it contributes to improvements in coalition performance, the optimal use of limited communication assets,and an improved sense of group cohesion, group solidarity and mutual trust.' as abstract.

there is a document named 'doc-1056' that
  has 'Semantic MediaWiki (SMW) is a popular semantic wiki engine that enables collective knowledge modeling within the expressivity of a small subset of OWL. However, SMW is limited in offering native support for modeling rules. In this paper, we show that by using templates and semantic queries provided by SMW, it is possible to model several types of rules within SMW, e.g., OWL entailment rules and logic programs. Such modeling practice enables us to perform some broadly useful inference tasks on SMW, e.g., integrity constraint checking.' as abstract.

there is a document named 'doc-1068' that
  has 'In a collaborative planning environment, many planners are working on a plan independently throughout the planning process. A unified semantic plan representation, such as Collaborative Planning Model (CPM), as well as a user-friendly Graphical Plan Authoring Language (GPAL), may have addressed the problems of potential semantic misunderstandings and plan visualization. However, given the complexity of a particular plan, it is still too difficult, if not impossible, for every single planer to understand all the details of the entire plan. In fact, each plan participant has its own role in planning process, and therefore may only need to understand a small portion of the plan, along with additional contextual or situational information that is not necessarily included in the plan itself. A planer usually has very specific preference (explicitly or implicitly defined) on what and how the plan, as well as the related contextual information, shall be rendered.' as abstract.

there is a document named 'doc-1136' that
  has 'This paper presents a lightweight probabilistic path authentication scheme for mobile ad hoc networks (MANETs) based upon a new cryptographic primitive composite MAC. The proposed path-authentication scheme allows us to reliably identify nodes on a route over which a sequence of packets traverses. This path-authentication scheme is robust against selfish or malicious nodes that do not follow the scheme. Furthermore, it allows us to detect, and up to a certain accuracy pinpoint, any misbehaving node that deviates from the correct forwarding behavior. In our scheme, composite MAC can have any length starting from one bit. This flexibility allows the proposed scheme to strike various trade-offs depending on the constraints imposed by the MANET and the desired security properties. We provide an informal security analysis and argue that a short MAC can be sufficient to authenticate paths with high probability.' as abstract.

there is a document named 'doc-1280' that
  has 'After nearly three decades of work on mobile ad-hoc networking we are starting to see a convergence of better radios and better understanding of performance needs for Mobile Ad Hoc Networking (MANET) routing schemes, delivering working networks. One part of the next stage of evolution of such systems will be to support the federation of different MANETs together, whether concatenated together or interleaved. In this paper, I present some initial thoughts on how one might start to tackle this interesting problem space, which appears to be rather more complex than the still contentious area of Inter-domain routing in the Internet which the creation of the Border Gateway Protocol (BGP) attempted to address.' as abstract.

there is a document named 'doc-1361c' that
  has 'New approaches to Quality-of-Service (QoS) Routing in wireless sensor networks which use different forms of learning are the subject of this paper. The Cognitive Packet Network (CPN) algorithm uses smart packets for path discovery, together with reinforcement learning and neural networks, while SelfHealing Routing (SHR) is based on an ant colony paradigm which emulates the pheromone based technique which biological ants use to mark paths and communicate information about paths between different insects of the same colony. In this paper we present first experimental results on a network testbed to evaluate CPN\'s ability to discover paths having the shortest delay, or shortest length. Then, we present small test-bed experiments and large-scale network simulations to evaluate the effectiveness of the SHR algorithm. Finally, the two approaches are compared with respect to their ability to adapt as network conditions change over time.' as abstract.

there is a document named 'doc-1384c' that
  has 'One of the challenges in a military wireless sensor network is the determination of an information collection infrastructure which minimizes battery power consumption. The problem of determining the right information collection infrastructure can be viewed as a variation of the network design problem, with the additional constraints related to battery power minimization and redundancy. The problem in its generality is NP-hard and various heuristics have been developed over time to address various issues associated with it. In this paper, we propose a heuristic based on the mammalian circulatory system, which results in a better solution to the design problem than the state of the art alternatives.' as abstract.

there is a document named 'doc-1417b' that
  has 'This paper reviews the coverage of formal Relational Algebra as it applies to distributed, federated databases in varying network topologies. The review shows that a number of Relational Algebra extensions allow distributed relations and federation of heterogeneous database schema. More concrete physical Relational Algebra extensions support access plans for multi-database query processing but lack cost functions dealing with specific network topologies such as scale-free networks, hyper-cubes and Kautz graphs. Statistic gathering techniques are highlighted which allow efficient distribution of database metrics, with the aim of providing optimized query processing.' as abstract.

there is a document named 'doc-1431b' that
  has 'This paper introduces the Global Interlinked Data Store (GIDS), a technique to support the easy creation and retrieval of interlinked semantic data within a web-scale distributed network environment such as the World Wide Web (WWW). By using the GIDS a web application developer can treat the network as a data store without worrying about files, databases or other traditional data storage concerns. The data that is created on the network can be subsequently accessed and navigated by end users and software agents alike. The GIDS proposes a novel three-stage data storage process which enables the data to be stored in up to three contextually relevant locations to enhance subsequent retrieval opportunities. We propose that the GIDS can provide a highly-scalable distributed capability to store and retrieve data directly on a network. We believe that the capability offered by the GIDS will be of significant use to rapidly formed diverse coalitions who wish to communicate and exchange semantic data in a large network environment such as the WWW. Based on commonly used Web standards, we have implemented the GIDS in a prototype which can be invoked via simple web service requests to read and write data as prescribed by the GIDS.' as abstract.

there is a document named 'doc-1445' that
  has 'The benefit of being able to combine, cooperatively, the antenna capabilities of a number of antenna nodes in order to create a beam formed antenna pattern is much regarded. Models and trials suggest that it is possible to achieve this gain with quite generous margins on position and phase accuracy and thus raise the possibility of being able to create adaptive beam formed antenna arrays in real-time from nodes as they move around at ground level. The challenges this raises are far more significant than those of some of the airborne models explored in the literature to date and provide a potential benefit to dismounted soldiers in locations where communications range may be severe problem.' as abstract.

there is a document named 'doc-1446' that
  has 'This paper frames the problems associated with the creation, communication, latency and capacity of ad-hoc distributed antenna arrays, in particular for the creation and support of cluster communication. Distributed antenna arrays are clusters of nodes in which utilisation of their individual resources is cooperatively combined to provide a more useful resource, for example to combine multiple transmission nodes to communicate long distances as a beamformed system. The applications of this approach are widespread and include military applications, both in the RF and acoustic domains, and in first responder and remote communication applications where the goal is to provide infrastructure links without the need for fixed backhaul capability. The paper looks at the issues involved and frames the problem of creating cooperative communication clusters.' as abstract.

there is a document named 'doc-1448a' that
  has 'As a fundamental research program, the International Technology Alliance (ITA) aims to explore innovative solutions to some of the challenges confronting US/UK coalition military forces in an era of network-enabled operations. In order to demonstrate some of the scientific and technical achievements of the ITA research program, we have developed a detailed military scenario that features the involvement of US and UK coalition forces in a large-scale humanitarian-assistance/disaster relief (HA/DR) effort. The scenario is based in a fictitious country called Holistan, and it draws on a number of previous scenario specification efforts that have been undertaken as part of the ITA. In this paper we provide a detailed description of the scenario and review the opportunities for technology demonstration in respect of a number of ITA research focus areas' as abstract.

there is a document named 'doc-1449' that
  has 'This paper attempts to provide an end-to-end walk- through of sensor-mission assignment techniques being developed in ITA Project 8, starting with information requirements, and ending with delivery of actionable information and intelligence. The walkthrough is situated within scenarios currently under development in TA3 and TA4. The paper has two aims: (1) to facilitate integration of P8 approaches, together with aspects of P9 and P12; (2) to align the P8 work with wider activities that use the TA3 and TA4 scenarios. The paper concludes with a list of open questions and issues.' as abstract.

there is a document named 'doc-1451' that
  has 'When a sensor network is deployed in the field it is typically required to support multiple simultaneous missions, which may start and finish at different times. Schemes that match sensor resources to mission demands thus become necessary. In this paper, we consider new sensor-assignment problems motivated by frugality, i.e., the conservation of resources, for both static and dynamic settings. In general, the problems we study are NP-hard even to approximate, and so we focus on heuristic algorithms that perform well in practice. In the static setting, we propose a greedy centralized solution and a more sophisticated solution that uses the Generalized Assignment Problem model and can be implemented in a distributed fashion. In the dynamic setting, we give heuristic algorithms in which available sensors propose to nearby missions as they arrive. We find that the overall performance can be significantly improved if available sensors sometimes refuse to offer utility to missions they could help based on the value of the mission, the sensor\'s remaining energy, and (if known) the remaining target lifetime of the network. Finally, we evaluate our solutions through simulations.' as abstract.

there is a document named 'doc-1453' that
  has 'The military environment presents special challenges for wireless mesh networking. In addition to optimal use of scarce radio resources, dependable network operation is critical: given frequent link and topology changes, rapid recovery of connectivity may be vital for mission success. This paper proposes a routing algorithm for wireless mesh networks with the primary goal of maximizing connectivity while limiting overhead. Rather than using one or more disjoint routes between a source and destination, a set of non-disjoint routes, or braid, is selected. To adapt to link changes, local rerouting is performed within the braid, thus avoiding network-wide recalculations. We analytically characterize the source-destination connectivity of the braid. Through simulation, we compare the reliability of braided routing and various other MANET routing protocols, including AODV, and quantify the relative amounts of control overhead incurred by braided routing and AODV.' as abstract.

there is a document named 'doc-1454a' that
  has 'In this paper, we study cost efficient multi-copy spraying algorithm for routing in Delay Tolerant Networks (DTN) in which a source-to-destination path does not exist most of the time. We present a novel idea and the corresponding algorithm for achieving the average minimum cost of packet transmission while maintaining the desired delivery rate by the given deadline. In the presented algorithm, the number of message copies in the network depends on the urgency of meeting the delivery deadline for that message. We find the parameters of efficient copying strategy analytically and validate the analytical results with simulations. The results demonstrate that our time dependent spraying algorithm achieves lower cost of message copying than the standard spraying algorithm while maintaining the desired delivery rate.' as abstract.

there is a document named 'doc-1455a' that
  has 'Coalition operations have become the norm for achieving military objectives in recent history. While providing the benefits of distributing costs and achieving community buyin, adding foreign cultures, expectations, and practices further exacerbate the difficulties associated with military planning and execution. In this paper we describe a number of these difficulties, and a Collaborative Planning Framework (CPF) designed to address these issues. The work performed to date on this framework, as well as future work is described.' as abstract.

there is a document named 'doc-1456a' that
  has 'Mission-critical scenarios, such as military or disaster response missions, often call for the formation of coalitions, made up of people from different countries or organizations and required to adhere to certain policies. These policies define the explicit obligations, permissions and prohibitions governing members of the coalition. While planning for joint action in these scenarios is already a complex problem for human planners, it is made more difficult or even impossible under such policy constraints, especially if policy conflicts exist between them. In this paper we propose that agents could be used to support human planners in coalitions, and present our work in the area of agent support for coalition mission planning under such policy constraints. We define a taxonomy of policies and outline the different types of support that agents can provide to human planners. We describe an experimental framework within which different types of agent support can be empirically evaluated within the context of a human planning problem.' as abstract.

there is a document named 'doc-1457a' that
  has 'Military commanders require precise command, control, and planning information available for a given mission, information that must be tailored for a particular area of operation, for a specific level of command, and for a specific time period. The problem of developing information of this kind is further complicated in a multi-national coalition setting where different components of a coalition plan are developed in semi- independent fashion, but then aggregated and composed to form an overall operational plan that is sufficiently flexible to support change as circumstances evolve. Furthermore, the modern information and communication technology and network-centric operations have opened up many opportunities for information dissemination and exchange. This increased availability of information often leads to planners having more information than they can reasonably digest. This information overload often makes it difficult for a planner/executor to find what they need and burdens them with the laborious task of separating the relevant from the irrelevant. To ease this burden, we envision an environment in which different planning collaborators will bring different contexts attributed by their rationale, situation awareness, understanding, etc. Planning artifacts will have to aggregate multiple contexts that went into the different steps of the plan. This paper will provide a foundation for context-aware and collaborative planning that will enable customized agents to traverse a diverse, distributed, frequently changing information space to identify relevant data. Once aware of the data, visual interfaces should provide the new information and facilitate understanding of changes among geographically distributed planners. As a first steps toward this vision we have developed a framework called Graphical Plan Authoring Language (G-PAL) that enables multiple distributed planners to collaboratively build plan components that can be composed later on to provide a global view of the plan.' as abstract.

there is a document named 'doc-1458' that
  has 'One of the challenges in processing information collected from a sensor network is to extract events of interest - usually activities that are uncommon and non-recurring. This problem is particularly hard when there is no a-priori knowledge of the nature of the events, and no signature patterns are known which can be compared to detect an event. In this paper, we propose a technique that can identify such events by observing that many such events will manifest themselves as occurring at different temporal scales. The method we propose is a nonparametric distribution based approach that uses multiple sliding windows to obtain the data distribution. We present techniques for determining the optimum values for bin width and window size for determining the distribution and the effect of the selected value on the choice of the range of scales. We believe this is the first work in the literature that combines multi-modal sensor data at different temporal scales to detect previously unknown abnormal events. We evaluate our technique using multi-modal data of human activities collected in an experiment at the US Army Research Laboratories (ARL). We show that analyzing sensor data at multiple temporal scales enables us to find uncommon non-recurring events of interest.' as abstract.

there is a document named 'doc-1459a' that
  has 'In wireless ad hoc sensor networks, nodes can possess only local information about their environment due to their limited range. Cooperation provides sensors with a broader range of information, often leading to an improvement in network\'s ability to meet its global objectives. However, increased communication caused by cooperation leads to a cost-quality tradeoff in the network operation. We model the cooperation based on the range of its information sharing. In h-cooperation, each node shares its information with those nodes that are at most h hops away. We analyze the tradeoff in three different applications and show that significant performance improvements arise when the optimal cooperation level is chosen.' as abstract.

there is a document named 'doc-1460a' that
  has 'We study target tracking with wireless binary sensor networks, in which each sensor can return only 1-bit information regarding target\'s presence or absence in its sending range. A novel, real-time and distributed target tracking algorithm for an imperfect binary sensing model is proposed, which is an extension of our previous work on the ideal binary sensing model.' as abstract.

there is a document named 'doc-1462a' that
  has 'This paper investigates properties of evolving networks based on a model of growth in which new networks are formed through the combination of existing ones. Networks are represented as graphs, and several binary graph operations (simple operations and products) are investigated. Two important network (or graph) properties, namely diameter and vulnerability, are studied. The utility of the combinational approach to growth lies in being able to predict or compute properties of a combined network in terms of the respective properties of the constituent graphs in the combination. The main aim of the paper is to demonstrate the viability and advantages of the combinational approach to the study of network diameter and vulnerability.' as abstract.

there is a document named 'doc-1463a' that
  has 'A three-phase fundamental research project is underway to determine the value of context-sensitive policy management technology, informed by algorithms based on cultural models and collaborative decision making, to coalition partners in mission planning and execution. Three project teams in TA2 and TA4 are collaborating and leveraging their research frameworks and current results in security policy lifecycle management, cultural analysis, and decision support in this effort. The paper describes the three technical foci, the potential value of their integration in a new perspective, a set of research questions, and the three-phase research approach of theoretical work, field interview research, and development of the system. The rationale and initial research reported in this collaboration may motivate and inform other collaborations within the ITA.' as abstract.

there is a document named 'doc-1466' that
  has 'In this paper, we propose a novel duty cycling algorithm for a large-scale dense wireless sensor networks. The proposed algorithm is based on a social behavior of nodes in the sense that individual node\'s sleep/wakeup decision is influenced by the state of its neighbors. We analyze the behavior of the proposed duty cycling algorithm using a stochastic spatial process. In particular, we consider a geometric form of neighborhood dependence and a reversible Markov chain, and apply this model to analyze the behavior of the duty cycling network. We then identify a set of parameters for the reversible spatial process model, and study the steady state of the network with respect to these parameters. We report that our algorithm is scalable to a large network, and can effectively control the active node density while achieving a small variance. We also report that the social behavior of nodes has interesting and non-obvious impacts on the performance of duty cycling. Finally, we present how to set the parameters of the algorithm to obtain a desirable duty cycling behavior.' as abstract.

there is a document named 'doc-1468a' that
  has 'Quality of information in a tactical sensor network is defined in terms of the extent to which the sensor network fills the requirements of command, and in its contribution to the picture of ground truth in operation. The latter definition can be constructed in terms of the capability of the sensor network, and regarded as a function that maps from a specific circumstance - or ground truth - to a representation of the meaning of the sensor network output in terms of probabilistic constraints on the locus of ground truths that could have led to that output. This QoI delivery function does not in itself inform the design process. We must go further and introduce mission specific bias by injecting mission specific priors in the calculation of output meaning, and synthesize a mission progress model that tracks the use of the information, and hence enables us to calculate the value of information provided by that sensor network during specific missions. We propose a Mission Abstraction Requirements Structure (MARS) that unifies the specification of command information as mathematical constraints with structuring of the stochastic transformations and abstractions required to transform sensing QoI capability to mission-specific value in assessment and optimization against requirements and economic constraints.' as abstract.

there is a document named 'doc-1470a' that
  has 'A sensor network comprises a collection of sensor nodes that can measure characteristics of their local environment, perform certain computations, and transmit the measurement result, typically in a collaborative fasshion, to an external data collection point for data processing and storage. The collected measurement result however often contain erroneous data due to inevitable system problems involving various hardware and software components ranging from the sensor device for data collection, to computation device for data fusion and processing, to communication device for data transmissions. Such "dirty data" are expected to be sporadic. In this research, our objective is to detect and repair such dirty data. Our approach is to leverage on the intrinsic redundancies and correlations among the collected data, as information about a single event of interest in a sensor network is usually reflected in multiple measurement data points. This data correlation can exhibit temporally, spatially, and across different data types. The inconsistency among multiple sensor measurements serves as an indicator for data quality problem. Furthermore, by carefully constructing a data model, we may be able to correct the dirty data in that data produced by one data source can serve as an error correction code for others. The focus of this paper is therefore to study methods that can effectively identify and correct errorneous data among inconsistent observations based on the correlation structure of various sensor measurement series. We propose a multivariate model to achieve this goal.' as abstract.

there is a document named 'doc-1471' that
  has 'We extend the existing network utility maximization (NUM) framework for wired networks to wireless sensor networks by formulating it in order to take into account interference among radio links. We study the conditions under which the formulated problem is a convex optimization problem with a feasible solution. Under such conditions, a distributed algorithm is proposed to solve the problem optimally. Finally, we provide numerical results, based on computer simulations, to show the performance of the proposed algorithm and the rate of convergence of its solution.' as abstract.

there is a document named 'doc-1472' that
  has 'We study blind estimation of transmission power of a node based on received power measurements obtained under wireless fading. Specifically, the setup consists of a set of monitors that measure the signal power received from the transmitter, and the goal is to utilize these measurements to estimate the transmission power in the absence of any prior knowledge of the transmitter\'s location or any statistical distribution of its power. Towards this end, we exploit spatial diversity in received-power measurements and cooperation among the multiple monitoring nodes; based on theoretical analysis we obtain the Maximum Likelihood (ML) estimate, derive fundamental geometrical insights and show that this estimate is asymptotically optimal. Finally, we provide numerical results comparing the performance of the estimators through simulations and on a data-set of field measurements.' as abstract.

there is a document named 'doc-1474a' that
  has 'New hardware and technologies enable low-power low-cost distributed sensing systems. To realize certain applications such as real-time event detection, target tracking, and system monitoring, time synchronization is essential. However, time synchronization both consumes limited battery power on the network nodes and can clog the bandwidth of the network. The choice of a time synchronization mechanism will depend on the application\'s requirement of timing accuracy as well as its energy budget. We derive the effect of time synchronization accuracy upon real time estimation and detection problems. We also quantify the costs and benefits of time synchronization algorithms. The intuitive assumption that using higher stability clocks will automatically improve duty cycling performance, and thus decrease power consumption, does not always hold true. In this article, we present the link between clock stability, impact on duty cycling, and the possible bandwidth savings that can be achieved by using temperature compensated clocks or clock drift estimation techniques. This paper formalizes this relationship based on an analytical framework using representative applications, namely, event detection and estimation. The analysis shows the impact of timing errors for different event durations, target speeds, number of sensors, and sampling frequencies. The analysis framework can also be used to estimate the maximum synchronization error each application can sustain while still achieving the desired Quality of Information (QoI).' as abstract.

there is a document named 'doc-1475' that
  has 'Much of the traffic carried by Sensor Networks will originate from routine measurements or observations by sensors which monitor a particular situation, such as the temperature and humidity in a room or the infrared observation of the perimeter of a house, so that the volume of routine traffic resulting from such observations may be quite high. When important and unusual events occur, such as a sudden fire breaking out or the arrival of an intruder, it will be necessary to convey this new information very urgently through the network to a designated set of sink nodes where this information can be processed and dealt with. This paper addresses the important challenge of avoiding that the volume of routine background traffic creates delays or bottlenecks that impede the rapid delivery of high priority traffic resulting from the unusual events. Specifically we propose a novel technique, the "Randomized Re-Routing Algorithm (RRR)", which detects the presence of novel events in a distributed manner, and dynamically disperses the background traffic towards secondary paths in the network, while creating a "fast track path" which provides better delay and better QoS for the high priority traffic which is carrying the new information. When the surge of new information has subsided, this is again detected by the nodes and the nodes progressively revert to best QoS or shortest path routing for all the ongoing traffic. The proposed technique is evaluated using a mathematical model as well as simulations.' as abstract.

there is a document named 'doc-1476a' that
  has 'It is hypothesized that plan rationale is key to collaborative planning, to support communication and negotiation of plans between human planners, especially when these planners come from different planning "cultures".  Progress has been made in Project 12, within the Collaborative Planning Model (CPM), on the capture, representation, analysis and display of the rationale for the existence of plan structures. However, such work has focused on the computational aspects of the calculation and use of rationale and it is important that human aspects be considered in the human use of rationale in these situations. This paper presents the computational aspects of the work, using a militarily-validated scenario, and proposes some human aspects that need to be considered in the next stage of the ITA work on the use of rationale in collaborative planning.' as abstract.

there is a document named 'doc-1478a' that
  has 'Pervasive and autonomic systems lack support for the design and specification of management collaborations between autonomous components. This paper advocates the use of a catalogue of architectural styles that can be composed to instantiate collaborations between autonomous entities, using the Self-Managed Cell framework. These architectural styles provide several useful abstractions for defining management relationships across SMCs.' as abstract.

there is a document named 'doc-1480' that
  has 'We consider the problem of broadcasting information from a source node to all nodes in a wireless network, using a local-control algorithm. For simplicity, we assume some oracle that provides the scheduling decisions required for interference. Under this assumption, we can show that a local control algorithm exists that achieves a stable system whenever the injection rate at the source is feasible for the network. We show results for the case of a single antenna and for multiple antennas operating independently.' as abstract.

there is a document named 'doc-1481a' that
  has 'The quality of computing certain aggregation functions based on incomplete measurements for the purpose of distributed network monitoring is considered. Network monitoring plays a fundamental role in network management systems by providing timely information on the network status, which is crucial for network administration purposes. To reduce network overhead and for easier assimilation, this information is usually presented by calculating a few key aggregate metrics. The aggregates are periodically computed from a large number of detailed events collected continuously during the course of the network operations. Under errors induced by network delays, the accuracy of typical aggregation functions used in network management systems (e.g., sum, average, maximum) is evaluated both analytically and by simulations. The results provide a quantifiable trade-off between accuracy and timeliness of the information acquired, which can then be used to design and optimize network management systems.' as abstract.

there is a document named 'doc-1483' that
  has 'A sensor network in the field is usually required to support multiple sensing tasks or missions to be accomplished simultaneously. Since missions might compete for the exclusive usage of the same sensing resource we need to assign individual sensors to missions. Missions are usually characterized by an uncertain demand for sensing resource capabilities. Consider for example a mission that requires video sensors to identify a target but the weather conditions and visibility range in the field are not exactly known: in this case the required number of sensors and the resolution of their cameras cannot be precisely determined. We can for example specify only the highest resolution of the cameras needed by the mission or the maximum number of video sensors required. If instead two missions require to identify two different targets that are located in nearby regions on the map, then these missions might compete for the exclusive control of a particular video sensor. Indeed the mission to which the video sensor will be assigned might decide to point the camera in a direction that could be completely opposite to where the other mission would require it.' as abstract.

there is a document named 'doc-1484a' that
  has 'The problem of detecting transient signals in additive white Gaussian noise in the presence of missing signal observations (samples) is considered. Specifically, a fusion center aims to detect the transient signals by collecting measurements from individual sensors through erasure channels. A quantifiable performance measure is proposed to analyze the impact of missing samples on the final decision, which is evaluated both analytically and numerically. The analysis serves as a first step towards understanding and mitigating the effect of incomplete observations on transient event detection.' as abstract.

there is a document named 'doc-1485a' that
  has 'For a sensor network to be reliable and useful, sensor data must sustain high Quality of Information (QoI). While QoI depends on many factors, the most crucial is the integrity of sensor data sources themselves. Sensor data quality may be compromised by causes such as noise, drifts, calibration, and faults. On-line detection and isolation of such misbehavior are crucial to assure high QoI for the end-user, and efficient management of network resources. We describe a two-tiered system for on-line detection of sensor faults. A local tier running at resource-constrained nodes uses an embedded model of the physical world with a hypothesis-testing detector to identify potential faults and notifies a global tier. In turn, the global tier uses these notifications for consistency checking among sensors and provides more robust estimates for events of interest, and also generates feedback to update the local models. We demonstrate the performance of our system by investigating its impact on the application QoI.' as abstract.

there is a document named 'doc-1489b' that
  has 'Cooperative diversity has recently emerged as a promising approach to improving reception reliability by realizing spatial diversity gains for nodes with single antenna. We consider here cooperative ad-hoc wireless networks where communications between two nodes can be assisted by a single relay using two time slots. This paper continues our investigation of PHY techniques and cross-layer routing algorithms in such networks. Specifically, we investigate here the optimal relay location for cooperative link in networks with infinite node density. By using this result, we analyze the upper-bound error performance for routing algorithms in the infinitely dense networks. Furthermore, we study the performance bounds for regularly dense networks with linear topology. Theoretical analysis shows that the proposed routing algorithm performs close to the optimal error performance.' as abstract.

there is a document named 'doc-1491a' that
  has 'We consider the problem of energy-efficient data fusion for hypothesis testing in sensor networks, where the spatial correlation structure of sensor measurements is modeled by a Markov random field and the neighborhood system is defined by means of a nearest-neighbor dependency graph. Two techniques that improve energy efficiency performance of an existing data fusion scheme are proposed and simulation results are provided.' as abstract.

there is a document named 'doc-1492a' that
  has 'In this paper we examine the Quality of Information (QoI) at the output of a wireless sensor network by considering the difference between the monitored environment and the interpreted data produced by the network. Using practical examples in an experimental setting, we hope to shed light on the concept of QoI and on the manner of estimating and evaluating it. We use a real wireless network of thirty-four Motes, in combination with simulated events, to help us formulate and understand the concept of QoI and its associated technical questions. Using algorithms such as trilateration and clustering to interpret the outputs of the sensor network, we explore several definitions of QoI, incorporating peak signal to noise ratio and the proportion of correctly detected events. Furthermore we investigate the impact that different packet transmission approaches have on the QoI and network power use. We show that QoI is timevarying, and that in-network processing allows QoI levels to be maintained while reducing network load.' as abstract.

there is a document named 'doc-1493' that
  has 'Providing reliable and efficient networking services in wireless ad hoc networks is very challenging due to high mobility and unstable wireless nature. Network coding (NC) and erasure coding (EC) are such coding schemes considered to be able to provide excellent ammunition against erasure networks. However, "the jury is still out" regarding which scheme is suitable in ad hoc networks. We present information on the performance of both schemes which may be useful for selecting the better coding scheme.' as abstract.

there is a document named 'doc-1494a' that
  has 'The quality of information delivered by a sensor network depends heavily on the integrity of data produced by the sensor nodes themselves. As such, the ability to model a sensor networking application scenario in the presence of faults that affect sensor data integrity is quite crucial. In this paper we describe a sensor network simulation framework that provides high fidelity modeling of sensor faults, and enables users to study end-to-end quality of information. Our sensor fault simulator is based on the popular, open-source simulator OMNet   and the associated Castalia package for modeling basic sensor network behaviors. The framework provides the ability to model various elements of a complete application scenario including node deployment, physical process, sensor faults, fault detection, and data fusion. This paper will describe the architecture of the simulation platform, the novel sensor fault model for modeling a variety of fault types and progressive failure behaviors in a parameterized fashion, and the enhancements to Castalia with integrated functionalities for fault detection.' as abstract.

there is a document named 'doc-1495a' that
  has 'We study in this paper the application of the network utility maximization (NUM) approach for jointly adapting sensor data rates and node transmission powers in a mission-oriented wireless sensor network. To extend the existing distributed adaptation approach to our scenario, where sensor data is streamed to multiple receivers via link-layer multicasts, we introduce new time-sharing constraints and periodic transmission schedules in the problem formulation. With an added power penalty in the objective function, the new multicast NUM optimization is proved to be convex, and a distributed algorithm is developed for the optimal allocation of rate and power. The convergence of the proposed algorithm is investigated by analysis and simulation.' as abstract.

there is a document named 'doc-1496a' that
  has 'Neighbor discovery is essential for the process of self-organization of a wireless network, where almost all routing and medium access protocols need knowledge of one-hop neighbors to execute. In this paper we study the problem of neighbor discovery in static and synchronous networks. We consider the case that time is divided into slots, each of duration equal to the time required to transmit a hello message. We first analyze the performance of a random discovery algorithm when transmitter nodes have no side information about the reception of their transmitted discovery packets. We then propose a simple feedback mechanism that provides reception status information to the transmitters. We show how one can use this extra information to achieve more efficient neighbor discovery. Under the assumption that all nodes are within the transmission range of each other, analysis and simulation of algorithms show that nodes discover their neighbors within a significantly smaller amount of time for the case that transmitters have side information about the reception of their packets.' as abstract.

there is a document named 'doc-1497a' that
  has 'In typical sensor network and mesh network scenarios, a subset of nodes (\'sources\') must send data to some designated set of destinations ("gateways"), while other nodes serve as relays. This some-to-some communication paradigm is distinct from the well-studied many-to-many, many-to-one, broadcast, and multicast paradigms, and is perhaps more prevalent than the other paradigms. We study the scaling laws for the data gathering capacity of such some-to-some large scale multihop wireless networks. We first derive upper and achievable lower bounds for the data gathering capacity, and then examine their design and performance implications. Our results show that the data gathering capacity is constrained by different factors in several different scaling regimes of the number of source and destination nodes, exhibiting distinct scaling laws in those regimes. This work fills a gap in our understanding of the capacity for various communication paradigms, and provides insights that should be useful for network planning and performance analysis of data gathering wireless network applications.' as abstract.

there is a document named 'doc-1499' that
  has 'Security policies are becoming more sophisticated. Operational forces will often be faced with making tricky risk decisions and policies must be flexible enough to allow appropriate actions to be facilitated. Access requests are no longer simple subject access object matters. There is often a great deal of context to be taken into account. Most security work is couched in terms of risk management, but the benefits of actions will need to be taken into account too. In some cases it may not be clear what the policy should be. People are often better at dealing with specific examples than producing general rules. In this paper we investigate the use of Grammatical Evolution (GE), a novel evolutionary algorithm to attempt to infer Fuzzy MLS policy model from decision examples. This approach couches policy inference as a search over a policy space for a policy that is most consistent with the supplied training decision set. Each candidate policy considered is associated with a fitness that indicates how consistent is the policy with the decisions provided. This fitness is then optimised using GE techniques. The results show this approach is promising.' as abstract.

there is a document named 'doc-1500a' that
  has 'A model of a military team\'s mission context describes the team and their mission, plans, progress, problems, and location. The model is dynamic because many of its elements and relationships between elements change during execution of the mission. A dynamic context model provides a resource that software agents can employ to anticipate and recognize the team\'s need for information or other resources. An experiment was conducted to determine whether such a model can be constructed and maintained from observable events that emerge from a team\'s collaboration. Teams participated in simulated missions while their voice communication, location, and related information were recorded. An analyst constructed a context model from these data, and whenever the context changed, the analyst also recorded the data that led to the change. Communication among the team provided details about their orders, plans, tactics, progress, and status. Their locations also provided information about progress and status. This experiment demonstrated that context models can be constructed and maintained from observations of team collaboration, and it suggested some ways that the analysis could be automated. Analyses of other teams and missions are needed to confirm and extend these results, and experiments with military personnel are needed to obtain a valid sample of collaboration patterns and for investigating heuristics for automating collaboration analysis.' as abstract.

there is a document named 'doc-1501' that
  has 'We explore two Semantic Web techniques arising from ITA research into semantic alignment and interoperability in distributed networks. The first is POAF (Portable Ontology Aligned Fragments) which addresses issues relating to the portability and usage of ontology alignments. POAF uses an ontology fragmentation strategy to achieve portability, and enables subsequent usage through a form of automated ontology modularization. The second technique, SWEDER (Sematic Wrapping of Existing Data sources with Embedded Rules), is grounded in the creation of lightweight ontologies to semantically wrap existing data sources, to facilitate rapid semantic integration through representational homogeneity. The semantic integration is achieved through the creation of context ontologies which define the integrations and provide a portable definition of the integration rules in the form of embedded SPARQL construct clauses. These two Semantic Web techniques address important practical issues relevant to the potential future adoption of ontologies in distributed network environments.' as abstract.

there is a document named 'doc-1503a' that
  has 'Distributed beamforming in wireless ad hoc networks has the promise of greatly improving network throughput. However, unlike traditional beamforming from a fixed array, the random locations of the nodes collaborating to form the array lead to a random beam pattern. In particular, the position and size of side lobes can vary greatly and have a significant impact on the concurrent transmissions that are the source of much of the throughput gain realized from distributed beamforming. Here, we present a simple model that captures this randomness and then use the model to consider the average throughput of large ad hoc wireless networks. Numerical results are compared to those obtained if one employs the oft-used pie-wedge approximation for a directed antenna beam, and the difference is shown to be significant in regions where the side lobe interference is nonnegligible.' as abstract.

there is a document named 'doc-1504a' that
  has 'We have demonstrated that stochastic process algebra models can make quantitative estimates of the quality of information in a military sensor network deployed in a simple target tracking scenario.1 These models abstract away from the physical reality by describing it as components that exist in discrete states with probabilistically invoked actions that change the state. The quality of information may be assessed by considering the probability that reports made by the network to its users are correct. For example, for the instances when a network reports that the target is at location A, we estimate the probability that the target is indeed at location A. Estimates of this kind may be made directly from the continuous time Markov chain that corresponds to the process algebra model. Process algebras are a good formalism for describing the behaviour of mobile sensor networks, but they are less suitable for making inferences about the performance of a network in deployment. In this paper we argue that dynamic Bayesian network models, which have been used in a variety of military applications, are a more suitable vehicle for understanding network performance and making inferences about quality of information. We show that it is possible to construct a Bayesian network over the state variables of the process algebra model, and make inferences by instantiation and probability propagation. The network is constructed dynamically, though in an approximate manner, by drawing an ergodic chain of samples from the continuous time Markov model and constructing a dependency graph from the samples. The samples are drawn at a rate that allows us to capture the temporal behaviour. By using a moving time window on the sample chain, and incrementally updating the dependency metric, the dependency graph can be allowed to change as the network changes its configuration in a deployment. Visualising the dependency structure can be used to provide a qualitative assessment of factors that will affect the quality of information. We expect that any military network will have a very sparse dependency structure, since most nodes communicate only with their near neighbours, and thus act independently of distant nodes. The sparsity of the resulting Bayesian network allows fast propagation of probabilities, and hence interactive querying for quality of information. The dependency structure also provides information on cliques of variables that can be joined providing a further abstraction to reduce the state space of the model.' as abstract.

there is a document named 'doc-1505a' that
  has 'A mission planner\'s point of view of sensor-enabled detection systems is considered and a hypothesis-testing-based computational framework for evaluating the quality of information (QoI) supported by a sensor network deployment is explored. Through a common, modular analysis framework, that decomposes the computational burden of QoI evaluation, the QoI properties of various fusion/decision architectures are investigated and trade-offs explored at the sensor, cluster, and system-level. Both finite and infinite-sized sensor networks are considered and extensions of the analysis framework to faulty sensor and the impact of calibration are also investigated.' as abstract.

there is a document named 'doc-1506a' that
  has 'Analysis of communications in human teams suggests that an important form of communication between team members is an "information providing" dialogue, in which team members update their fellows with information that they regard as important to the task at hand. In this paper we introduce and analyse a formal model of such a form of dialogue, seeing this as a necessary first step in providing software support for this kind of communication.' as abstract.

there is a document named 'doc-1517' that
  has 'Sensor networks are used widely within the military and civilian domains for a variety of monitoring and surveillance purposes. In all of these deployments, algorithms and schemes that can analyze the properties and nature of the coverage provided by a set of sensors over a target area are of key importance. Such algorithms provide a key input to the problem of planning the sensor coverage for an area which is to be subject to surveillance.' as abstract.

there is a document named 'doc-1523a' that
  has 'In this paper, we demonstrate that probabilistic context free grammars (PCFGs) can be used to recognize events from a sensor data stream. A fast PCFG inference algorithm based on Stolcke(1994) and Chen(1996) is presented which utilizes the set of observation strings as training data. A realworld scenario is presented and we also show that multi-modal sensor information can be utilized using Dempster-Shafer theory of evidence.' as abstract.

there is a document named 'doc-1524' that
  has 'As part of an exploratory study into linguistic sources of coalition miscommunication, we have interviewed a number of UK and US military and civilian staff. Initial analysis of the data has shown that there are various types of linguistic variations and cultural differences manifested by the US and UK groups. American English and British English differ in complex ways not only in terms of lexical differences but also, perhaps more importantly, in terms of language use due to cultural differences. While this study and analysis are preliminary, the results provide support for our initial hypotheses. Importantly, the current analysis suggests that many relevant issues are largely pragmatic in nature, not just involving lexical and grammatical differences but indicating differences in the way the two cultures use the "common" language.' as abstract.

there is a document named 'doc-1525a' that
  has 'Our objective is to build software agents that can support the operations of coalition teams. One way to provide support is to handle some of the routine communciation between human team members, allowing them to concentrate on the job of completing their tasks. To build agents that can do this job, we need to understand the kinds of communciations that take place between the human team members. Accordingly we have analysed the communications that take place between team members completing a prototypical simulated task. In this paper, we describe two independent analyses of the transcript of the communication between members of a human team enagaged in a military task. We give the results of the analyses, compare them, and summarise what we learned from the two exercises.' as abstract.

there is a document named 'doc-1532' that
  has 'Cooperative diversity is well known for its capability to combat multi-path fading and increase reception reliability, which motivates its wide applications to various communication scenarios. In this correspondence, a new form of cooperative transmission protocol, named as opportunistic decode-forward protocol, is constructed by combining opportunistic strategies with non-orthogonal transmission. An achievable diversity-multiplexing tradeoff (DMT) is developed to evaluate the spectral efficiency of the proposed scheme. Compared with some existing protocols, the proposed scheme achieves a better DMT, particularly for 1 2 _ r _ N N+1 , where N denotes the number of relays and r is the multiplexing gain. By increasing the number of the relays, it can be shown that the achievable DMT can approach the optimal multiple-input single-output upper bound. Monte-Carlo simulation results demonstrate that the proposed protocols can achieve better error performance than the comparative schemes in most simulation conditions.' as abstract.

there is a document named 'doc-1538' that
  has 'Effective deployment of limited and constrained intelligence, surveillance and reconnaisance (ISR) resources is seen as a key issue in modern network-centric joint-forces operations. The aim of our work is to enable proactive and reactive deployment of sensors and other information sources to best support the objectives of a task (or mission) being undertaken. In this paper, we consider one aspect of the deployment problem: proactive assignment of sensors and sources to mission tasks. We view this sub-problem as a matchmaking activity: matching the ISR requirements of tasks to the ISR-providing capabilities of available sensors and sources, and the platforms that carry them. A key issue is that of defining sufficiently-rich representations of these various elements - missions, tasks, ISR requirements, ISR capabilities, sensors, sources, and platforms - to support the matchmaking activity. We argue for an approach based on the use of ontologies: formal models of the various elements that can be used with deductive reasoning mechanisms to produce matches that are logically sound. We introduce a new ontology based on the military Missions and Means Framework (MMF), and show that the matchmaking activity is necessarily multidimensional in nature. We indicate how our approach builds on previous work in representing sensors and sources for various purposes, and highlight the role of current Web standards in providing an engineering foundation for our approach.' as abstract.

there is a document named 'doc-1539' that
  has 'Policy-based management of the security of a military communications network can simplify the configuration process, while increasing security and availability. An effective policy-based approach requires analysis of policies for inconsistencies, and for desired security properties. It also must provide for the refinement of high-level security goals into concrete policies. This paper defines a language based on first-order logic formulae containing explicit time arguments which is expressive enough for specifying a range of authorization and obligation security policies, while supporting the formalisms and automated tools needed for analysis and refinement. Both system behavior and the semantics of the policies themselves are defined in terms of execution traces, to enable reasoning about algorithmic solutions to policy analysis. The paper also proposes some analysis tools based on the use of logical abduction.' as abstract.

there is a document named 'doc-1540' that
  has 'Along the line of research using Latent Semantic Analysis (LSA), we conducted an analysis of the discourse data related to mission execution known as the Singapore Data. The analysis results provide some initial insights into the strengths and weaknesses of LSA as a methodology for analyzing discourse, suggesting that pragmatic aspects of language use are essential parameters to successful understanding of communication threads. This analysis serves as a pilot study for developing a computational pragmatics framework for automating the analysis of coalition communication.' as abstract.

there is a document named 'doc-1541a' that
  has 'Nowadays data dissemination often happens in vehicular sensor networks (VSN) and other mobile ad hoc networks in military & surveillance scenarios. The performance of data dissemination depends on many different parameters including speed, motion pattern, node density, topology, data rate, and transmission range. This multitude makes it difficult to accurately evaluate and compare data gathering protocols implemented in different simulation or testbed scenarios. In this paper, we introduce Neighborhood Change Rate (NCR), a unifying measurement for different motion patterns used in epidemic dissemination, a contact-based data dissemination. By its intrinsic property, the NCR measurement is able to describe the spatial and temporal dependencies and well characterize a dissemination / harvesting scenario. We illustrate our approach by applying the NCR concept to MobEyes, a lightweight data gathering protocol. We further analytically study the effective NCR for Markov type motion models, such as Real Track mobility model. A closed-form expression has been derived. From this analytic solution, the NCR can be approximated from the initial scenario settings, such as velocity range, transmission range, and real map/street information. The closed-form formula for NCR can be further employed to evaluate the ED process. The mathematical relationship between the dissemination index and the effective NCR is established and it allows predicting the performance of the ED process in realistic track motion scenarios. The experiment results showed that the analytic expressions for the NCR and for the evaluation of the ED process closely match the discrete-event simulations.' as abstract.

there is a document named 'doc-1542' that
  has 'Sensor networks represent powerful new data collection paradigm that facilitates the development of a wide range of new generation of intelligent applications. The utility of sensor networks can thus be measured by the quality of information (QoI) they provide to the applications that depend on them. This paper focuses on the concept of QoI in sensor networks. It introduces the concept of QoI and related information metadata for describing it. Furthermore, it motivates the use of QoI as a key element in building autonomous sensing systems and intelligent sensor-dependent applications. Finally, it illustrates the QoI concept discussing pertinent quality related metadata using a realistic sensor network application for monitoring and tracking in a military setting.' as abstract.

there is a document named 'doc-1543' that
  has 'This paper describes the work on the Collaborative Planning Model in Project 12. An hypothesis is presented to attempt to unify the different planning doctrines under a common model. This has guided the development of a generic planning OWL ontology containing aspects of constraint-based reasoning and teamwork. The ontology also models "reasoning steps" and the dependencies between expertise, issues and decisions. The relations between reasoning steps begin to provide a vocabulary for the exploration of different doctrines. An example of collaborative planning, execution and replanning has been used to exercise the ontology, and some initial tools have been generated as a plugin to Protege to assist the input, analysis and diagramming of plans.' as abstract.

there is a document named 'doc-1546' that
  has 'Research into the retrieval and dissemination of mission-specific information across sensor networks is leading to the development of many novel new algorithms. Testing the relative merits of such algorithms, and exploring interoperability issues between them, is difficult unless they share a common test and validation framework. The wide variety of algorithms being developed (including sensor/mission matching, mission scripting, resource allocation, network routing, and data fusion algorithms) presents unique challenges to the development of such a framework and its subsequent instrumentation to provide experimental results. This paper describes the creation of such a framework (or "Fabric") prototyped on top of commercial off-theshelf (COTS) software. We show the architecture of the Fabric, and describe services that are provided to manage sensors, the network topology, and inter-node communication. We describe the Fabric\'s implementation using IBM publish/subscribe messaging technology (a compact message broker, called the microbroker, which provides publish/subscribe messaging on small devices), and how this approach results in a reconfigurable data stream model using topics to share data between consumers and a network of inter-connected sensors (real and simulated) and intermediate utility nodes. We will show how new algorithms can be deployed into the framework, and how the replay of recorded sensor data is supported. Finally we demonstrate the real-world utility of the fabric by its application to a specific research problem.' as abstract.

there is a document named 'doc-1547' that
  has 'This paper is the first step in our research plan towards addressing the fundamental question of how software agents can best aid distributed human teams performing timestressed critical tasks in uncertain and dynamic environments. Based on prior work, we hypothesize that to improve the performance of human teams, agents must do some combination of the following: (1) reduce the cost of the humans\' information processing; (2) decrease uncertainty in the task; (3) improve coordination between team members; (4) directly assist in task completion. In order to (a) establish an experimental baseline of the performance of human-only teams for some particular task domain, and (b) best understand where agents can provide best utility in supporting human teamwork, we designed scenarios and performed experiments with human teams performing a time-stressed, collaborative search task in a multi-player gaming environment. The collaborative search task recreates some of the challenges faced by human teams during search and rescue operations, such as the one described in the Holistan scenario. In our experiments, we analyze (1) verbal communication between team members and (2) the effects of presenting or omitting task progress information. By ascertaining the information processing and coordination requirements of this team task, we expect to identify "insertion points" for agent assistance to human teams. Agent assistance will be particularly critical to military teams as their operations become more agile and situation specific. As unfamiliar forces are brought together for different coalition missions, agent support of teamwork becomes crucial.' as abstract.

there is a document named 'doc-1548' that
  has 'Firewalls are a effective means of protecting a local system or network of systems from network-based security threats. In this paper, we propose a policy algebra framework for security policy enforcement in hybrid firewalls, ones that exist both in the network and on end systems. To preserve the security semantics, the policy algebras provide a formalism to compute addition, conjunction, subtraction, and summation on rule sets; it also defines the cost and risk functions associated with policy enforcement. Policy outsourcing triggers global cost minimization. We show that our framework can easily be extended to support packet filter firewall policies. Finally, we discuss special challenges and requirements for applying the policy algebra framework to MANETs.' as abstract.

there is a document named 'doc-1549' that
  has 'A sensor network consists of a large number of small sensing devices that are able to collect information about their surroundings. When such a network is deployed in a field it is usually required to support multiple missions. Hence, schemes that match sensors to missions become necessary. In this paper, we formally define the sensor-mission matching problem and examine two approaches to solve it. We first consider a centralized approach in which the decisions on which sensors are selected and assigned to missions are made in a single node. We propose two centralized schemes: mission-side greedy and sensor-side greedy. The second approach we consider is distributed, with the decisions made by multiple nodes. We propose two schemes that fall under this approach: a bidding-based scheme and a multi-round proposal scheme. We also show simulation results comparing the performance of these solutions.' as abstract.

there is a document named 'doc-1550' that
  has 'A threat model for sensor networks and mobile ad hoc networks (MANETs) is introduced. Components that can be used to form an adversary model are developed. Threat categories, modes of use, and a variety of threats to system assets are identified, including threats to communications, infrastructure services, individual nodes and human users. Example envisaged applications of the threat model are discussed, covering threats to secure information flows, to threshold and identity based cryptography and to risk and trust management.' as abstract.

there is a document named 'doc-1551' that
  has 'Service Oriented Architectures (SOA) are increasingly being used by architects of business IT systems to encapsulate key business components and make them available for efficient reuse across multiple business functions. An SOA within a business is typically a managed environment relying on good IT management practice within a reliable networked infrastructure. In a military environment, reliable IT infrastructures are not guaranteed; particularly where ad-hoc wireless networks are used. When sensors are represented as services, additional metadata is required to describe, for example, their current energy status. This places further requirements on an SOA which are identified later. The need for an agile SOA which is more dynamic and flexible than existing business implementations is discussed and postulates for the additional features of SOA for wireless sensor network environment are proposed.' as abstract.

there is a document named 'doc-1554a' that
  has 'This paper investigates the interaction between network coding and link-layer transmission rate diversity in multihop wireless networks. By appropriately mixing data packets at intermediate nodes, network coding allows a single multicast flow to achieve higher throughput to a set of receivers. Broadcast applications can also exploit link-layer rate diversity, whereby individual nodes can transmit at faster rates at the expense of corresponding smaller coverage area. We first demonstrate how combining rate-diversity with network coding can provide a larger capacity again for data dissemination of a single multicast flow. We present a linear programming model to compute the maximal throughput that a multicast application can achieve with network coding in a rate-diverse wireless network. We then present simulation results comparing the performance of network coding in combination with transmission rate diveristy, for a realistic stream-oriented application. Our results provide preliminary evidence that wireless network coding may lead to a latency-vs-throughput tradeoff.' as abstract.

there is a document named 'doc-1555' that
  has 'The capacity gain of network coding has been extensively studied in wired and wireless networks. Recently, it has been shown that network coding improves network reliability by reducing the number of packet retransmissions in lossy networks. However, the extent of the reliability benefit of network coding is not known. This paper quantifies the reliability gain of network coding for reliable multicasting in a wireless network where network coding is most promising. We define the expected number of transmissions per packet as the performance metric for reliability and derive analytical expressions characterizing the performance of network coding. For tree-based multicast, we derive expressions for the expected number of transmissions at the multicast source and inside the multicast tree. We also analyze the performance of error control mechanisms based on rateless codes and automatic repeat request (ARQ). In particular, we show that the expected number of transmissions using ARQ compared to network coding scales as _(log K) where K _ _ is the number of multicast receivers. We then use the analytical expressions to study the impact of multicast group size on the performance of different error control schemes. Our numerical results show that network coding significantly reduces the number of retransmissions in lossy networks compared to an end-to-end ARQ scheme, however, rateless coding and link-by-link ARQ achieve performance results comparable to that of network coding.' as abstract.

there is a document named 'doc-1558' that
  has 'The processes underlying complex collaborative activities such as military decision making are inherently variable. Even within a single organization there are many variants of processes that have the same purpose. When coalition members must work together, these differences may be especially large, baffling and disruptive. Coordination theory provides a method and vocabulary for modeling complex collaborative activities in a way that makes both the similarities and differences between them more visible. To illustrate this we modeled parts of the decision-making process as defined in a field manual. The methods for coordination defined in this model were all drawn from a repository of collaboration processes. This approach has promise for helping to merge or integrate different processes.' as abstract.

there is a document named 'doc-1559b' that
  has 'In the literature of ad-hoc sensory networks, a wellstudied problem is that of achieving full sensory coverage within a region according to the boolean model of coverage. We introduce a new technique to achieve full coverage that anticipates bounded random deviations of resultant positions of some or all sensors from their designated positions. Various scenarios of placement for arbitrary arrangements of sensors are presented, and illustrated using two widely used grid arrangements. Quantitative results linking radius of coverage, grid granularity and tolerance for inexact placement are provided for triangular and square grid arrangements. Finally, we elaborate on various practical applications cutting across the aforementioned scenarios and arrangements.' as abstract.

there is a document named 'doc-1560' that
  has 'This paper presents a theoretical and methodological framework for studying cultural differences and commonalities in specialized knowledge domains. This framework, referred to as Cultural Network Analysis, provides a collection of methodologies for characterizing the mental representations distributed among members of cultural groups. This framework is being developed as an approach to increase our understanding of real, complex, and dynamic operational environments, such as is the case in coalition planning teams. In coalition planning, human actors are tasked with communicating and collaborating with other humans who have different cultural understandings of the tasks at hand. A solid scientific understanding of the knowledge employed in complex, dynamic work domains will provide us with a foundation for developing design principles for tools and systems that can better support complex tasks such as coalition planning by taking cultural differences in understanding into account.' as abstract.

there is a document named 'doc-1561b' that
  has 'Network coding, where relay nodes combine the information received from multiple links rather than simply replicating and forwarding the received packets, has shown the promise of significantly improving system performance. In very recent works, multiple researchers have presented methods for increasing system throughput by employing network coding inspired methods to mix packets at the physical layer: physicallayer network coding (PNC). A common example used to validate much of this work is that of two sources exchanging information through a single intervening relay - a situation that we denote the "exchange channel". In this paper, achievable rates of various schemes on the exchange channel are considered. Achievable rates for traditional multi-hop routing approaches, network coding approaches, and various PNC approaches are considered. A new method of PNC inspired by Tomlinson-Harashima precoding (THP), where a modulo operation is used to control the power at the relay, is introduced.' as abstract.

there is a document named 'doc-1562a' that
  has 'In this paper, we study the effect of cooperative transmission on the routing decision for wireless ad-hoc networks. The influence of cooperative transmission to the wireless link cost is first studied, with or without node selection, which shows that the quality of wireless links could be improved significantly. To reduce system overhead, a distributed strategy of node selection is proposed by carefully designing the carrier sensing protocol in the medium access control layer. Then routing optimization is investigated to understand the effects of improved link cost to the routing decision, where the optimal solution of the optimization problem is developed and later used as a quantitative criterion of the routing decision. Our developed analytical and simulation results show that the criteria using cooperative transmission typically yield more efficient routes than the non-cooperative schemes.' as abstract.

there is a document named 'doc-1562b' that
  has 'In this paper, we study the effect of cooperative transmission on the routing decision for wireless ad-hoc networks. The influence of cooperative transmission to the wireless link cost is first studied, with or without node selection, which shows that the quality of wireless links could be improved significantly. To reduce system overhead, a distributed strategy of node selection is proposed by carefully designing the carrier sensing protocol in the medium access control layer. Then routing optimization is investigated to understand the effects of improved link cost to the routing decision, where the optimal solution of the optimization problem is developed and later used as a quantitative criterion of the routing decision. Our developed analytical and simulation results show that the criteria using cooperative transmission typically yield more efficient routes than the non-cooperative schemes.' as abstract.

there is a document named 'doc-1563a' that
  has 'In this paper, we study network coding in the context of wireless communications. A new form of network coding is developed without the assumptions of precise phase synchronization and high transmission power. Wireless diversity is efficiently utilized by applying a distributed strategy of relay selection to network coding. To facilitate performance evaluation, two information-theoretic metrics, the outage and ergodic capacity, are studied. Our analytical and simulation results show that the proposed protocol can achieve more robust performance and higher system throughput than the comparable schemes.' as abstract.

there is a document named 'doc-1563b' that
  has 'In this paper, we study network coding in the context of wireless communications. A new form of network coding is developed without the assumptions of precise phase synchronization and high transmission power. Wireless diversity is efficiently utilized by applying a distributed strategy of relay selection to network coding. To facilitate performance evaluation, two information-theoretic metrics, the outage and ergodic capacity, are studied. Our analytical and simulation results show that the proposed protocol can achieve more robust performance and higher system throughput than the comparable schemes.' as abstract.

there is a document named 'doc-1564' that
  has 'Situation awareness is a critical element of military decision superiority in a wide variety of operational contexts. Improved situation awareness can benefit operational effectiveness by facilitating the planning process, improving the quality and timeliness of decisions, and providing better feedback about the strategic and tactical consequences of military actions. The military coalition environment presents a number of challenges to situation awareness research; not only in terms of the technical approaches used to enhance situation awareness, but also in terms of the models and conceptual frameworks used to analyse situation awareness. This paper outlines an approach to enhancing situation awareness that is grounded in the use of Semantic Web technologies. We describe the challenges to both individual and team situation awareness presented by coalition military environments, and we discuss ways in which semantic technologies might be used to address these challenges. We suggest that an approach featuring domain ontologies, reasoning capabilities, semantic queries and semantic integration techniques provides the basis for an integrated framework for improving situation awareness in military coalition contexts. We provide an example of our approach in the form of the InfoGlue framework for adaptive, context-aware information retrieval.' as abstract.

there is a document named 'doc-1565' that
  has 'A configuration consisting of sensors and communication nodes that support user database functions can be modeled as a network of distributed databases. In this paper we investigate certain properties of such networks, and examine the ways in which two or more database networks can be linked into a single network so as preserve or achieve desirable properties. In particular, we examine how merging can be done so as to minimize the diameter and maximize the resiliency of the resulting network, doing so at minimal cost.' as abstract.

there is a document named 'doc-1566a' that
  has 'Our primary hypothesis is that it should be possible to enrich data fusion by semantic processing, with wide potential application. In order to achieve our aim we need to represent the semantic data and enable reasoning about it in a framework that can be aligned with data fusion. Ontologies are most suited to this task as they allow for appropriate representation of data structure; some approaches include probabilistic representation. These can be aligned with data fusion approaches, such as Bayesian, which can fuse by including estimates of uncertainty. We shall describe our initial approaches towards establishing our hypothesis. We shall survey the enabling technologies, showing how they can contribute to our goal. We shall describe our selection of application data which derives from an acoustic sensor (military) scenario. We shall show how feature subset selection can reduce information-redundancy and improve efficiency in these domains, prior to fusion to enhance performance further. We shall explore the semantic attributes and the representations that can be deployed for enrichment purposes, showing how ontologies can be used in this context. In these respects we are aiming to show how we can approach enrichment of data fusion by semantic technologies, how this can capitalise on the current stock of techniques, and illustrate the potential benefits associated with this new approach.' as abstract.

there is a document named 'doc-1568' that
  has 'We propose an on-line fault detection and diagnosis framework in sensor networks, called Inspect, which is based on a hybrid tiered approach to integrity checking. Inspect tiered design combines the benefits of distributed and centralized approaches, thus improving the responsiveness and efficiency of the fault detection. More precisely, Inspect consists of a local tier built at each sensor node and capable of detecting anomalies, and a centralized tier built at the sink and capable of distinguishing between sensor faults and unexpected temporal-spatial variations in the phenomenon, and detecting more complex types of faults. Inspects offers several desirable features: on-line detection, application independence, and increased robustness to temporalspatial variations. Moreover, Inspect provides confidence bounds that can be dynamically tuned according to the user requirements for a better resource management, and addresses the problem of detecting more complex types of faults.' as abstract.

there is a document named 'doc-1570' that
  has 'This paper proposes the use of the virtual organization framework in managing collaboration in a mixed team of software agents and humans aided by such agents. The paper argues that this framework facilitates an integrated management approach and sets the scene for experimental work to demonstrate the effectiveness of the approach.' as abstract.

there is a document named 'doc-1576' that
  has 'Trust authority (TA) services are both important infrastructure services for layered protocols requiring the availability of an identification and authentication mechanism such as access control mechanisms and confi- dentiality services, and can also be viewed as exemplars for the secure and efficient distribution of computations in general. While such general problems have been studied extensively, tactical MANET environments impose a number of requirements and constraints such as RF range and cost, battery limitations, and computational capabilities which call for more specific approaches. In this paper we report the analysis of algorithms for TA service distribution based on cluster head algorithms and improvements on the basic algorithms based on the specific requirements as identified in the course of simulations of tactical scenarios and realizing appreciable increases in efficiency over the general case in the process.' as abstract.

there is a document named 'doc-1584' that
  has 'In this paper we explore the "interface" between Identity-based Public Key Cryptography (ID-PKC) and Mobile Ad-hoc Networks (MANETs). In particular we examine the problem of naming and namespace design in an Identity-based Key Infrastructure (IKI). We examine the potential impact that different types of identifiers may have on the utility of ad hoc networks where an IKI provides the underlying key infrastructure. We also highlight problems inherent in extending namespaces to allow inter-operability amongst heterogeneous trust domains.' as abstract.

there is a document named 'doc-1672' that
  has 'Broadcast scheduling has been extensively studied in wireless environments, where a base station broadcasts data to multiple users. Due to the sole wireless channel\'s limited bandwidth, only a subset of the needs may be satisfiable, and so maximizing total (weighted) throughput is a popular objective. In many realistic applications, however, data are dependent or correlated in the sense that the joint utility of a set of items is not simply the sum of their individual utilities. On the one hand, substitute data may provide overlapping information, so one piece of data item may have lower value if a second data item has already been delivered; on the other hand, complementary data are more valuable than the sum of their parts, if, for example, one data item is only useful in the presence of a second data item. In this paper, we define a data bundle to be a set of data items with possibly nonadditive joint utility, and we study a resulting broadcast scheduling optimization problem whose objective is to maximize the utility provided by the data delivered.' as abstract.

there is a document named 'doc-1677' that
  has 'ABox abduction is the process of finding statements that should be added to an ontology to entail a specific conclusion. In this paper, we propose an approach for probabilistic abductive reasoning for SHIQ. Our evaluations show that the proposed approach significantly extends classical abduction by effectively and correctly estimating probabilities for abductive explanations. Lastly, based on the ideas proposed for SHIQ, we describe a tractable algorithm for DL-Lite.' as abstract.

there is a document named 'doc-1678' that
  has 'In modern coalition operations, decision makers must be capable of obtaining and fusing data from diverse sources. The reliability of these sources may be variable, and, in order to protect their interests, the data they provide may be obfuscated. The trustworthiness of fused data is dependent not only on the reliability of the sources, but also on the type and extent of the obfuscation used. New problems arise for both data providers and consumers in these contexts; the consumers must determine how to evaluate the trustworthiness of providers in the presence of differing levels of obfuscation, while the providers must be able to determine the appropriate level of obfuscation to ensure that trust in them is maintained. In this paper, we outline this rich problem area. We discuss trust and obfuscation in these contexts and the complex relationships between them.' as abstract.

there is a document named 'doc-1681' that
  has 'An assistant refers to a (human or software) agent that helps a user accomplish her goals, ideally with minimal instructions from the user. In this paper we describe a software assistant agent that can proactively assist human users situated in a time-constrained dynamic environment. We specifically aim at assisting the user\'s normative reasoning-reasoning about prohibitions and obligations. When human users are cognitively overloaded normative stipulations hinder user\'s ability to plan to both accomplish goals and abide by the norms. In contrast to reactive assistant agent systems (that perform certain tasks on behalf of the user in reaction to user cues), this paper presents a goal-driven approach where the agent sets its own goals, plans and executes a series of actions to accomplish the identified goals, steering users towards norm compliance. Our approach uses probabilistic plan recognition to predict the user\'s future plan based on the user\'s current activities, identifying likely norm violations in the predicted user plan and updating the assistant\'s goals to accommodate potential needs for resolving norm violations. This paper introduces the general framework for the goal-driven autonomous assistant agent. To validate the approach we implemented an assistant in the context of a military peacekeeping scenario that involves norm reasoning within various coalition partners. This approach is the first that manages norms in a proactive and autonomous manner.' as abstract.

there is a document named 'doc-1683' that
  has 'Planning under pressure in time-constrained environments while relying on uncertain information is a challenging task. This is particularly true for planning the response during an ongoing disaster in a urban area, be that a natural one, or a deliberate attack on the civilian population. As the various activities pertaining to the emergency response need to be coordinated in response to multiple reports from the disaster site, a user finds itself cognitively overloaded. To address this issue, we designed the Anytime Cognition (ANTICO) concept to assist human users working in time-constrained environments by maintaining a manageable level of cognitive workload over time. Based on the ANTICO concept, we develop an agent framework for proactively managing a user\'s changing information requirements by integrating information management techniques with probabilistic plan recognition. In this paper, we describe a prototype emergency response application in the context of a subset of the attacks devised by the American Department of Homeland Security.' as abstract.

there is a document named 'doc-1684' that
  has 'In this paper we describe a domain for the application of the results of the research conducted as part of project 6 of BPP11. This domain, and specific scenarios we will construct in the domain, is intended to both focus initial research and also help identify potential issues early in the research. Finally, an end-to-end scenario created in this domain can be used to illustrate key capabilities resulting from the ensuing research.' as abstract.

there is a document named 'doc-1686' that
  has 'Upper bounds on the capacity scaling of a hybrid network are given via cut-set techniques. A one-dimensional network with n randomly placed ad hoc nodes and b(n) regularly placed base stations is considered, where the wired network connecting the base stations is assumed to have finite bandwidth f(n). It is shown that the scaling of per-node throughput achieved by ad hoc nodes is upper bounded by min{b(n)/n, f(n)/n}, and this bound is shown to be achievable for b(n) log b(n) 6 n. Similar results are also shown for two-dimensional hybrid networks.' as abstract.

there is a document named 'doc-1722' that
  has 'The secure transmission of information in wireless networks without knowledge of eavesdropper channels or locations is considered. Two key mechanisms are employed: artificial noise generation from system nodes other than the transmitter and receiver, and a form of multi-user diversity that allows message reception in the presence of the artificial noise. We determine the maximum number of independently-operating and uniformly distributed eavesdroppers that can be present while the desired secrecy is achieved with high probability in the limit of a large number of system nodes. For the case of equal path-loss between all users, a number of eavesdroppers that is exponential in the number of systems nodes can be tolerated. In the more pertinent case of uniformly distributed system nodes and uniformly distributed eavesdroppers of unknown location, any number of eavesdroppers whose growth is sub-linear in the number of system nodes can be tolerated. The proposed approach significantly outperforms a power control approach based on standard multi-user diversity.' as abstract.

there is a document named 'doc-1723' that
  has 'Consider the transmission of secret messages to users in a downlink cellular scenario, where the message to a given user must be kept secret from all of the other users. Multiuser diversity suggests an opportunistic approach that sends a message secretly to the user with the current best channel; however, the secrecy rate goes to zero in the limit of a large number of users. Here, channel reciprocity is exploited via a two-way secrecy scheme to provide a constant positive secrecy rate to the user with the best channel. Next, motivated by the desire to transmit to a given user (rather than opportunistically to the user with the best channel), a second scheme is developed that employs relaying from other users from whom the message is still kept secret. The secrecy rates of the proposed schemes are analyzed analytically in the limit of a large number of system users. In addition, for a finite number of users, simulation results demonstrate that the proposed two-way secrecy schemes are able to provide significant gains in the secrecy rates in both the opportunistic and non-opportunistic scenarios over a large range of user numbers.' as abstract.

there is a document named 'doc-1728' that
  has 'Policies are declarations of constraints on the behaviour of components within distributed systems, and are often used to capture norms within agent-based systems. A few machine-processable representations for policies have been proposed, but they tend to be either limited in the types of policies that can be expressed or limited by the complexity of associated reasoning mechanisms. In this paper, we argue for a language that sufficiently expresses the types of policies essential in practical systems, and which enables both policygoverned decision-making and policy analysis within the bounds of decidability. We then propose an OWL-based representation of policies that meets these criteria using and a reasoning mechanism that uses a novel combination of ontology consistency checking and query answering. In this way, agent-based systems can be developed that operate flexibly and effectively in policyconstrainted environments.' as abstract.

there is a document named 'doc-1732' that
  has 'Distributed databases involving multiple parties, which may want to share their information selectively, requires effective implementations of distributed disclosure policies. While some research based on policies restricting the visibility of table attributes has been done, access control on tuple sets and horizontally data partition have not been considered. We will discuss the research issues brought up by these extensions.' as abstract.

there is a document named 'doc-1735' that
  has 'This demonstration will show the results of initial investigations into techniques to support information extraction through the use of Controlled Language that is the research focus of ITA Project 4, Task 2 during the BPP11. The basic direction of the research is towards an agent-based environment where multiple agents can collaboratively consume and create information in the common format of a Controlled Natural Language known as ITA Controlled English (CE). The agents can be human or machine, with all agents communicating and being directed or configured only via the common CE language. This demonstration will show early progress in support of this goal and we hope to show some basic agents that implement relevant aspects of basic linguistic theory. We also aim to demonstrate a structural decomposition of the target corpus, again using agents that operate in this CE-based environment, and where possible we will also demonstrate some related earlier work (from BPP09) regarding the automated conversion of "linked data" into this common format as an additional data source for consumption as usage by the linguistic processing agents. There is potential for demonstration of linkages with other ITA research that is considering CE as a representation format and if possible we will also demonstrate the potential for integrating these sources too.' as abstract.

there is a document named 'doc-1737' that
  has 'Data fusion plays a major role in assisting decision makers by providing them with an improved situational awareness so that informed decisions could be made about the events that occur in the field. This involves combining a multitude of sensor modalities such that the resulting output is better (i.e., more accurate, complete, dependable etc.) than what it would have been if the data streams (hereinafter referred to as \'feeds\') from the resources are taken individually. However, these feeds lack any context-related information (e.g., detected event, event classification, relationships to other events, etc.). This hinders the fusion process and may result in creating an incorrect picture about the situation. Thus, results in false alarms, waste valuable time/resources. In this paper, we propose an approach that enriches feeds with semantic attributes so that these feeds have proper meaning. This will assist underlying applications to present analysts with correct feeds for a particular event for fusion. We argue annotated stored feeds will assist in easy retrieval of historical data that may be related to the current fusion. We use a subset of Web Ontology Language (OWL) [1], OWL-DL to present a lightweight and efficient knowledge layer for feeds annotation and use rules to capture crucial domain concepts. We discuss a solution architecture and provide a proof-of-concept tool to evaluate the proposed approach. We discuss the importance of such an approach with a set of user cases and show how a tool like the one proposed could assist analysts, planners to make better-informed decisions.' as abstract.

there is a document named 'doc-1739' that
  has 'In order to press maximal cognitive benefit from their social, technological and informational environments, military coalitions need to understand how best to exploit available information assets as well as how best to organize their socially-distributed information processing activities. The International Technology Alliance (ITA) program is beginning to address the challenges associated with enhanced cognition in military coalition environments by integrating a variety of research and development efforts. In particular, research in one component of the ITA (\'Project 4: Shared Understanding and Information Exploitation\') is seeking to develop capabilities that enable military coalitions to better exploit and distribute networked information assets in the service of collective cognitive outcomes (e.g. improved decision-making). In this paper, we provide an overview of the various research activities in Project 4. We also show how these research activities complement one another in terms of supporting coalition-based collective cognition.' as abstract.

there is a document named 'doc-1743' that
  has 'Service-oriented Architecture for sensor network applications aims at providing composable sensor network services supporting complex functionality within a specific application domain, together with tools for composing such complex services from a set of primitive ones. A fundamental step performed by any component service in the service composition process is the selection of input service providers that will provide the data that it needs to carry out its own functionality in the composition graph. In this paper, we propose the use of real options theory for selecting component services. Real options are designed to reduce the risk associated with an investment by delaying the investment decision for a certain period of time, allowing for consideration of alternatives of the initial investment. Thus, they enhance managerial flexibility in the presence of uncertain information regarding the environment. We think about activated sensor services as investments (in terms of sensor network resources), and apply the switch options variant to manage the risks of high cost that may result from the low reliability and limited resources of selecting certain sensor services in a given service composition. We further propose a model that captures the costs and benefits of maintaining multiple options for services during the selection process and a method that applies it to make such selection decisions.' as abstract.

there is a document named 'doc-1746' that
  has 'Directional forwarding is useful in wireless ad hoc networks, by which the next-hop neighbors are restricted to a directional sector at a specific angle. For example, opportunistic forwarding can avoid duplicate transmissions by selecting a subset of forwarders in a certain direction. Also, emerging directional antenna design enables directional forwarding to improve signal quality, avoid interference, and reduce transmission energy. However, directional forwarding can also diminish the connectivity. In this paper, we study a fundamental problem that arises in a wireless network, relying on directional forwarding. In particular, we focus on wireless networks where nodes are randomly placed with their directional transmission footprints that are aligned toward the data collectors. We aim to shed light on the impact of directional forwarding to connectivity. We first examine through simulation the percolation probability and the number of cross-area paths, at different spread angles of directional transmission footprints. We observe that there is a critical spread angle, above which there is little impact on these properties, while power consumption can be effectively mitigated. Analytically, we derive upper and lower bounds for the critical thresholds. Moreover, we show that with high probability there exist at least _(n/ log n) number of disjoint paths across an strip area of _(n)_n, when above the critical thresholds, which casts light on the capacity of the wireless ad hoc networks in the presence of directional forwarding.' as abstract.

there is a document named 'doc-1747' that
  has 'Although wireless networks have become ubiquitous, surprisingly few models of user-level mobility have been developed and validated against traces of measured user behavior. In this paper, we develop a simple, parameterized, open queueing network model of user mobility among access points in a campus network. Using CRAWDAD traces of user-access-point affiliation over time, we compare model-predicted performance with the performance actually observed in the traces, and find that such a simple queueing model can indeed be used to accurately predict a number of performance measures of interest.' as abstract.

there is a document named 'doc-1748a' that
  has 'A wide range of forwarding strategies have been developed for multi-hop wireless networks, considering the broadcast nature of the wireless medium and the presence of random fading that results in time-varying and unreliable transmission quality. Two recently proposed strategies are opportunistic forwarding, which exploits relay diversity by opportunistically selecting an overhearing relay as a forwarder, and cooperative forwarding, which relies on the synchronized transmissions of relays to reinforce received signal strengths. Although these strategies are well-known in the literature, there lacks a thorough comparative analysis of their network-level performance in a realistic SINR (signal-to-interference-and-noise ratio) setting. In this paper, we develop Markov chain models for these protocols in the case of multiple competing flows in a general network setting. We first evaluate these models in simple small-scale networks, finding that opportunism often outperforms cooperation - a result corroborated by simulations in more general network settings. We also present an approximate fixedpoint model to efficiently compute the throughput of the Markov chain performance models in large networks. We identify the interference resulting from the larger number of transmissions under cooperative forwarding as a cause for mitigating the potential gains achievable with cooperative forwarding.' as abstract.

there is a document named 'doc-1749' that
  has 'In this paper, we investigate how commonly used in-network aggregation approaches impact the target tracking quality in multi-hop wireless sensor networks. Specifically, we use the expected mean squared error (MSE) of the target location estimate to quantify the target tracking quality, and investigate how in-network aggregation affects the expected MSE. We start with proposing an aggregation scheme that preserves a sufficient statistic for making an optimal estimate from the sensor measurements even with data aggregation at the intermediate nodes. Then assuming a Brownian motion mobility model for the target and Gaussian measurement noise for the sensors, we analytically study the impact of aggregation in three increasingly more complicated scenarios that have single or multiple tracking tasks and consider or not consider queuing delay at intermediate sensor nodes. Our results demonstrate that aggregation improves tracking quality in all three scenarios. Furthermore, our analysis provides guidelines on how to choose parameters when using aggregation for target tracking in practice.' as abstract.

there is a document named 'doc-1750' that
  has 'Secure multiparty computation (SMC) is a distributed computation scheme involving N parties, where each party pi has a secret si (for 1 _ i _ N), and each party wishes to learn the result of computing f(s1, ..., sn) for some agreed-upon function f. If the participating parties trust a party pT to keep their secrets, they can send these secrets to pT to compute f and distribute the result. SMC can do better: using a multi-round cryptographic protocol the N parties can compute f themselves in a manner such that no party sees another\'s secret.' as abstract.

there is a document named 'doc-1754' that
  has 'We propose a novel algorithm to determine the availability or the states of services considering the dynamics and scale of the networks. Our approach is based on network tomography and spatial correlation of the services. We use servicelayer dependency grpahs and e2e client-service measurements in a dynamic network to evaluate the internal states of the services. We describe the system and monitoring model that will be employed in our algorithm. We explain the simulation setup of a service-level system that would be used to evaluate our approach.' as abstract.

there is a document named 'doc-1755' that
  has 'In this paper we present a framework and a set of algorithms for determining faults in networks when large scale outages occur. The design principles of our algorithm, netCSI, are motivated by the fact that failures are geographically clustered in such cases. We address the challenge of determining faults with incomplete symptom information due to a limited number of reporting nodes in the network. netCSI consists of two parts: hypotheses generation algorithm, and ranking algorithm. When constructing the hypotheses list of potential causes, we make novel use of the positive and negative symptoms to improve the precision of the results. The ranking algorithm is based on conditional failure probability models that account for the geographic correlation of the network objects in clustered failures. We evaluate the performance of netCSI for networks with both random and realistic topologies. We compare the performance of netCSI with MAX-COVERAGE, and achieve 128% of gain in accuracy.' as abstract.

there is a document named 'doc-1757' that
  has 'In collaborative missions, coalition members depend on one another to deliver on different aspects of shared goals. In such settings, task delegation decisions are complicated because activities of coalition members may be regulated by policies. Especially when such policies are private, learning these policies become crucial to estimate the outcome of delegation decisions. In this paper, we present an approach that utilises domain knowledge in aiding the learning of policies. Our approach combines ontological reasoning, machine learning and argumentation in a novel way to accomplish this. Using our approach, decision makers can reason about the policies that others are operating with, and make informed decisions about to whom to delegate a task. In a series of experiments, we demonstrate the utility of this novel combination of techniques. Our empirical evaluation shows that more accurate models of others\' policies can be developed more rapidly using various forms of domain knowledge.' as abstract.

there is a document named 'doc-1759' that
  has 'The emerging use of mobile ad hoc networks combined with current trends in the use of service-based systems pose new challenges to accurate simulation of these systems. Current network simulators lack the ability to replicate the complex message exchange behaviour of services and service simulators do not provide accurate simulation of mobile network properties. In this paper we provide an overview of a framework for capturing both the service behavioural model and a precise network simulation engine.' as abstract.

there is a document named 'doc-1760' that
  has 'The combination of service-oriented applications, with their run-time service binding, and mobile ad hoc networks, with their transient communication topologies, brings a new level of complex dynamism to the structure and behavior of software systems. This complexity challenges our ability to understand the dependence relationships among system components when attempting to perform analyses such as fault localization. Current methods of dynamic dependence discovery, developed for use in fixed networks, assume that dependencies change slowly. Moreover, they require relatively long monitoring periods as well as substantial memory and communication resources, which are impractical in the mobile ad hoc network environment. We describe a new method, designed specifically for this environment, that allows the analyst to trade accuracy against cost, yielding dynamic snapshots of dependence relationships. Through extensive packet-level simulations, we evaluate the performance of our method in terms of the accuracy of the discovered dependencies, and draw insights on selection of critical parameters under various operational environments.' as abstract.

there is a document named 'doc-1769' that
  has 'Consider a scenario in which two parties A and B hold sets of records RA and RB, respectively, and would like to communicate the join, Join(RA, RB) of the recordsets to another party D. In a distributed environment, A, B and D may not be directly connected, and might require the help of an intermediate node C. If C is not necessarily trusted, then the recordsets must be encrypted, say using the public key PKD of D. On the surface, it would appear that this prevents C from contributing in the computation, leaving no options beyond passively forwarding the encrypted records. In this paper, we study the possibility of allowing C to aid in the computation-ideally by computing an encryption of Join(RA, RB) from the encrypted recordsets. Under certain restrictions, we offer a number of theoretical positive results based upon varying types of homomorphic encryption, all of which provide IND-CPA-type privacy against a passive adversary C. However, under such strong security requirements we show that there are a number of (seemingly fundamental) limitations to employing these techniques in practice. Furthermore, we show partial negative results which illustrate the difficulty of this task when homomorphic encryption is used in a "black-box" way. In light of these results, we also consider relaxed notions of security which admit more efficient solutions.' as abstract.

there is a document named 'doc-1772' that
  has 'The increased capabilities and service requirements of users in wireless mobile networks have put more challenges on the wireless network. In the environments where the networking performance of the services vary frequently, it is important that each user has as accurate information of the up-to-date QoS (Quality of Service) as possible, such as the availability, delay, throughput, etc., from its own perspective, so that the one offering the best quality can be selected at any given moment. Continuously monitoring these performance by each individual node, however, would incur prohibitively high communication and processing overhead. In this paper, we propose a novel service monitoring scheme that utilizes the cooperation between the nodes. In our approach, each node makes use of the end-to- end performance monitoring results measured by other nodes to estimate its own performance to the services in a fully distributed manner. Furthermore, the distributed algorithm does not require explicit structure and coordination between the nodes, making it ideal for highly dynamic networking environments.' as abstract.

there is a document named 'doc-1779' that
  has 'In a predicate encryption scheme an authority generates master public and secret keys, and uses the master secret key to derive personal secret keys for authorized users. Each user\'s personal secret key SKf corresponds to a predicate f defining the access rights of that user, and each ciphertext is associated (by the sender) with an attribute. The security provided is that a ciphertext associated with attribute I can be decrypted only using a personal secret key SKf for which f(I) = 1, i.e., for which the given access rights f allow decryption of ciphertexts having attribute I. Predicate encryption generalizes identity-based encryption, broadcast encryption, attribute-based encryption, and more, and has been suggested as a mechanism for implementing secure information flow and distributed access control in scenarios involving multiple security domains. In this work, we introduce and study the notion of traceability for predicate encryption schemes, thus generalizing the analogous notion that has been defined in the specific context of broadcast encryption. Traceability allows a group manager to apprehend malicious insiders who leak their personal secret keys to an adversary, or to determine which authorized users\' keys have been compromised. In addition to defining the notion, we show how to add traceability to the most expressive predicate encryption scheme currently known.' as abstract.

there is a document named 'doc-1799' that
  has 'We propose the use of a Declarative Networking proxy, called a universal proxy, for securing coalition overlay networks. The primary objective of our approach is to define a framework that provides a high level of flexibility and adaptability to meet stringent security requirements in ad-hoc joint missions.' as abstract.

there is a document named 'doc-1800' that
  has 'In practice, a coalition network often involves entities from different parties. Each entity may be regulated by its own set of private policies, whose evaluation may depend on some private domain information, i.e., the network consists of multiple policy enforcement/decision points (PEPs/PDPs). The formal policy framework we have previously developed for the ITA, which assumes a centralised PEP/PDP, becomes inadequate for policy analysis over such networks, as confidentiality concerns preclude the possibility of centralising both the policies and domain information. In this paper, we extend the existing framework to allow the modelling of a system with multiple PEPs/PDPs and the specification of distributed policies. We then show how DAREC 2 , a logic-based system developed for distributed confidential abductive inference, can be used for solving decentralised policy analysis tasks under this new framework.' as abstract.

there is a document named 'doc-1801' that
  has 'We describe a method for policy refinement. The refinement process involves stages of decomposition, operational- ization, deployment and re-refinement, and operates on policies expressed in a logical language flexible enough to be translated into many different enforceable policy dialects. We illustrate with examples from a coalition scenario, and describe how the stages of decomposition and operationalization work internally, and fit together in an interleaved fashion. Domains are represented in a logical formalization of UML diagrams. Both authorization and obligation policies are supported.' as abstract.

there is a document named 'doc-1803' that
  has 'This paper addresses making access control deci- sions under uncertainty, when the benefit of doing so outweighs the need to guarantee the correctness of the decisions. For instance, when there are limited, costly, or failed communica- tion channels to a policy-decision-point (PDP). Previously, local caching of decisions has been proposed, but when a correct decision is not available, either a PDP must be contacted, or a default decision used. We improve upon this model by using learned classifiers of access control decisions. These classifiers, trained on known decisions, infer decisions when an exact match has not been cached, and uses intuitive notions of utility, damage and uncertainty to determine when an inferred decision is preferred over contacting a remote PDP. There is uncertainty in the inferred decisions, introducing risk. We propose a mechanism to quantify such uncertainty and risk. The learning component continuously refines its models based on inputs from a central PDP in cases where the risk is too high or there is too much uncertainty. We validated our models by building a prototype system and evaluating it against real world access control data. Our experiments show that over a range of system parameters, it is feasible to use machine learning methods to infer access control decisions. Thus our system yields several benefits, including reduced calls to the PDP, communication cost and latency; increased net utility and system survivability.' as abstract.

there is a document named 'doc-1806' that
  has 'This paper looks at the work that has been done on mobility modelling and how this can be pulled through for use on the ITA experimental facility. When running experiments on the experimental framework, researchers would like to subject their applications to realistic approximations of a military environment, which fundamentally incorporate patterns of movement. Previous work has shown that it is nontrivial to generate such scenarios and so the mobility modelling tools were created to simplify this process. The mobility modelling tools can be used to generate configuration files by allowing users to think in terms of higher level military behaviours and doctrine. Secondly the tools can be used to provide randomised variants of movement patterns are vary across a spectrum of characteristics (to enable sensitivity analysis). Thirdly, the tools can provide visualisation capabilities to help show case the experiments in 2D or 3D using VBS2, the MOD standard simulation environment.' as abstract.

there is a document named 'doc-1816' that
  has 'The development of future pervasive computing systems, where information will be distributed on-demand across heterogeneous networks, highlights the necessity for an efficient framework to determine the relevancy of provided information with respect to one\'s needs. This paper considers the problem of selecting the most "spatiotemporally" relevant providers in order to meet a user\'s information needs over a time period of interest. Initially, the spatiotemporal relevancy metric is proposed to measure the degree of relevancy of sensory information with respect to both its spatial and temporal characteristics. Based on this metric, the selection of the most relevant set of providers under budget constraints is expressed as an integer programming optimization problem and a two-level dynamic programming (DP) algorithm is proposed to solve it optimally. Moreover, a number of alternative methods are proposed in order to accelerate the provider selection process by making approximations either to the overall optimization problem formulation or the relevancy calculation method itself. Finally, the performance of the proposed methods are examined both analytically and by simulation for a number of provider scenarios.' as abstract.

there is a document named 'doc-1849c' that
  has 'In a military scenario, commanders need to determine what kinds of information will help them execute missions. The amount of information available to support each mission is constrained by the availability of information assets. For example, there may be limits on the numbers of sensors that can be deployed to cover a certain area, and limits on the bandwidth available to collect data from those sensors for processing. Therefore, options for satisfying information requirements should take into consideration constraints on the underlying information assets, which in certain cases could simultaneously support multiple missions. In this paper, we propose a system architecture for modeling missions and allocating information assets among them. We model a mission as a graph of tasks with temporal and probabilistic relations. Each task requires some information provided by the information assets. Our system suggests which information assets should be allocated among missions. Missions are compatible with each other if their needs do not exceed the limits of the information assets; otherwise, feedback is sent to the commander indicating information requirements need to be adjusted. The decision loop will eventually converge and the utilization of the resources is maximized.' as abstract.

there is a document named 'doc-1855c' that
  has 'We introduce an approach to representing intelligence, surveillance, and reconnaissance (ISR) tasks at a relatively high level in controlled natural language. We demonstrate that this facilitates both human interpretation and machine processing of tasks. More specifically, it allows the automatic assignment of sensing assets to tasks, and the informed sharing of tasks between collaborating users in a coalition environment. To enable automatic matching of sensor types to tasks, we created a machine-processable knowledge representation based on the Military Missions and Means Framework (MMF), and implemented a semantic reasoner to match task types to sensor types. We combined this mechanism with a sensor-task assignment procedure based on a well-known distributed protocol for resource allocation. In this paper, we re-formulate the MMF ontology in Controlled English (CE), a type of controlled natural language designed to be readable by a native English speaker whilst representing information in a structured, unambiguous form to facilitate machine processing. We show how CE can be used to describe both ISR tasks (for example, detection, localization, or identification of particular kinds of object) and sensing assets (for example, acoustic, visual, or seismic sensors, mounted on motes or unmanned vehicles). We show how these representations enable an automatic sensor-task assignment process. Where a group of users are cooperating in a coalition, we show how CE task summaries give users in the field a high-level picture of ISR coverage of an area of interest. This allows them to make efficient use of sensing resources by sharing tasks.' as abstract.

there is a document named 'doc-1903' that
  has 'We present a square root limit on low probability of detection (LPD) communication over additive white Gaussian noise (AWGN) channels. Specifically, if a warden has an AWGN channel to the transmitter with non-zero noise power, we prove that o ( _ n ) bits can be sent from the transmitter to the receiver in n AWGN channel uses with probability of detection by the warden less than  for any  > 0. Moreover, in most practical scenarios, a lower bound on the noise power on the warden\'s channel to the transmitter is known and O ( _ n ) bits can be covertly sent in n channel uses. Conversely, attempting to transmit more than O ( _ n ) bits either results in detection by the warden with probability one or a non-zero probability of decoding error as n _ _. Further, we show that LPD communication on the AWGN channel allows one to send a non-zero symbol on every channel use, in contrast to what might be expected from the square root law found recently in image-based steganography.' as abstract.

there is a document named 'doc-1999' that
  has 'The socially-distributed nature of cognitive processing in military coalitions means that various features of the coalition communication environment (e.g., communication network topology) have the potential to influence collective cognitive outcomes. At the present time, however, we have little understanding of how the specific features of coalition communication environments influence the dynamics of collective cognitive processing. In order to address this issue, a computational model of one particular aspect of cognitive processing, namely collective sensemaking, is proposed. The computational model provides the basis for computer simulations that can be used to provide some initial insight into how collective cognition is affected by a variety of psychosocial and technological variables. In addition to a description of the model, some specific proposals for future simulation work are outlined. The results of experiments using this model can be used to guide decisions about what kind of real-world observational and experimental studies to perform. In addition, when combined with empirical studies that seek to validate model outputs in specific situations, the model can be used to support the development of techniques and technologies that maximize the collective cognitive potential of military coalition organizations.' as abstract.

there is a document named 'doc-2011' that
  has 'The dynamics of MANETs and other networks mean that there is a continual trade-off between the overhead of gathering network state information, and the value derived from obtaining large volumes of information to make a better informed decision. As a result, in almost all situations routing and management algorithms will only have partial information available to them, and so this paper investigates what the optimal sampling strategy should be. We extend previous analytical results through simulation to allow us to characterise the spatiotemporal properties of the optimal strategy as a function of the network stability, network size/shape, and sample size. Results on a simplified network model indicate that there are clear regions where different strategies are optimal, results which can be used to inform the design of future networking protocols.' as abstract.

there is a document named 'doc-2013' that
  has 'We look at the evolution of a fixed size preferential attachment random graph model which is based on the Gaian database. It is well known that the model starts out scalefree. However, we show that as the graph evolves it loses this scale-free nature and becomes more regular as nodes leave and enter the network. This change in characteristic will impact the performance of queries within a Gaian database network. We show that this happens for a wide variety of related models and the impact of capping the number of connections per node is considered. We present some rigorous results, heuristics and computer simulations.' as abstract.

there is a document named 'doc-2014' that
  has 'This paper addresses the behaviour of queries propagated in Gaian Database networks. In particular, we analyse the effect of utilising multiple network paths to achieve loadbalancing of queries around busy nodes and assess the possibility of delegating queries to better connected nodes to reduce the total amount of data propagated in the network. Simulations of query propagation, load balancing and delegation are considered to better understand the behaviour of queries and to optimise the data transmission cost in the Gaian Database. Our findings show quite clearly that load-balancing and delegation can help to improve performance.' as abstract.

there is a document named 'doc-2015a' that
  has 'The cost of query evaluation in a Dynamic Distributed Federated Databases (DDFD) depends on the topology connecting the database nodes together. Different topologies provide opportunities to adopt a variety of query optimisation strategies and topology also influences the efficiency of these strategies. We describe a number of strategies to optimise join queries and then derive cost estimation formulae. We use these to visualize and explore the performance of join queries in different distributed topologies. We consider three candidate topologies; a random preferential attachment network and two hypercube based models. The costs of maintaining these topologies are also formulated and compared. Costs formulas are defined at a coarse grained level, using a small number of parameters and derive only the dominant or average behaviours of the queries and topologies considered. This approach is intended to provide insights to likely optimisation candidates, leading to further refinement of the models. We show that query delegation to well connected hub nodes is beneficial in scale free networks and that knowing the location of the data is beneficial for choosing where to evaluate a query in all topologies. We ascertain that a semi-join strategy can be beneficial in distributed topologies when the join attributes are small, there is repetition of join attributes or the join produces a small number of results. We show that the cost of scaling a network is better in the preferential attachment network, indicating that a hypercube is more suitable for smaller, stable networks or where the rate of queries is high. These results can be used to reduce query evaluation cost in a Gaian Database network by selecting suitable query strategies and network topologies.' as abstract.

there is a document named 'doc-2016' that
  has 'Distributed query optimization is a rich topic extending to the earliest days of database technology. The methods developed for System R in the 80\'s continue to be the most widely used in commercial products. But evolution and diversification of database architecture since the 90\'s has exposed the need for more sophisticated query optimizers. This is particularly true for distributed, heterogeneous, dynamic and/or federated databases. We examine the problem of join query optimization in a Dynamic Distributed Federated Database (DDFD) and the role that network topology, specifically an engineered hypercube, could play in the cost estimation. Considering that many nodes in a DDFD may rely on batteries and wireless radios, we would like to minimize network information transfer to maximize network availability and lifetime. While in any topology it may be assumed a querying node can estimate distance to data sources, in an unknown topology, the querying node has no knowledge of intersite distances. In a hypercube, these distances can be easily calculated at the querying node based on the sites\' labels. What is more, query optimization in the past has not fully taken account of the topology of networks as a cost factor in executing joins. Due to the unavailability of open-source implementations, we have developed and implemented a distributed join query optimizer for comparative performance studies. Our preliminary experiments show that incorporating inter-node distances into the cost model often produces much better plans and never worse, regardless of the search algorithm (greedy or dynamic programming). We confirm a significant advantage of semijoin adoption and quantify the performance of the greedy algorithm in relation to dynamic programming, but acknowledge that a dynamic programming approach may often be unfeasible in a DDFD for large-scale distribution. Based on our findings, we will present some practical considerations for improvement. The main contribution of this research is a quantitative evaluation of distributed query performance that takes account of network topology.' as abstract.

there is a document named 'doc-2017' that
  has 'A Dynamic Distributed Federated Database (DDFD) offers advantages in a variety of applications. In previous work we introduced HyperD, a DDFD whose nodes are arranged in a virtual hypercube structure. We showed that this engineered structure reduces the cost of communication and facilitates processing of complex queries involving joins and semijoins. Moreover, we showed that the additional cost of maintaining this engineered structure is often outweighed by the savings produced. In this paper we present an improved protocol for building and maintaining HyperD. This protocol builds on the version of HyperD introduced earlier, and describes the implementation in greater detail. The main contributions of this paper are theorybased performance analysis and experimental results confirming the theoretical performance characteristics of the HyperD protocol. We show that building a hypercube DDFD is feasible and proves to be beneficial in terms of reducing the communication cost.' as abstract.

there is a document named 'doc-2021' that
  has 'The development of pervasive computing systems and services, where information will be distributed on-demand across heterogeneous networks, highlights the necessity for an effective framework to determine the relevancy of provided information with respect to one\'s needs. This paper considers the problem of selecting the most "spatiotemporally" relevant providers in order to meet a user\'s information needs over a time period of interest. Initially, the spatiotemporal relevancy metric is proposed to measure the degree of relevancy of sensory information with respect to both its spatial and temporal characteristics. Based on this metric, the selection of the most relevant set of providers under budget constraints is expressed as an integer programming optimization problem and a two-level dynamic programming (DP) algorithm is proposed to solve it optimally. Moreover, a number of alternative methods are proposed in order to accelerate the provider selection process by making approximations either to the overall optimization problem formulation or the relevancy calculation method itself. Finally, the performance of the proposed methods are examined both analytically and by simulation for a number of provider scenarios.' as abstract.

there is a document named 'doc-2022' that
  has 'Fast-paced data-to-decision systems are heavily dependent on the reliable sharing of sensor-derived information. At the same time a diverse collection of sensory information providers would want to exercise control over the information shared based on their perception of the risk of possible misuse due to sharing and also depending on the consumer requirements. To attain this utility vs. risk trade-off, information is subjected to varying but deliberate quality modifying transformations which we term as obfuscation. In this paper, treating privacy as the primary motivation for information control, we highlight initial considerations of using feature sharing as an obfuscation mechanism to control the inferences possible from shared sensory data. We provide results from an activity tracking scenario to illustrate the use of feature selection in identifying the various trade-off points.' as abstract.

there is a document named 'doc-2023' that
  has 'In modern coalition operations, decision makers must be capable of obtaining and fusing information from diverse sources. The reliability of these sources can vary, and, in order to protect their interests, the information they provide could be altered, e.g., obfuscated, to limit the inferences that can be made with it. The trustworthiness of fused information depends on both the reliability of these sources and their inference management policies. Information consumers must determine how to evaluate trust in the presence of inference management techniques, such as obfuscation, while information providers must determine the appropriate level of obfuscation in order to ensure both that they remain trusted, and do not reveal any private information. In this paper, we present and formalise trust in the context of inference management and discuss the relationships between the two. We illustration the pertinent concepts via a multi-party coalition scenario, and present numerical examples, using subjective logic computational techniques, of how trust and obfuscation can influence belief levels in information gathered.' as abstract.

there is a document named 'doc-2024' that
  has 'Decision makers (humans or software agents alike) are increasingly faced with the challenge of examining large volumes of information originating from heterogeneous sources requiring them to ascertain trust in various pieces of information. While several authors have explored various trust computation models on static data, past work has typically assumed: (i) a statistically significant number of ratings are available prior to trust assessment, and (ii) assessed trust values tend to vary slowly over time. In contrast, military settings warrant: (i) trust assessment over partial, uncertain and streaming (live and realtime) information from heterogeneous sources, and (ii) coping up with the dynamic and evolving nature of the ground truth. Within the context of executing the OODA loop for decision making, our research objective is to develop a family of trust operators for dynamic information flows for assessing trust over data-inmotion rather than a large corpus of static data. In this paper, we show how to exploit the computational toolset of subjective logic to build a framework for trust assessment in this case. Furthermore, we describe an implementation of the framework using ITA assets (Information Fabric Services and Controlled English Fact Store) and present an experimental evaluation that quantifies the efficacy with respect to accuracy and overhead of the proposed framework.' as abstract.

there is a document named 'doc-2025' that
  has 'Detection and localization of events is an important activity of military coalition networks characterized by distributed sensing and information exchange. However, the variability in mutual trust between members results in the formulation of sharing policies which often require reports to be deliberately obfuscated while maintaining the plausibility of their content. In this paper, using localization reports as evidences in support of (or against) hypotheses about event locations, we develop the foundations of an evidential reasoningbased approach that uses subjective logic for information fusion and inferencing for localization in the presence of incomplete and conflicting knowledge. To do so, we exploit our recent extensions of subjective logic that accommodate the spatial relationships that naturally exists between location reports from the different members. After highlighting our spatial extensions, we apply them in building an inferencing algorithm for a specific example scenario of primary user localization in a cognitive radio network. The reports are provided by various secondary users in the network. Through extensive simulations, we analyze the performance and the effect of various design parameters, showing a 90% accuracy in localization. Finally, we provide comparison results with other localization techniques via simulations.' as abstract.

there is a document named 'doc-2028a' that
  has 'Gennaro et al. (Crypto 2010) introduced the notion of verifiable computation, which allows a computationally weak client to outsource the computation of a function F on dynamically chosen inputs x1, . . . , x` to a more powerful but untrusted server. Following a pre-processing phase (that is only carried out once), the client can send some representation of its input xi to the server; the server returns an answer that allows the client to recover the correct result yi = F(xi), accompanied by a proof of correctness that prevents the server from convincing the client to accept an incorrect result. The crucial property of the scheme is that the work done by the client in preparing its input and verifying the server\'s proof is less than the time required for the client to compute F on its own. In this paper we extend their notion of verifiable computation to the multi-client setting, where N computationally weak clients wish to outsource to an untrusted server the computation of a function F over their joint inputs x1, . . . , xN without communicating with each other. We present a construction for (noninteractive) multi-client verifiable computation based on fully homomorphic encryption, Yao\'s garbled-circuit construction, and any identity-based encryption scheme.' as abstract.

there is a document named 'doc-2031' that
  has 'In sensing applications, sensors with different attributes provide their reports (i.e., subjective opinions) about real-time events. Each such opinion may have different degree of uncertainty and their sources may have different perceived trustworthiness. Subjective Logic provides a unified framework for fusing such uncertain opinions by considering trustworthiness of their sources. DL-Lite is tractable subset of Description Logic that could be used to reason about events and their consequences. A weakness of DL-Lite is its inability to handle uncertainties. In this paper, we combine Subjective Logic and DL-Lite to propose a tractable language for fusing and making inferences over subjective opinions.' as abstract.

there is a document named 'doc-2035' that
  has 'This paper describes the research focus and current work of the ITA project 4 "Shared Understanding and Information Exploitation", which uses a Controlled Natural Language, named Controlled English (CE), to (1) express the results of information extraction from free English text, (2) extend and adapt the information extraction process itself, and (3) allow the user to query the results, infer information relevant to their goals and review the reasoning that led to conclusions. As an unambiguous yet readable variant of natural English, CE allows the expression of information contained in text in a way that makes it readily accessible to a variety of groups and organizations from different nations, without conversion to an underlying technical format. The information extraction system itself is expressed in CE, enabling different specialists, from knowledge engineers and linguists to domain specialists (e.g. military analysts), to more easily add and develop new knowledge. This enables the system to cover new domains, extensions of existing domains and take account of the language needed to express these, extending it in ways needed by diverse users. This paper presents an outline of the current techniques being employed to perform information extraction, along with a brief introduction to the concept of CE and associated technologies. The paper describes how extracted facts may be used in the analysts\' reasoning and how rationale may be constructed from linguistic and analytic reasoning to help review the quality, context and provenance of the inferred information.' as abstract.

there is a document named 'doc-2039' that
  has 'In the presence of vast amount of data and their semantic representation, it is a formidable task for a human decision-maker to effectively locate the most relevant facts, identify critical conflicts, and master a big picture of the information for high quality decision making. This paper proposes a presentation framework which applies argumentation-based reasoning to present relevant facts and answers. Knowledge retrieved from a distributed semantic KB are fed into an argumentationbased reasoning engine which re-organizes the knowledge into coherent arguments, estimates the beliefs of the arguments, and analyzes the pattern of conflicts among the arguments to preliminarily determine the acceptability of these arguments for the decision-maker to review. In order to lower the decision-maker\'s cognitive load, the argumentation is pruned to present only the arguments and the conflicts that most likely concern the decision-maker. This argumentation pruning algorithm can be adapted to enable a decision-maker to interact with the system and navigate through the information incrementally unfolding the argumentation constructed for the answers.' as abstract.

there is a document named 'doc-2041' that
  has 'The unprecedented increase in both the number of data sources and the amount of data available in military coalition networks threatens to slow down and paralyze decision making by overwhelming decision makers with irrelevant data, and to lead to poor decisions as critical relevant facts are lost in the sea of irrelevant data. A similar situation is currently witnessed on the Web of Data (Linked Data): as the semantic web expands, ontological data becomes distributed over a large network of data sources on the Web. In both cases, evaluating queries that aim to tap into this distributed semantic database necessitates the ability to consult multiple data sources efficiently. In this paper, we propose methods and heuristics to efficiently query distributed ontological data based on a series of properties of summarized data. In our approach, each source summarizes its data as another RDF graph, and relevant section of these summaries are merged and analyzed at query evaluation time. We show how the analysis of these summaries enables more efficient source selection, query pruning and transformation of expensive distributed joins into local joins.' as abstract.

there is a document named 'doc-2046' that
  has 'In a coalition network where database servers of multiple parties are linked to facilitate information sharing, a data owner usually needs to authorize different portions of its information to be accessible by different peer parties; consequently, each party has a distinct authorized view over the data stored in the coalition network. Also it is desirable that each party is able to define the authorizations autonomously. The requirements, however, impose new challenges on safe query evaluation, because query planning has to resolve the view discrepancy between parties due to the independently defined authorizations, ensuring each server does not receive data it is not authorized to see, and meanwhile guarantees correct query results. We present an algorithm that, given a set of authorizations and a query plan, generates a safe execution strategy. The algorithm enables per-party access authorization and autonomous access control. Following the principle of efficient distributed computation, it explicitly supports distributed query evaluation. A new join method, named split-join, is proposed, significantly reducing the interserver data transmission cost by pushing the join computation as close as to the data sources. The proofs of the correctness and safety of the algorithm are presented. The performance evaluation result is reported.' as abstract.

there is a document named 'doc-2051' that
  has 'Recent results in the area of distributed systems and networks have promoted the use of declarative languages for the development of executable specifications of distributed systems (e.g. distributed storage systems, sensor networks, and routing protocols). Their underlying principle of using declarative programming methods for expressing distributed states, and specifying strategies for computing and maintaining such states according to local information and communication between nodes, is, however, at the core of distributed computing in large. Leveraging on this body of work, we present a general framework for the analysis of declarative distributed computing applications using Answer Set Programming Solvers. This enables the specification of distributed solutions to computational tasks in terms of declarative programs that are not only executable but verifiable. Programs can be analysed with respect to a variety of properties and under different communication models. We take a simple but sufficiently interesting example in the domain of voting/gossip distributed algorithms to show the kind of analysis possible and report experimental results.' as abstract.

there is a document named 'doc-2052' that
  has 'Researchers have recently shown that declarative database query languages, such as Datalog, could naturally be used to specify and implement network protocols and services. This recognition has opened new avenues for analyzing routing protocols given that the semantics of Datalog rules are well understood, and different forms of declarative programming have been developed to solve NP-hard problems and could be applied. In this paper, we present a declarative framework to implement routing protocols and analyze desirable properties. The main components for the analysis include a protocol model, where the protocol execution is specified, a network communication model, and an analysis model, which specifies a network configuration and the set of properties to verify. We demonstrate the generality of our framework by implementing three broad classes of protocols - a path vector, a link state, and a recently proposed Mobile Ad-Hoc Networks (MANET) routing protocol - and showing how different properties, including convergence, loopfree, and disjoint path routing, can be verified.' as abstract.

there is a document named 'doc-2053' that
  has 'We consider the problem of endhost-based shortest path routing in a network with unknown, time-varying link qualities. Endhost-based routing is needed when internal nodes of the network do not have the scope or capability to provide globally optimal paths to given source-destination pairs, as can be the case in networks consisting of autonomous subnetworks and coalition networks with endhost-based policy constraints. Assuming the source can probe links along selected paths, we formulate the problem as an online learning problem, where an existing solution achieves a performance loss (called regret) that is logarithmic in time with respect to (wrt) an offline algorithm that knows the link qualities. Current solutions assume coupled probing and routing; in contrast, we give a simple algorithm based on decoupled probing and routing, whose regret is only constant in time. We then extend our solution to support multi-path probing and cooperative learning between multiple sources, where we show an inversely proportional decay in regret wrt the probing rate. We also show that without the decoupling, the regret grows at least logarithmically in time, thus establishing decoupling as critical for obtaining constant regret. Although our analysis assumes certain conditions (i.i.d.) on link qualities, our solution applies with straightforward amendments to much broader scenarios where these conditions are relaxed. The efficacy of the proposed solution is verified by trace-driven simulations.' as abstract.

there is a document named 'doc-2057a' that
  has 'Multicast has been well-studied in the networking literature in both wired and wireless network contexts in the last several decades. However, multicast routing under physical or logical communication constraints has not received much atten- tion. Such communication constraints are imposed in scenarios such as coalition networks where two teams are participating in a joint operation, or command-and-control (C2) networks where the flow of control obeys hierarchical relationship regardless of the physical connectivity between the communication devices. In this paper, we first consider the problem of multicast routing in coalition network environments where nodes can talk across coalition boundaries via "gateway" routers. The goal here is to find a minimum cost node-weighted Steiner tree with a novel twist that the node costs are not purely additive - in fact, the overall cost to multicast to the specified sink nodes depends on how many times the gateway nodes had to perform the "gatewaying operation". We show how one can transform (or augment) an input network graph with non-additive costs to another one with additive costs. We then show how existing approximation algorithms for computing Steiner trees can be executed on the augmented graph and still achieve the O(logn) approximation guarantee. We study by thorough simulations the impact of the size and structure of the graph as well as the spatial distribution of coalition nodes and the weights on them on the overall cost of the multicast tree. We also consider the multicasting problem under constraints imposed by the C2 hierarchy. We discuss the initial formulation of the problem and the associated tradeoffs, and some preliminary results on solution approaches.' as abstract.

there is a document named 'doc-2059' that
  has 'Mobile communication platforms of ships, aircrafts, or ground vehicles often operate in highly dynamic conditions with constantly changing infrastructure and access to communication resources. Efficient techniques for rapid and yet stable communication of such fleets with their control centres and between cooperating vehicles within the fleet is a challenging but important area of study with the potential to facilitate the analysis and design of efficient and robust communication systems. Multi-path extensions of data transmission protocols aim to take advantage of path diversity to achieve efficient bandwidth allocation without causing instability. Such multipath resource-pooling extensions of data routing and congestion control intrinsically implement decentralisation with implicit resource sharing. In this paper, we build on the recent theoretical work on fluid model approximations of multi-path TCP as well as the ACITA 2011 demo and study their application to the scenarios in which a convoy with two communication nodes (representing a convoy\'s head and tail) establishes channels with a set of radio/WiFi towers and a satellite relaying information to a remote destination; these channels have time-varying capacities which depend on the position and dynamics of the convoy.' as abstract.

there is a document named 'doc-2060' that
  has 'Since dynamic wireless networks evolve over time, optimal routing computations need to be performed frequently on time-varying network topologies. However, it is often infeasible or expensive to gather the current state of links for the entire network all the time. We provide a thorough analytical characterization of the effect of various link-state sampling strategies operating under a limited sampling budget on the performance of the minimum-latency routing policy in a special class of dynamic networks. We show that for a two-state Markov link-dynamics model parameterized by probabilities p, q, if links are more likely to turn on than off at each time instant (p > q), a "depth-first" sampling strategy is optimal, whereas a "breadth-first" sampling strategy is optimal if links are more likely to turn off than on (p < q)-under the Cut Through (CuT) latency model, i.e., when the packet-forwarding latency is negligible compared to the time scale of the link dynamics. We precisely characterize the optimallatency spatial-sampling schedules for one-shot interrogation. We also present numerical simulation results on comparing various spatio-temporal sampling schedules under an overall sampling rate constraint, and initial results on comparisons of optimal schedules under a Store-and-Advance (SoA) packet-forwarding latency model.' as abstract.

there is a document named 'doc-2062' that
  has 'Cellular networks offer an intriguing opportunity for deployment in the military theater. They can provide broad, long-range coverage and potentially high quality mobile connectivity for the warfighter, using relatively inexpensive standardized industrial components. Cellular technology not only enables mobility, but also allows redundant connectivity using multiple wireless paths to improve availability, reliability, and performance. Multiple paths enable the potential to shift traffic from broken or congested paths to higher-quality ones as traffic characteristics dynamically change, particularly during movement. However, little work has been done to date studying cellular networks or their suitability as a hybrid network. This paper characterizes the behavior of cellular networks and their utility as a hybrid platform, examining 3G, 4G, and WiFi networks for both single-path and multi-path data transport. Our contribution is two-fold: First, we perform measurements of single path transport using TCP over major US cellular wireless networks (both 4G and 3G), and characterize them in terms of throughput, packet loss, and round-trip time. Second, we measure and evaluate transport using multi-path TCP in cellular environments and show that leveraging path diversity under changing environments is a promising solution for more reliable and efficient TCP transfer. We identify potential issues in using multi-path TCP which can limit performance.' as abstract.

there is a document named 'doc-2064' that
  has 'We study the wireless secrecy capacity scaling problem where the question of interest is how much information can be shared among n randomly located nodes such that the throughput is kept information-theoretically secure from m eavesdroppers also present in the network. We present achievable scaling results for both one-dimensional and two-dimensional networks. We show that in a 1-D network, n nodes can share a per-node throughput that scales as 1/n which can be kept secure from m randomly located eavesdroppers of unknown location as long as m grows more slowly than n/ log n. For a 2- D network, the per-node secure throughput scales as 1/ _ n log n for any number of eavesdroppers of unknown location which could be arbitrarily located inside this network. These results provide a significant improvement over previous work which either assumed known eavesdropper locations or the number of eavesdroppers that could be tolerated were very limited. The key technique realizing these improvements is the application of simple network coding methods, which were known to help secrecy in a network but their extension to wireless physical-layer secrecy had been limited.' as abstract.

there is a document named 'doc-2065' that
  has 'The rate at which information can be exchanged between nodes in a hybrid network is investigated. The network includes n ad hoc nodes and b base stations, where the ad hoc nodes can share information through the wired infrastructure in addition to wireless ad hoc communication. The per-node throughput scaling achievable in this hybrid network as n (and b) grows was studied in previous work for both one-dimensional and two-dimensional networks. This work completes this previous work in three ways. First, the wired network is modeled more realistically where the base stations are only connected to their nearest neighbors with finite bandwidth links as opposed to a fully connected network with infinite bandwidth. Second, through cutset methods, we show upper bounds on the throughput which matches the lower bounds for a wide range of values of b. Finally, by establishing a new result on the maximum of a sequence of Poisson random variables (which is of independent interest), we improve previous lower bounds in the extreme case where the number of base stations scale almost on the same order with the number of ad hoc nodes, and also show a matching upper bound in that case for one-dimensional networks.' as abstract.

there is a document named 'doc-2069' that
  has 'We investigate the problem of identifying individual link metrics in a communication network through measuring accumulated end-to-end metrics over selected paths, under the assumption that link metrics are additive (e.g., delay) and constant in the measurement duration. Based on linear algebra, we know that all the link metrics can be uniquely identified when the number of linearly independent paths is equal to the number of links in the network. There lacks, however, a fundamental theory to relate the number of linearly independent paths (and thus link identifiability) to externally observable parameters such as network topology, number of monitoring nodes, and routing restrictions. The aim of this paper, therefore, is to study constraints on the network topology for identifying additive link metrics, conditioned on the number of monitoring nodes being fixed, and cycles being prohibited in constructing measurement paths. Our first main result is that it is impossible to identify all the link metrics in any network with a nontrivial topology (having more than one link) using only two monitoring nodes; nevertheless, the interior links not incident with any monitoring node might be identifiable. Our second main result is a set of necessary and sufficient conditions for identifying all the interior links using two monitoring nodes. Furthermore, we show that these conditions have a natural extension to identifying the entire network using three or more monitoring nodes. To the best of our knowledge, this is the first work providing fundamental constraints on network topology for identifying additive link metrics using end-to-end measurements on cycle-free paths.' as abstract.

there is a document named 'doc-2094a' that
  has 'This paper introduces an algorithm for determining the center-of-data node in a hypercube network. This is useful for reducing network traffic where data from a subset of nodes needs to be consolidated at one location for processing. The algorithm is proven to produce the optimum location for processing if it is given the hypercube addresses of the data source nodes and the volumes of data at each node.' as abstract.

there is a document named 'doc-2103' that
  has 'Basic research on hybrid networks that exploit and interoperate with a diversity of wireless technologies is critical, in particular to develop the mathematical abstractions and frameworks that will enable adaptive control of such hybrid wireless behaviors. This includes establishing fundamental performance limits and characterizing how these limits impact the design of practical adaptive control algorithms, the convergence of these algorithms in a dynamic environment, and the ability to ensure low delays while providing high throughput.' as abstract.

there is a document named 'doc-2104' that
  has 'Although communication networks have been extensively studied in the research literature, little attention has been devoted to hybrid networks comprised of various types of infrastructure (e.g., wifi, satellite, cable). Basic research on such networks is critical, in particular to develop the mathematical abstractions and frameworks that will enable their adaptive control. This includes establishing fundamental performance limits and characterizing how these limits impact the design of practical adaptive control algorithms.' as abstract.

there is a document named 'doc-2113a' that
  has 'Whilst much of the research on authentication in peer to peer networks focuses on distributed authentication services, in current military systems the use of a centralized authority, such as the Kerberos ticketing framework predominates. Kerberos v5 is targeted at giving users access to a specific service with the option of delegating credentials to other authenticated nodes to enable them to act as proxies to access the service. The model does not work in situations where there are many services, distributed across a rapidly changing network, which could respond to a single request. An example of such a distributed set of services is a Gaian Database, where the nodes represent distributed data services and the queries represent the service requests. In this work we describe an extension to the Kerberos ticketing framework that provides the delegated credentials \'on demand\' for nodes that can respond to the service request. We describe an implementation of the protocol that is used to enable authenticated policy-based access control using the Gaian Database to access distributed data sources in a military coalition scenario. The approach has been demonstrated in support of a Coalition Warfare Program (CWP) demonstration held at the NATO International Fusion Centre (IFC) at RAF Molesworth UK.' as abstract.

there is a document named 'doc-2128' that
  has 'The Gaian Database (GaianDB) is a dynamic, distributed federated database which combines the principles of large distributed databases, database federation, and network topology in a changing, ad-hoc environment. The GaianDB uses a preferential attachment strategy to form a scale-free network topology [3]. Preferential attachment algorithm defines that when a new node wishes to attach to the network the node broadcasts to all reachable nodes a message indicating that it is available to join the network; each node receiving the broadcast calculates a time widow based on the number of node connections it has. The node responds to the connection request at a random time within the calculated time window. Nodes with the higher degree preferentially respond faster than nodes with lower degree and connect to the available outgoing edges of the new node.' as abstract.

there is a document named 'doc-2129' that
  has 'The Gaian Database (GaianDB) is a dynamic, distributed federated database where nodes belonging to different parties can connect to each other. Such a hybrid network [1], [2] consists of an interconnected graph of nodes. A node issues a query to the network, which propagates throughout the network. Each node having an answer sends results back along the reverse path of query propagation to the querying node. Gaian database uses controlled flooding to propagate queries. Since the querying node does not know which nodes have answers when a node sends out a query, it is forwarded to all connected nodes. Each intermediate node forwards the first received copy of a query to all neighbours and rejects any further copies of the same query (for the experiments this is considered the "baseline").' as abstract.

there is a document named 'doc-2130' that
  has 'Sensors deployed in the field periodically gather data. Be it visual, audio, or radar data for target identification, they greatly enhance the battlefield awareness and hence the capability of friendly forces. Combining the sensing capabilities of the coalition, by sharing sensor data among members, further enhances the coverage and accuracy of the sensing network.' as abstract.

there is a document named 'doc-2132' that
  has 'Sharing of assets such as sensors and services is essential for effective coalition operations where two or more partners work in cooperation. Typically, user-created tasks compete for the use of relatively scarce assets in a highly dynamic operational environment. Efficient use of resources requires automated support for assigning assets to tasks, that takes into account policies on the sharing of assets. We consider two kinds of high-level sharing policy: the first based on a traditional asset ownership model where assets belong to a coalition partner and may or may not be shared with other partners; and the second based on an "edge" team-based model where users are grouped into teams spanning multiple coalition partners and the team has access to assets from any partner represented in the team. We compare the effect of these two kinds of sharing policy on the performance of an existing asset-task assignment protocol. We find that while the traditional ownership model allows slightly better performance, the difference is only marginal, so an "edge" team-based model offers a viable alternative sharing approach.' as abstract.

there is a document named 'doc-2133a' that
  has 'A number of languages have been developed to concisely express policy, but these are often difficult for a non-technical user to understand. We are investigating the use of ITA Controlled English (CE) to express policies that are both human-readable and machine parsable. We present a simple model for writing authorization and obligation policies in CE. We provide a full example of the policies and rules for a simple messaging scenario including the CE models, facts and rules. The language is compared to the Common Information Model Simple Policy Language (SPL) and limitations of using CE for policy expression are explored.' as abstract.

there is a document named 'doc-2137' that
  has 'In a coalition network where database servers of multiple parties are linked to facilitate information sharing, a data owner usually wants to authorize different portions of its information, in an antonomous way, to be accessible by different peer parties; consequently, each party has a distinct authorized view over the data stored in the coalition network. The requirements, however, impose new challenges on safe query evaluation, because query planning has to resolve the view discrepancy between parties due to the independently defined authorizations, ensuring each server does not receive data it is not authorized to see, and meanwhile guarantees correct query results. We present an algorithm that, given a set of authorizations and a query plan, generates a safe execution strategy. The algorithm enables per-party access authorization and autonomous access control. Following the principle of efficient distributed computation, it explicitly supports distributed query evaluation. A new join method, named split-join, is proposed, significantly reducing the inter-server data transmission cost by pushing the join computation as close as to the data sources. The proofs of the correctness and safety of the algorithm are presented. The performance evaluation result is reported.' as abstract.

there is a document named 'doc-2138' that
  has 'Previous distributed or federated database systems widely assume a central trusted server for query planning and optimization. However, in many coalition scenarios such central server is hard to establish or maintain. In this paper, we propose a decentralized query planning service for data sharing in coalition networks. The policy confidentiality issue during collaboration is also discussed.' as abstract.

there is a document named 'doc-2141a' that
  has 'In addition to the peer/data organization and search algorithm, the overlay network topology (logical connectivity graph) among peers is a crucial component in unstructured peer-to-peer (P2P) networks. Its topological characteristics significantly affect the efficiency of a search on such unstructured P2P networks. Scale-free (power-law) overlay network topologies are one type of the structures that offer high performance for these networks. However, a key problem for these topologies is the high connectivity (i.e., load) in only a small portion (i.e., hubs) of nodes. In fact, the peers in a typical unstructured P2P network may not want or be able to cope with such high connectivity/load. Therefore, some hard cutoffs are often imposed on the number of edges that each peer can have to achieve implementation feasibility and fairness among all peers, resulting in limited or truncated scale-free networks. In this paper, we discuss and analyze the growth of such limited scale-free networks and propose two different algorithms for constructing perfect scale-free overlay network topologies with flexible scale-free growth exponent (_) and with low communication overhead during the construction phase. Through extensive simulations, we also evaluate the performance of proposed approaches and compare them with existing solutions. The simulation results demonstrate that the proposed approaches indeed generate perfect scale free networks and can achieve the best performance in different search algorithms with right parameters with much smaller overhead than the existing solutions.' as abstract.

there is a document named 'doc-2144a' that
  has 'We present a dynamic multipath routing protocol in which packets from different applications dynamically choose their paths by taking into account the price to be paid for taking each path and their ability to pay. We propose a mechanism in which the prices reflect congestion on routers and thus the waiting time for passing the router by a packet and ability to pay is defined by the application priority and the packet waiting time. These prices increase as usage of shorter routes increases. As a result, low priority applications tend to avoid paths with high prices. Instead, they go via low price routes which may be longer but faster to pass by avoid waiting for passage at congested routers. This enables high priority traffic to get through quickly via short paths as their priority (often representing their social value) enables them to pay high prices with little wait. Thus, our approach segregates traffic flows of different applications and lowers congestion and delays for all applications. We further show that our dynamic path allocation technique performs well both in normal and emergency situations in which network is partially damaged. Our dynamic pricing mechanism quickly adapts routing to the damaged network, increases utilization of the partial network and minimizes delays.' as abstract.

there is a document named 'doc-2146' that
  has 'This short paper proposes a trust-based dynamic service composition scheme for heterogeneous tactical networks where resources are restricted and the network operations deployed under time-critical conditions demand fast cadence. Unlike the existing approaches, this work employs composite trust metric so that a user may consider multiple dimensions of trust towards service provider selection aiming to maximize the service satisfaction on the received services. Leveraging the idea of matching required qualifications of service request with the trustworthiness of the service providers, the proposed scheme maximizes the user service satisfaction based on the tradeoff between trust and risk.' as abstract.

there is a document named 'doc-2147a' that
  has 'Service composition in sensor networks combines elementary services with a specific functionality to create a service with higher level functionality. The previous efforts in automating composition were sending full information about all services across the entire sensor network, creating a security risk and imposing significant communication overhead. Furthermore, learning based composition or error detection methods do not consider global information, leading to inefficiencies in the generated composition graphs. In this paper, we propose a probabilistic context-free grammar (PCFG) based modeling technique to construct service compositions. The successful compositions created for the given application are treated as statements belonging to an efficient composition PCFG of this application. The given set of such compositions is used to derive this PCFG automatically. Future composition could be then easily constructed with the help of such PCFG. We present our methodology for achieving such modeling and provide examples of its use to demonstrate its advantage over previous work. We also evaluate the resulting improvements in performance of compositions and in the costs of their creation.' as abstract.

there is a document named 'doc-2153a' that
  has 'In hybrid wireless networks, where the fixed cellular network infrastructure is utilized to provide enhanced network coverage and communication performance for nodes in mobile ad-hoc networks, the selection of the gateway for each node towards the external network needs to be based on accurate and timely network performance perceived by each mobile node. Continuously monitoring these performance metrics by each individual node, however, would incur prohibitively high com- munication and processing overhead. In this paper, we propose a distributed network probing mechanism, called CoPing, that utilizes the cooperation among the nodes in the measurement process of path performance in MANETs towards the gateways of HWNs. In our approach, each node makes use of the end-to-end performance probing results measured by other nodes to estimate its own performance to the gateways in a fully distributed manner. Furthermore, the distributed process does not require explicit structure and coordination between the nodes, making it ideal for highly dynamic networking environments. Through the combination of analysis and experimental evaluation, we show our cooperative probing mechanism can achieve accurate and efficient path performance results for gateway selection in HWNs.' as abstract.

there is a document named 'doc-2168' that
  has 'Modern multi-organisational coalitions are capable of bringing diverse sets of capabilities, assets and information sources to bear on complex and dynamic operations. However, the successful completion of these operations places demands on the trust between coalition partners. When it is necessary to rely on other partners, decision-makers must be able to make rapid and effective trust assessments and decisions. We propose a decision-theoretic model which allows controls to be used, as well as trust, to increase confidence in initial interactions. We consider explicit incentives, monitoring and reputation as examples of such controls.' as abstract.

there is a document named 'doc-2171' that
  has 'In this paper, we propose an algorithm to effi- ciently diagnose large-scale clustered failures. The algorithm, Cluster-MAX-COVERAGE (CMC), is based on greedy approach. We address the challenge of determining faults with incomplete symptoms. CMC makes novel use of both positive and negative symptoms to output a hypothesis list with a low number of false negatives and false positives quickly. CMC requires reports from about half as many nodes as other existing algorithms to determine failures with 100% accuracy. Moreover, CMC accomplishes this gain significantly faster (sometimes by two orders of magnitude) than an algorithm that matches its accuracy.' as abstract.

there is a document named 'doc-2173' that
  has 'This paper describes temporal extensions to subjective logic, allowing an opinion to be temporally discounted through a convolution of the belief mass assignments to a frame of discernment. This convolution is based on a sensor\'s error model, and following its application, standard subjective logic operators can be used to perform further reasoning.' as abstract.

there is a document named 'doc-2174' that
  has 'We investigate trust propagation in delegation situations, which often occur in hierarchical organisations and coalition structures. In doing so we define a delegation chain representing the sub-delegation process. Such delegation chains present a problem for current trust evaluation mechanisms, which are unable to accurately divide trust among the chain members, resulting in degraded system performance. In this paper we investigate the effects of sub-delegation on a probabilistic trust model and propose a model of weighting trust updates based on shared responsibility. We evaluate this model in the context of a simulated multi-agent system and describe how different weighting strategies can affect probabilistic trust updates when sub-delegation is possible.' as abstract.

there is a document named 'doc-2175' that
  has 'This paper describes current work in the experimental exploitation of obfuscation techniques in a distributed, shared, middleware infrastructure. The aim of the work is to develop a system, based upon the ITA Information Fabric, for information sharing in coalition operations with data obfuscation and variable trust. The work carried out to-date meets the following needs: demonstration of the obfuscation of information flowing across and between message/service buses; visualization of the effect of information obfuscation with a representative data set; and the incorporation of information obfuscation techniques in the ITA Information Fabric. This obfuscation capability provides a basis for the practical evaluation of the broader research aims of ITA Project 6, Task 1, "Information Fusion in Coalition Operations under Data Obfuscation and Variable Trust".' as abstract.

there is a document named 'doc-2180' that
  has 'In coalition networks, communication is subject to security constraints in addition to physical requirements. We study the problem of connectivity in a coalition network which includes nodes from two different teams, where two nodes from different teams can only communicate through gateway nodes also present in the network. We present our recent analytical and numerical results on the problem and comment on their potential implications.' as abstract.

there is a document named 'doc-2184' that
  has 'State-of-the-art work on Natural Language Processing (NLP) is undertaken in the DELPH-IN project [7]. This paper explores ITA Controlled English for expressing knowledge in one component, the Linguistic Knowledge Builder, in a more humanunderstandable form, facilitating NLP capability in the ITA whilst retaining human involvement in the processing.' as abstract.

there is a document named 'doc-2186' that
  has 'Representations of ontologies have typically used the Web Ontology Language (OWL), often specifically using the RDF-XML format for representation. This XML representation is useful where machine understanding and processing of the ontology is required. In applications where humans and machines interact to create a conceptual model, using an XML vocabulary may not be the best solution particularly where the human user is a subject matter expert and not skilled in semantic web techniques. Our research into text extraction and information representation using controlled natural languages (CNLs) has identified the need for documenting the provenance of information extracted from a text corpus and its subsequent use by information analysts. In this paper we present a representation of the current W3C draft standard (PROV-DM) using CNL. We discuss how this might provide a narrative of the provenance documentation and describe further work on how provenance causality may provide an annotation framework for representing trust metrics in an information management environment.' as abstract.

there is a document named 'doc-2192' that
  has 'In this short paper, we describe a synthetically gen- erated hierarchically labelled network of nodes with geo-location information that depicts a notional military deployment scenario. We perform a preliminary study of the basic characteristics of the network and explore its use in ITA research.' as abstract.

there is a document named 'doc-2195' that
  has 'We analyze the tradeoff between the amount of signaling overhead incurred in path selection in a MANET with time-varying wireless channels and the application-level throughput and end-to-end power expended on the selected path. Here, increased overhead increases the accuracy of the link-state estimates used in path selection but decreases the amount of bandwidth available for application use. We develop an information-theoretic, bounding approach to quantify the signaling overhead. Specifically, we investigate (i) the time granularity at which link state is sampled and communicated, and (ii) the minimum number of bits needed to encode this link state information, such that the expected power consumption within a sampling interval is minimized subject to a fixed sourcedestination goodput constraint. We formulate an optimization problem that provides a numerically computable solution to these questions, and quantitatively demonstrate that short sampling intervals incur significant overhead while long intervals fail to take advantage of the temporal correlation in link state. Additionally, we find that using a small number bits per sample do not provide sufficient information about the network while using too many bits provide little additional information at the expense of increased overhead. Our work can be used by network operators as a tool to determine parameters like the optimal state update frequency and the number of bits per sample.' as abstract.

there is a document named 'doc-2196' that
  has 'We analyze the tradeoff between the amount of signaling overhead incurred in path selection in a MANET with time-varying wireless channels and the application-level throughput and end-to-end power expended on the selected path. Here, increased overhead increases the accuracy of the link-state estimates used in path selection but decreases the amount of bandwidth available for application use. We develop an information-theoretic, bounding approach to quantify the signaling overhead. Specifically, we investigate (i) the time granularity at which link state is sampled and communicated, and (ii) the minimum number of bits needed to encode this link state information, such that the expected power consumption within a sampling interval is minimized subject to a fixed sourcedestination goodput constraint. We formulate an optimization problem that provides a numerically computable solution to these questions, and quantitatively demonstrate that short sampling intervals incur significant overhead while long intervals fail to take advantage of the temporal correlation in link state. Additionally, we find that using a small number bits per sample do not provide sufficient information about the network while using too many bits provide little additional information at the expense of increased overhead. Our work can be used by network operators as a tool to determine parameters like the optimal state update frequency and the number of bits per sample.' as abstract.

there is a document named 'doc-2238' that
  has 'Fault localization in general refers to a technique for identifying the likely root causes of failures observed in systems formed from components. Fault localization in systems deployed on mobile ad hoc networks (MANETs) is a particularly challenging task because those systems are subject to a wider variety and higher incidence of faults than those deployed in fixed networks, the resources available to track fault symptoms are severely limited, and many of the sources of faults in MANETs are by their nature transient. We present a method for localizing the faults occurring in service-based systems hosted on MANETs. The method is based on the use of dependence data that are discovered dynamically through decentralized observations of service interactions. We employ both Bayesian and timing-based reasoning techniques to analyze the data in the context of a specific fault propagation model, deriving a ranked list of candidate fault locations. We present the results of an extensive set of experiments exploring a wide range of operational conditions to evaluate the accuracy of our method.' as abstract.

there is a document named 'doc-2262' that
  has 'The attached zip file contains all of the resources required to load the ISTAR Controlled English Knowledge Base.' as abstract.

there is a document named 'doc-2439' that
  has 'Consider the scenario where a user wants to perform a keyword search over F. The user could either download the entire file and perform the search himself (but that would be costly) or he could provide the keyword to the server and ask for the result of the search. But in this latter case, he has to trust the server to perform correctly. Is there a way to "secure" this remote keyword search?' as abstract.

there is a document named 'doc-2479' that
  has 'Accurate and timely intelligence analysis is critical for making well-informed decisions in preparation and conduct of any military operation. However, delivering effective analysis demands a high cognitive workload of the intelligence analyst. This is because the analyst needs to collect, structure, and interpret information from diverse and disparate sources to make informed conclusions about evolving situations. Collaborative analysis may reduce this cognitive burden, but requires analysts with different expertise, perspectives and resources to actively work together. Existing analytical tools support individual analysts but may not provide the flexibility required for analysts to work in collaboration. In this paper, we discuss the potential of a system that facilitates the acquisition, annotation, and sharing of intelligence across a coalition. The system supports a hypothesis-driven crowd-sourcing mechanism to select appropriate collaborators for performing analysis. We explore mechanisms to support collaborative evidential reasoning by structuring the analysis using argument schemes and augmenting them with provenance information to enhance information sharing. Finally, we investigate presentation techniques and architectures to support global digestion and propagation of information for enabling a collaborative space for analysis. We argue that this approach facilitates effective intelligence analysis across a team and improves the military decision making process.' as abstract.

there is a document named 'doc-2480' that
  has 'In a coalition, decision makers may cooperate to create plans of activities for fulfilling requirements that are unachievable by individuals. However, decisions made by individuals may conflict with those of others due to concurrent use of resources or different normative expectations. For effective teamwork, mechanisms that enable agreements to be reached resolving interdependencies between plans are essential. Argumentation-based models of dialogue may support decision makers in agreeing what to do analysing pros/cons for the identification of a solution. In this paper, we propose a model of argumentation schemes to be integrated in a deliberative dialogue for the identification of plan, goal and norm conflicts. We employed this model in a dynamic multiagent system, where self-motivated agents engage in dialogue for establishing agreements on interdependent normdriven plans with individual objectives. We show that the use of our model of arguments in dialogue facilitates the resolution of conflicts and the identification of more feasible interdependent plans, through the sharing of focussed information driven by argumentation schemes. We, then, discuss the potential of employing argumentation schemes to support decision makers in revealing structured background information for resolving conflicts between interdependent plans.' as abstract.

there is a document named 'doc-2490' that
  has 'Cognitive social simulations, enabled by cognitive architectures (such as ACT-R), are particularly well-suited for advancing our understanding of socially-distributed and sociallysituated cognition. As a result, multi-agent simulations featuring the use of ACT-R agents may be important in improving our understanding of the factors that influence collective sensemaking. While previous studies demonstrate the feasibility of using ACTR to model collective cognition, as well as sensemaking processes at the individual level, the development of an ACT-R model of collective sensemaking in a coalition environment presents a range of relatively novel methodological, technological and modeling challenges. Such challenges include the need to equip ACT-R agents with communication capabilities, the need to deal with highly dynamic information environments, the need to support intelligent information retrieval capabilities, and the need to represent inter-agent cognitive differences. These challenges shape the nature of research and development efforts to create a multi-agent simulation capability that can be used to explore the impact of different sociotechnical interventions on collective sensemaking processes. In this paper, we discuss the research efforts being undertaken to address these challenges in the context of the International Technology Alliance (ITA) research program. We also discuss the motivations for using ACT-R to model collective sensemaking processes and outline some opportunities for model application and empirical evaluation.' as abstract.

there is a document named 'doc-2492' that
  has 'Several papers (Google scholar indicates about 1000  papers) have adopted the concept of mixzones in road networks for information flow control. In this paper we identify an unaccounted side channel which results in gross underestimation of information release in prior models. We quantify information release and show using both real and synthetic datasets the extent of underestimation may vary between 3-14x.' as abstract.

there is a document named 'doc-2494' that
  has 'This paper presents design and model for policy enabled information flow control of real-time data among multiple administrative domains. In coalition environments, the data sharing among various organizations is a challenging issue, especially in real-life combat missions in which real-time intelligence and reconnaissance data is required at the edge of the network and organizational policies change over time as the situation evolves. The data fusion algorithms producing intelligence information may need data flow from multiple administrative domains and form ad-hoc workflow graphs. For such scenarios, we propose a Streams-Fabric Hybrid Middleware system for such data sharing in real-time. Our design leverages stream processing capabilities of IBM InfoSphere Streams and the lightweight service oriented WSN capabilities of the ITA Information Fabric to extend the data stream analytics to the edge of the network. Using policy enforcement capabilities of ITA Information Fabric, the system enables Streams to exchange data with external platforms. Contributions of this paper are; (1) data and control model for data exchange between different administrative domains, (2) policy enforced information flow control in workflows spanning over multiple organizations.' as abstract.

there is a document named 'doc-2498' that
  has 'Fact extraction from unstructured sources is key to supporting human-machine cognitive tasks, but requires natural language processing (NLP) to provide more comprehensive and formal expression of facts in terms of an agreed conceptual model. We describe BPP13 research to extend the BPP11 work on Controlled English (CE) for output, configuration and rationale in fact extraction, by integrating DELPH-IN linguistic resources; specifically a comprehensive English grammar, (the English Resource Grammar, or ERG), an efficient parser (PET) and a formalism for semantics (Minimal Recursion Semantics or MRS). Integration requires the representation of structures in the ERG/PET/MRS system within CE, based upon a mapping between the underlying Typed Feature Structure representation and the BPP11 common linguistic model in CE. Entries in the lexicon must be mapped into concepts in the domain model; grammar rules must be mapped into CE rules and other CE structures, so that a user can tailor and understand linguistic reasoning on unstructured sources for their own domain; sentence semantics in MRS must be mapped into CE domain semantics in order to extract CE facts for further reasoning. We explore initial mechanisms for these mappings, as we seek to understand how semantic processing by NLP can be deepened to take more account of domain semantics, expressed in CE, improving quality of fact extraction.' as abstract.

there is a document named 'doc-2499' that
  has 'Research is being undertaken into sense-making by collaborative agents, based upon a cognitive framework of human behaviour, ACT-R, together with communication between the agents. We explore the use of Controlled English for this purpose.' as abstract.

there is a document named 'doc-2500' that
  has 'The demonstration 1 will explore enhanced fact extraction from unstructured text documents based upon the integration of comprehensive, state of the art, linguistic resources, grammars and lexicons. We will continue to follow the principles of the BPP11 linguistic research, using Controlled English (CE) to allow the user to build linguistic processing for their own particular domain; this will ultimately involve the construction of a domain model in CE, the linking of the domain concepts to the lexicon, the representation of extracted facts in these domain concepts, the configuration of the processing for specific linguistic phenomena occurring in the unstructured documents to be analysed, the further domain reasoning from these extracted facts into high value information, and the expression of the rationale for the linguistic and domain reasoning in order to explain and review the reasoning by which the high value information has been derived. Such a system will continue to support users such as analysts, where information from structured and unstructured text must be combined to produce a product.' as abstract.

there is a document named 'doc-2501' that
  has 'Decision making processes over large military coalition networks involve querying over multiple dynamic and distributed data sources. Data that is implicit, but relevant to the decision making process, can be inferred by ontological reasoning. Evaluation of such reasoning tasks in a decentralized setting involves redistribution of data across the network to answer multiple join sub-queries. However, it is important to minimize the associated communication costs while querying over constrained military coalition networks with limited bandwidth. We present a baseline approach for distributed reasoning over a set of nodes of a peer-to-peer lightweight dynamic distributed federated database, GaianDB. We present a scalability study of the baseline approach with varying size of data and varying number of data sources. We also present our experience in identifying the performance bottlenecks, and discuss possible optimization techniques to minimize reasoning and communication costs.' as abstract.

there is a document named 'doc-2503' that
  has 'Service-oriented Architectures (SOAs) for Wireless Sensor Networks (WSNs) are an active research topic. Yet, autonomous configuration of services for real life constraints (spatio-temporal, input/output interoperability, policies, security etc.) is still a challenging problem. In domains such as emergency response and military multi-partner coalition operations, constraints applied to auto-configured services are typically regulated through policies. Traditionally these policies are created at the center of the operational network by high-level decision makers and are expressed in low-level policy languages (e.g. Common Information Model Simplified Policy Language) by technical personnel. This makes them difficult to understand and work with by non-technical users operating at the edge of the network. In this paper we investigate the use of Controlled English (CE) as a means to define a policy representation that is both human-friendly and machine processable. We present a policybased SOA management approach by developing a CE domain model that allows CE-expressed policy rules, which are evaluated directly by the service composition process to configure compliant services. The use of a CE policy model is intended to benefit coalition networks by bridging the gap between technical and non-technical users in terms of policy creation and negotiation, while at the same time being directly processable by a policychecking system without transformation to any other technical representation.' as abstract.

there is a document named 'doc-2504' that
  has 'We present a system that is capable of performing real-time sensing, actuation and policy-aware information flow control through an RTD2D-Information Fabric Hybrid Middleware. The framework is suitable for deployment on heterogeneous data collection, analysis and distributed computing platforms. The system leverages the high-speed data stream processing capabilities of IBM InfoSphere Streams and the lightweight service oriented Wireless Sensor Network capabilities of the ITA Information Fabric to extend the data stream analytics to the edge of the network. Using ITA Information Fabric, the system enables IBM InfoSphere Streams to utilize disparate computing platforms and exchange data with external platforms using policy enforcement capabilities of the ITA Information Fabric. The system enriches ITA Information Fabric services with parallel high performance analytics of the backbone network and other salient capabilities such as dynamic workload management of the IBM InfoSphere Streams. With such a system, we aim to (1) exchange knowledge between high end processing platforms and lightweight mobile platforms to enable intelligent decision making, (2) enable policy enforced information flow control, (3) dynamic management of the network and network resources.' as abstract.

there is a document named 'doc-2505' that
  has 'The configuration, monitoring and control of largescale distributed systems is a complex and daunting affair. Policy-Based Management Systems (PBMS) have been proposed as suitable paradigm to reduce this complexity and provide a means for automated administration. One of the reasons why PBMSs are not broadly adopted is that it is difficult to develop and maintain them whilst guaranteeing integrity in terms of policy conflicts. Conflicts occur when two or more policies are applied simultaneously and their resulting actions contradict each other, leading to unpredictable system behaviors. We present a policy conflict analysis model for semiautomatically detecting and resolving static and dynamic conflicts using a Controlled English (CE)-based approach. CE is a Controlled Natural Language which is understood by both humans and machines and in which domain model definition and policy rules execution can be precisely expressed. CE also encourages the development of novel hybrid ways of reasoning and thus conflicts detection. In this demonstration, we show policy conflict resolution in a dynamic service configuration scenario using the ITA Information Fabric, a middleware framework for developing sensor networks, and test it under authorization and obligation policies, which control asset sharing in multi-partner coalition operations in a real military scenario.' as abstract.

there is a document named 'doc-2508' that
  has 'Providing analysts and decision makers with a means of assessing how certain a fact is and what the sources of uncertainty are is an important part of the provenance of a piece of intelligence. A major source of uncertainty derives from the various text reports that analysts and decision makers rely on. One of the major tasks of Project 4.2 is to use CE to extract facts from such reports, infer important information, and provide this to end-users [1]. Part of this involves assessing the degree of uncertainty associated with these text reports, in addition to the reliability of the providers of these reports and expressing these in CE. This paper will investigate the sources of uncertainty that derive from language and its use. This will include explicit expressions of uncertainty, like modal verbs (may, must) and adverbs (possibly, probably), but also various pragmatic sources of uncertainty, including inferences like presuppositions and implicatures and other aspects of how a proposition is framed.' as abstract.

there is a document named 'doc-2513' that
  has 'In this paper, we present the design and evaluation of a decentralized query planning system for federated databases in coalition networks. Besides the goals of reducing the network cost of information sharing, protecting data confidentiality, and protecting query confidentiality, a key motivation of our work is to protect policy (i.e., authorization) confidentiality, which is a unique security problem with coalition networks. We have tested our design on a simulator, and the results show that it is capable in finding efficient and policy-confidentiality-preserving query plans.' as abstract.

there is a document named 'doc-2515' that
  has 'The increasing popularity of smartphones and other similar multi-modal wireless devices has created an opportunity for the realization of large-scale hybrid (or heterogeneous) networks. Typically, modern mobile devices are likely to support a short range communication interface (e.g. IEEE 802.11/WiFi) and/or a longer range communication interface (e.g. cellular data link wireless technology). Multi-hop wireless networking over WiFi can help to extend the range of cellular networks in low SINR regions as well as to alleviate network congestion. Conversely, equipping a few nodes in a mobile ad hoc net- work (MANET) with cellular radios can help to heal wireless network partitions and, thus, to improve the overall network connectivity. One can envision large scale group communication (or multicast) applications including real-time video conferencing (e.g., iPhone FaceTime), P2P video and file sharing, and "voice call groups" in disaster relief and military hybrid networks. In this paper, the problem of resource-efficient multicast in hybrid wireless networks which include both point-to-point (cellular) and broadcast (MANET) links is considered. The underlying op- timization problem is a hybrid of two well-known NP-hard graph optimization problems-the Minimum Steiner Tree problem (for point-to-point links) and the Minimum Steiner Connected Dominating Set problem (for broadcast links). We consider both edge- and node-weighted versions of this problem and use two distinctly different methodologies to formulate two algorithms with guaranteed approximation factors. We demonstrate by means of numerical simulations of standard deployment scenarios that while one algorithm outperforms the other in terms of the tree cost, the latter outperforms the former in terms of complexity and other practical considerations. Finally, using algorithmic ideas from percolation theory, we demonstrate the trade-off between network connectivity and multicast cost, when hybrid capability is added incrementally to the nodes of a MANET-only deployment.' as abstract.

there is a document named 'doc-2517' that
  has 'This paper presents recent advances of our declarative networking (DN) framework for the specification and analysis of declarative distributed computing. These include language refinement and analysis optimisation. For the former, we introduce in our Datalog-based declarative networking language a "prev" operator. This allows us to drop the distinction between transient predicates and persistent predicates, thus benefiting of the following advantages: (i) the body of DN rules, describing state transition, can now refer to both the previous state and the new state without the need for creating auxiliary predicates and introducing additional rules for generating them; (ii) the description of distributed algorithms becomes consequently more compact; and (iii) the computational model of state transitions much simpler. As for the optimisation of our analysis approach, we describe a new transition system-based approach that enables us to overcome a bottleneck limitation of the ASP solver used in our previous pure logic-based model computation approach for the analysis of general properties such as convergence. This new approach is inspired by model checking techniques and integrates both ASP and graph algorithms. We show how, by means of this optimisation, we are able to scale the analysis task of "always converge" for BGP to network topologies with dispute wheels that are twice the size of those considered by other existing analysis approaches, to further emphasise the advantages of our solution. We conclude with a detailed survey of related techniques for analysis of network routing protocol specifications.' as abstract.

there is a document named 'doc-2518' that
  has 'Declarative networking (DN) has recently been proposed as a methodology for specifying and executing distributed applications. Over the past two years we have developed an infrastructure, called Distributed State Machines (DSM), to support the execution of distributed applications, and demonstrated how this infrastructure can be integrated into a debugger tool for helping developers debug and test distributed applications over static network topologies. In the context of dynamic networks, developers phase the challenge of developing distributed applications that are robust to network changes (e.g. lack of connectivity, communication delays, topology changes). Building upon our previous experience on developing debugging tools for DSM, this demonstration illustrates a new integrated tool for developers to simulate dynamic networks and collect and visualise data about the execution of distributed applications, written in DN, in order to evaluate application responses to network changes.' as abstract.

there is a document named 'doc-2520' that
  has 'This paper proposes a novel algorithm for distributed patter matching using our declarative networking framework. The algorithm is intended to address the problem of detecting attacks that exploit multi-__path capability. Monitor nodes cooperate in real-__time, asynchronously, with autonomy and cooperate to detect distributed attacks. The algorithm is assumed to run at this monitor layer. The distributed attack detection problem that we study is defined by an attack signature matching at the destination node where the difference pieces of the attack are assembled. Given an attack signature, defined through a regular grammar, the system dynamically generates and deploys a distributed automaton across the network, allowing the monitors to correlate their states, and effectively identify the specific attack before reaching the destination node. The idea is to express an input automaton for matching signature as an input/output automaton in order to allow the distributed monitors to synchronise their automata by asynchronously exchanging local states. The algorithm is executed and evaluated using the DSM engine, developed by IBM (US), which has already been used to implement a range of distributed applications such as routing protocols, distributed network anomaly detection, and distributed computation of reputation and opinion formation about agents in a network.' as abstract.

there is a document named 'doc-2522' that
  has 'In military coalition operations, it is important to deliver real-time situational awareness (SA) to units on the ground, and this can be enabled through a mobile micro-cloud which utilizes a combination of computing resources at the edge and the core of a tactical hybrid network to run SA applications. One of the challenges in a mobile micro-cloud is to efficiently place application requests onto the computing resources, taking coalition security constraints into account. Towards this end, in this paper, we study security-aware placement of an application. The application is described as a graph with nodes denoting modules that have communication demands between them, and the physical network is a graph with nodes denoting servers with computing resources that are connected by communication links. We abstract the security requirements as domain constraints between nodes in application graph and nodes in physical graph that restrict the set of physical nodes each application node can be mapped to, as well as conflict constraints among nodes in the application graph that restrict the set of application nodes that can be mapped to the same physical node. Based on this abstraction, we formulate the problem as a mixed-integer linear program, and discuss possible techniques of solving it. The proposed work can be regarded as a first step towards a general framework for efficient application placement in a mobile micro- cloud.' as abstract.

there is a document named 'doc-2523' that
  has 'Mobile micro-cloud envisions a logical network composed of two components, the core (e.g., the command and control center) - with access to large quantities of static (and possibly stale) information and the edge (e.g., the forward operating base) - with access to smaller quantities of more real- time and dynamic data. The edge and core are separated by dynamic and performance constrained networks with a many-to- one relationship between the core and the edge. The goal of the mobile micro-cloud is to deliver situational awareness to the small units (primarily interacting with the edge) in a timely and resource aware manner. Fundamental to this mobile micro-cloud paradigm is the flexibility for users to deploy varied applications dynamically as demands, capacity, connectivity, and mission requirements continuously evolve. This "runtime" approach is in contrast to historical systems that are provisioned based on fixed requirements for specific applications. In this paper, we examine various aspects of the mobile micro-cloud. First, we present an approach to deriving semantics for consistent representation of application requirements in order to enable a generic approach to application deployment in the mobile micro-cloud environment. Second, we examine the advantages of migrating an application (or service) to the edge and quantify these gains through preliminary experimental results. Third, we examine the problem of mapping applications (identified for migration) to available resources that are changing dynamically in a Security-aware manner. Finally, we illustrate the prototype platform for the mobile micro-cloud and its characteristics.' as abstract.

there is a document named 'doc-2542' that
  has 'Nowadays, intensive data streams become more and more prevalent in our daily life, such as scalable system logs, network monitoring, social event stream, among many others. To digest such intensive stream, modern data providers usually outsource the data storage and data serving to a third-party Cloud which can not be fully untrusted. Given a query result from the Cloud, a data consumer may need to verify the data correctness and result freshness. In this paper, we address the problem of authenticating the freshness and correctness in an outsourced streaming database. To sign the intensive data stream, we propose an INCBM-TREE that INCrementally builds a Bloom filter- embedded Merkle TREE. INCBM-TREE uses Merkle tree to verify data correctness and uses BLOOM FILTER\'s to verify the data (non)- membership for freshness. Given N memory size, the incremental construction of INCBM-TREE allows for building a data batch of size 2N for signing. Comparing to prior work, the main contribution of this work includes: _ The first work to address the query result freshness in a multi- version data stream. _ A lightweight data signing scheme, which requires small memory footprint and lends itself to a scenario with intensive streams and low-power providers.' as abstract.

there is a document named 'doc-2543' that
  has 'In this paper, we consider the problem of adaptive stream processing in hybrid coalition networks (HCNs). We first introduce and motivate the data stream management (DSM) model that naturally captures the processing, communication and data access characteristics of several real-time applications. Using a situational awareness scenario as motivation, we identify the key challenges and requirements for efficient DSM operation in an HCN environment and then focus on two specific issues. First, we consider routing of real-time, streamed tuples of data (e.g., objects extracted from images) that must be matched against data stored in an archival database in the backend cloud. We outline two approaches, Minimum Delay routing and backpressure, that can be used to dynamically route tuples either to the backend cloud infrastructure for processing, or to a virtual cluster in the MANET in a congestion-sensitive manner. In the first case, real-time image data must be uploaded to the backend; in the latter case, archived data must be downloaded to the MANET\'s virtual cluster. Our second challenge involves adaptive query planning for DSM systems deployed in dynamic HCNs. Here, we introduce a network-aware query planning approach that replicates op- erators within the MANET and then routes tuples to operator replicas over the highest quality available network paths. Our initial experiments show that this approach can achieve more effective stream processing in dynamic networks compared to static query planning, which computes a single deployment plan for a dataflow graph.' as abstract.

there is a document named 'doc-2569c' that
  has 'Recent developments in sensing technologies, mobile devices and context-aware user interfaces have made it possible to represent information fusion and situational awareness for Intelligence, Surveillance and Reconnaissance (ISR) activities as a conversational process among actors at or near the tactical edges of a network. Motivated by use cases in the domain of Company Intelligence Support Team (CoIST) tasks, this paper presents an approach to information collection, fusion and sense-making based on the use of natural language (NL) and controlled natural language (CNL) to support richer forms of human-machine interaction. The approach uses a conversational protocol to facilitate a flow of collaborative messages from NL to CNL and back again in support of interactions such as: turning eyewitness reports from human observers into actionable information (from both soldier and civilian sources); fusing information from humans and physical sensors (with associated quality metadata); and assisting human analysts to make the best use of available sensing assets in an area of interest (governed by management and security policies). CNL is used as a common formal knowledge representation for both machine and human agents to support reasoning, semantic information fusion and generation of rationale for inferences, in ways that remain transparent to human users. A pilot experiment with human subjects shows that a prototype conversational agent is able to gather usable CNL information from untrained human subjects.' as abstract.

there is a document named 'doc-2575' that
  has 'In tactical hybrid wireless networks, network connection problems such as connection failures, excessive delays, link outage, etc., occur frequently due to various reasons including limited network resources, user mobility, unreliable links and equipment failures. Finding out the root-cause of such failures is a challenging issue due to the complexity of how various factors contribute to the given symptoms. In this short paper, we propose a novel analytical framework and algorithms to identify the likely sources of problems experienced by the users in tactical hybrid wireless networks.' as abstract.

there is a document named 'doc-2593' that
  has 'We investigate the problem of identifying individual link metrics in a communication network from accumulated end-to-end metrics over selected measurement paths, under the assumption that link metrics are additive and constant during the measurement, and measurement paths cannot contain cycles. According to linear algebra, all link metrics can be uniquely identified when the number of linearly independent measurement paths equals n, the number of links. It is, however, inefficient to collect measurements from all possible paths, whose number can grow exponentially in n, as the number of useful measurements (from linearly independent paths) is at most n. The aim of this paper is to develop efficient algorithms for constructing linearly independent measurement paths and calculating link metrics. We propose one algorithm which can construct n linearly independent, cycle-free paths between monitors without examining all candidate paths, whose complexity is quadratic in n. A further benefit of the proposed algorithm is that the generated paths satisfy a nested structure that allows linear-time computation of link metrics without explicitly inverting the measurement matrix. Our evaluations on both synthetic and real networks verify the superior efficiency of the proposed algorithms, which are orders of magnitude faster than benchmark solutions for large networks.' as abstract.

there is a document named 'doc-2594' that
  has 'We investigate the problem of identifying individual link metrics in a communication network by measuring end-toend metrics of selected paths between monitors, assuming that link metrics are additive and constant during the measurement, and measurement paths cannot contain cycles. Previous work showed that even the minimum number of monitors required for complete identification can be large in some practical networks, suggesting high monitor deployment cost. We therefore address this issue by proposing an efficient algorithm to place a fixed number of monitors for maximizing network identifiability, with concrete results for the two-monitor case. Evaluation on real ISP topologies shows that although a large number of monitors is required for complete identifiability, we can usually identify a substantial portion of links using two optimally placed monitors.' as abstract.

there is a document named 'doc-2595' that
  has 'In this paper, we study the design of measurements to infer link parameters by measuring end-to-end performance along selected paths. While existing work focuses on the inference problem under given path measurements, the accuracy of inference fundamentally hinges on the set of available measurements. To this end, we propose a generic framework to design measurements for link parameter tomography to allow for most accurate inference of link parameters. Given a link model and a set of path measurements, it is known from estimation theory that the Fisher information matrix (FIM) characterizes a lower bound on the error of the optimal unbiased estimator, asymptotically achievable by the maximum likelihood estimator (MLE). We therefore formulate our measurement design problem to minimize a function of the FIM that represents the total estimation error. We apply this framework to inferring link loss rates from path losses.' as abstract.

there is a document named 'doc-2596' that
  has 'With the popularity of mobile devices and the pervasive use of cellular technology, there is widespread interest in hybrid networks and on how to achieve robustness and good performance from them. As most smart phones and mobile devices are equipped with dual interfaces (WiFi and 3G/4G), a promising approach is through the use of multi-path TCP, which leverages path diversity to improve performance and provide robust data transfers. In this paper we explore the performance of multi-path TCP in the wild, focusing on simple 2-path multipath TCP scenarios. We seek to answer the following questions: How much can a user benefit from using multi-path TCP over cellular and WiFi relative to using the either interface alone? What is the impact of flow size on average latency? What is the effect of the rate/route control algorithm on performance? We are especially interested in understanding how application level performance is affected when path characteristics (e.g., round trip times and loss rates) are diverse. We address these questions by conducting measurements using one commercial Internet service provider and one major cellular carrier in the US.' as abstract.

there is a document named 'doc-2597' that
  has 'MPTCP is a new transport protocol that enables mobile devices to use several physical paths simultaneously through multiple network interfaces, such as WiFi and cellular. However, wireless path characteristics change frequently in mobile environments, causing difficulties for MPTCP. For example, WiFi associated paths often become unavailable as devices move, since WiFi has intermittent connectivity caused by its short signal range and susceptibility to interference. In this work, we improve MPTCP to manage path usage based on the associated link status. This variant, called MPTCP-MA, uses MAC-Layer information to locally estimate path quality and connectivity. By suspending/releasing paths based on their quality, MPTCP-MA can more effectively utilize restored paths. We have implemented and deployed MPTCP-MA in Linux and Android. Our experimental results show that MPTCP-MA can more efficiently utilize an intermittently available path than MPTCP, with Wifi throughput improvements of up to 72 percent.' as abstract.

there is a document named 'doc-2745' that
  has 'Cognitive architectures are computational frameworks that can be used to model aspects of human cognition; however, they are typically used in situations that involve relatively limited forms of perceptuo-motor engagement with external environments. In this paper, we show how ACT-R can be integrated with Microsoft\'s XNA Framework to support sophisticated forms of interaction with 3D virtual environments. The XNA Framework forms part of Microsoft\'s XNA Game Studio, which provides a managed runtime environment that supports the process of video game development. By demonstrating how ACTR can be integrated with the XNA Framework, we hope to show how ACT-R agents could be embedded in a range of virtual 3D multi-player game environments. This capability could be used to support future research efforts associated with the development of computational models of embodied, situated and extended cognitive processes.' as abstract.

there is a document named 'doc-2746' that
  has 'ACT-R is one of the most widely used cognitive architectures, and it has been used to model hundreds of phenomena studied in the cognitive psychology literature. In spite of this, there are relatively few studies that have attempted to apply ACTR to situations involving social interaction. This is an important omission since the social aspects of cognition have been a growing area of interest in the cognitive science community, and an understanding of the dynamics of collective cognition is of particular importance in many organizational settings. In order to support the computational modeling and simulation of socially-distributed cognitive processes, a simulation capability based on the ACTR architecture is described. This capability features a number of extensions to the core ACT-R architecture that are intended to support social interaction and collaborative problem solving. The core features of a number of supporting applications and services are also described. These applications/services support the execution, monitoring and analysis of simulation experiments. Finally, a system designed to record human behavioral data in a collective problem-solving task is described. This system is being used to undertake a range of experiments with teams of human subjects, and it will ultimately support the development of high fidelity ACT-R cognitive models. Such models can be used in conjunction with the ACT-R simulation capability to test hypotheses concerning the interaction between cognitive, social and technological factors in tasks involving socially-distributed information processing.' as abstract.

there is a document named 'doc-2748' that
  has 'Information from unstructured sources is key to human-machine cognitive tasks, but requires Natural Language fact extraction, together with a reasoning capability that allows the user to express assumptions and rules to infer high value information, all based upon a domain conceptual model. We describe research in the use of Controlled English (CE) for fact extraction and reasoning, applied to a complex problem-solving task, the ELICIT identification task, which has posed many challenges.  Our research has integrated the DELPH-IN English Resource Grammar (ERG) to extract detailed linguistic information based upon a deep parsing of sentences, used a CE domain model to guide transformation of the linguistic information to domain facts in a principled way, extended CE syntax to allow meta-reasoning for dynamic rule extraction from sentences, applied assumptions to handle NL ambiguities and track sources of uncertainty such as sentence interpretations and linguistic expressions of uncertainty, and solved a component of the ELICIT task. However significant challenges remain in the search for a general solution to the application of domain knowledge to fact extraction. Even ELICIT sentences show considerable uncertainty and ambiguity that require a detailed understanding of the domain to overcome, and we have therefore started to analyse the specific relationships between domain knowledge and the resolution of ambiguities, in order to make further progress.' as abstract.

there is a document named 'doc-2753' that
  has 'Providing analysts and decision makers with a means of assessing the certainty of information extracted from NL can be an important part of the provenance of a piece of intelligence for decision-making. We outline an analysis of English uncertainty expressions. We show how ITA Controlled English can be used to represent the uncertainty expressions and infer important information in support of decision-making.' as abstract.

there is a document named 'doc-2754' that
  has 'We propose the use of distributional semantics, a well-known technique in Natural Language Processing, to ameliorate the problems posed by out-of-domain vocabulary to reasoning systems that rely on a controlled language. We present a brief overview of distributional methods and sketch an example of how they can be deployed in ITA research.' as abstract.

there is a document named 'doc-2756' that
  has 'ITA Controlled English (CE) is a subset of En- glish that is both human-friendly and machine processable and which allows for inference rules to be expressed that capture knowledge at a number of levels ranging from human "common sense" through to detailed definitions of lexical relationships. CE is being used for automated fact extraction from Natural Language sentences, and addresses the problem of ambiguity (e.g. that "tank" can mean military tank or water tank) by making assumptions and ruling out inconsistent interpretations via the application of domain knowledge. In some circumstances however, a degree of uncertainty may remain which affects the reasoning process, thus leading to undesired conclusions being inferred which can confuse a decision-maker. While there are mechanisms in CE to handle this situation, in this paper we suggest that modern argumentation-based approaches can provide a more comprehensive architecture for dealing with the uncertainty contained in assumptions, and provide a structure in which more knowledge from a wider range of reasoning sources can be applied in a principled way, by reusing effective algorithms developed in the past decades for dealing with argumentation decisions problems. The research question addressed in this paper, illustrated using a military-relevant example, is of interest to the fields of natural language processing, natural language interfaces, argumentation theory and decision making.' as abstract.

there is a document named 'doc-2760' that
  has 'Provenance is the information that represents the lifetime of a piece of data or an object, including how it is affected and changed by other objects, agents, or processes. PROV is a W3C standardised model of provenance designed for use in environments such as the Web. Previous work in the ITA has addressed mapping the logical structures of the PROV data model to Controlled English, allowing us to factor provenance into reasoning and rationale. We extend this existing work, tackling a challenge that arises when converting PROV from RDF to CE: the PROV data model is graph-based, whereas textual documents, including CE documents, are linear in structure. We describe an approach to serialising the provenance graph, that can be used to create not just CE, but additionally CE Gist, and natural language texts, with the aim of increasing the accessibility of the provenance data to human users.' as abstract.

there is a document named 'doc-2762' that
  has 'We describe the results of a demonstration experi- ment, applying elements of ITA human-information interaction technologies to assist in assessing the community impacts of the NATO Summit being held in Newport, South Wales, September 2014. The experiment is centred on the application of (1) Controlled English for knowledge representation and reasoning, and (2) the MOIRA (Mobile Intelligence Reporting Agent) conversational interface for information capture, querying, and sense-making. The main research question addressed is whether these technologies assist subject-matter experts - in this case social scientists with expertise in community policing - to make sense of a complex, evolving situation in real time, drawing on open source intelligence including social media. The experiment focusses on the 3 months leading up to the Summit and aims to address a broad range of impacts including public safety, perceived economic benefits to the host region, and disruption to public infrastructure including transport.' as abstract.

there is a document named 'doc-2763' that
  has 'When using natural language to convey information to describe a scene or event, human subjects use a variety of language constructs to convey quantities and sizes. The implied precision of these quantities ranges from specific values to approximate terms, yet multiple observations of the same scene from different observers can be used to construct an aggregated estimate of the actual value with an associated level of confidence. This paper presents the experiences in creating a human-friendly conceptual model using the Controlled English language and the subsequent usage of this model to facilitate the interpretation of these natural language inputs into the format required by information aggregation algorithms. This is done in the context of a crowdsourced intelligence gathering experiment in which a variety of human users are asked for information relating to the length of lines and waiting times at various locations. The results of the experiment along with some details of the knowledge elicitation exercise used to define the Controlled English models are presented in this paper.' as abstract.

there is a document named 'doc-2767' that
  has 'Data in real world applications is often subject to some kind of uncertainty, which can be due to incompleteness, unreliability or inconsistency. This poses a great challenge for ontology-based data access (OBDA) applications, which are expected to provide a meaningful answers to issued queries, even under uncertain domains. There has been several extensions proposed to classical OBDA systems to tackle this problem, with probabilistic OBDA and fuzzy OBDA being the most relevant ones. However, these extensions present some limitations with respect to their applicability. Probabilistic OBDA deal only with categorical assertions, and fuzzy OBDA addresses the problem of modelling vagueness, rather than uncertainty. In this paper, subjective OBDA is proposed as an extension of classical OBDA systems. Subjective OBDA allows to model uncertainty in the data through the application of opinions, which encapsulate our degrees of belief, disbelief and uncertainty for each given assertion. A method to answer queries under subjective ontologies is presented in this paper, which provides the means to build an opinion associated with each candidate answer for the given query.' as abstract.

there is a document named 'doc-2770' that
  has 'Successful coalition operations require contribu- tions from the coalition partners which might have hidden goals and desiderata in addition to the shared coalition goals. Therefore, there is an inevitable risk-utility trade-off for in- formation producers due to the need-to-know vs. need-to-hide tension, which must take into account the trustworthiness of the other coalition partners. A balance is often achieved by deliberate obfuscation of the shared information. To decide a suitable balance, we describe a unified framework for obfuscating both categorical and semantic data. First, we model domain- dependent knowledge (domain ontology) regarding the shared data and combine it with various privacy-preserving techniques to perform semantic obfuscation. Second, to avoid detection of the obfuscation process itself, the framework defines a novel plausibility metric. This metric ensures that the obfuscated data conforms to the temporal and spatial correlation exhibited by the underlying model for a data source. Finally, the framework uses a game-theoretic approach to model the strategic behavior of members and determine the best obfuscation strategy that maximizes the success of the coalition operation while balancing the risk of sharing.' as abstract.

there is a document named 'doc-2772' that
  has 'We consider the problem of reducing the network monitoring overhead by exploiting measurement correlation. Examples of the considered networks include sensor networks and data centers. In these contexts, sensor readings and resource utilization of servers show correlation over time. We consider the problem of selecting the optimal subset of nodes (sensors, servers, etc.) to monitor the network. On the basis of the measurements provided by these monitors, the remaining (unobserved) measurements are estimated using a framework based on the theory of jointly distributed Gaussian random variables (RVs). We formulate an optimization problem which aims at minimizing the Mean Square Error (MSE) in the estimation of the unobserved measurements under a constraint on the number of observed monitors. We propose an heuristics to efficiently solve this combinatorial problem. Simulations carried out on real traces from sensor networks show that our algorithms are able to significantly reduce the monitoring overhead while achieving close to optimal estimation error measured as MSE.' as abstract.

there is a document named 'doc-2774' that
  has 'Shared information can benefit an agent, allowing others to aid it in its goals. However, such information can also harm, for example when malicious agents are aware of these goals, and can then thereby subvert the goal-maker\'s plans. In this paper we describe a decision process framework allowing an agent to decide what information it should reveal to its neighbours within a communication graph in order to maximise its utility. We assume that these neighbours can pass information onto others. The inferences made by agents receiving the messages can have a positive or negative impact on the information providing agent, and our decision process seeks to assess how a message should be modified in order to be most beneficial to the information producer. The decision process exploits provider\'s subjective beliefs about others in the system, and therefore makes extensive use of the notion of trust with regards to the likelihood that a message will be modified and passed on by the receiver, and the likelihood that an agent will use the information benefitting the provider or against the provider.' as abstract.

there is a document named 'doc-2775' that
  has 'The value offered by a coalition is dependent on the data shared between constituent members. However, sharing too much data poses risks from drawing inferences that may be undesirable to the provider. Thus, there is a need to control shared data such that utility-providing inferences can be accurately drawn whilst undesirable inferences can be prevented. In this paper, we summarize our work towards realizing an inference management architecture in coalition networks. The architecture integrates two key assets, ipShield and the ITA Information Fabric, enabling rule-based control over data at both the edge and the core of the information network for achieving the desired risk and value trade-off. We are evaluating our architecture on the ITA Experimentation Framework which virtualizes various network characteristics that are representative of the military networks. In doing so, our work further extends the functionality of the Experimentation Framework with context-aware message sharing across network tiers.' as abstract.

there is a document named 'doc-2779' that
  has 'Decision support systems must cope with uncertainty and inconsistency that are unavoidable in real world applications. In order to assist decision-makers, such systems must also expose the supporting and conflicting reasons that were utilized in the decision making process. Argumentation theory is a natural tool to apply in such situations. Unfortunately, argumentation-based formalisms often ignore argument strength and its propagation, which are critical in reasoning about uncertainty and conflict handling. Those few systems that utilize it either assume that the strength of arguments is given, or that it is derived from known strengths in an ad-hoc manner (c.f. the weakest link principle). This paper introduces Markov Argumentation Random Fields (MARFs), a framework combining Markov Random Fields and widely used argument labeling techniques. In MARFs, the compatibility of reasoning within arguments establishes potential field for these arguments; the incompatibility of reasoning across defeating arguments is then reflected in the potential difference of arguments. MARFs enable the use of inconsistent; uncertain; incomplete knowledge; and noisy data, thereby facilitating sensitivity analysis of the conclusions and learning argumentative inference structures.' as abstract.

there is a document named 'doc-2780' that
  has 'US Regionally Aligned Forces and UK Military Stabilisation Support Group lack domain models to support information sharing and extraction from civil-military data repositories for planning and training purposes. These repositories contain data on Political, Military, Economic, Social, Information, and Infrastructure (PMESII) effects for critical thinking about the socio-cultural dynamics of civil- military operations, such as the agribusiness and food security domains essential to engagement and building partner capacity. Our research objective is to use Controlled English developed within the International Technology Alliance for fact extraction and reasoning within the civil-military domain. We use natural language text from unclassified civil-military data repositories and doctrine to define key concepts. We aim to represent these concepts in CE, a human writable/machine readable controlled language using the CE Framework developed within the ITA to build a CE domain model for natural language processing and reasoning to improve fact extraction and human-machine interaction. Human reasoning demonstrated within the natural language text drawn from civil-military data repositories will be expressed in CE as rationale statements (reasoning steps). The information contained within these data repositories exemplifies reasoning processes and lessons learned within the civil-military domain that are applicable to the development of a cross- domain model.' as abstract.

there is a document named 'doc-2781' that
  has 'As warfare progresses to a more unconventional environment, maintaining dynamic military intelligence is critical for leaders in the field. Advances in simplified computer languages known as Controlled Natural Languages create opportunities to improve intelligence gathering and dissemination. We have envisioned a living knowledge base that records biographical data on insurgents, contacts, and civilians in the field. The soldier would have the ability to add to, remove from, or edit existing information using a programming language that is designed to be quickly understood. With the conversational interface MOIRA system developed under the International Technology Alliance program [1], the user can request graphical and visual information concerning interpersonal, inter-village, or inter-tribal networks, IED and enemy force heat maps, and relevant biographical data of any subject within his/her area of operations. This can be potentially adapted to existing technology such as a Blue Force Tracker, simple GPS systems, or Google Glass.' as abstract.

there is a document named 'doc-2783' that
  has 'Obtaining good performance for declarative query languages in a centralized or distributed setting requires an optimized total system, with an efficient data layout, good data statistics, and careful query optimization (e.g. [1]). One key piece of such systems is a query planner that translates a declarative query into a concrete execution plan with minimal cost. This problem has been extensively studied - in particular, in the relational database literature [2], [3]. The traditional solution builds a cost-model that, based on data statistics [4], [5], is able to estimate the cost of a given query execution plan. However, since the number of execution plans can be extremely large, only a small subset of all valid plans are constructed (using heuristics and/or greedy approaches that consider plans likely to have a low cost) [6]. The cost of those selected candidate plans are then estimated using the cost-model, and the cheapest plan is selected for execution. The chosen plan is a local optimal and not guaranteed to be a global optimal. Even with sub-optimal plans, the performance of an optimizer is still considered satisfactory, if it performs better (in terms of evaluation times) when compared to other competing optimizers. Yet, there is an alternative metric to measure how well the optimizer performs: how far its local optimal plans are from global optimal plans. However, finding a global optimal is challenging and is one of the reasons why the heuristic planners were devised in the first place. To the best of our knowledge, there is no practical mechanism for assessing how good these planners are, i.e. whether they produce optimal plans given the data layout and statistics available.' as abstract.

there is a document named 'doc-2784' that
  has 'The dynamic environment of Hybrid Wireless Networks used in military operations, poses significant challenges to efficient provisioning of functionality of service-based systems to client applications. With the transient topology, the services hosted on mobile nodes may become temporality unavailable or the cost of transferring data across distant parts of network too high. To address this problem we have designed a new dynamic service placement mechanism, which allows to dynamically re-place services within the network. The mechanism places services to reflect the changes in the transient network topology as well as in the various system properties such as the workload generated by the client applications. In our approach we use a dynamic multilayer model to represent the service graph which is embedded into a topology graph representing the network nodes. On this model we apply an optimization algorithm to find the optimal placements of services. The mechanism is repeatedly executed to adjust the placements of services as changes in the network topology and system use occur. Our approach makes use of integer constraint programming and its linear programming relaxation to find the service placements and we compare their effectiveness and cost.' as abstract.

there is a document named 'doc-2785' that
  has 'The management of large scale service based systems hosted in dynamic environments of hybrid wireless networks is a complex task. Such systems are designed as composite services integrating heterogeneous service components such as sensor, software and data services. In this paper, we propose a new situational aware framework which allows the network operators to effectively manage such complex systems. The framework integrates three key components. The Service Placement component and the Configuration Service which collectively arrive at decision about placements and compositions of the services. An instrumental element of the framework is the Policy Based Management System providing the network operators with a tool to dynamically control these mechanisms.' as abstract.

there is a document named 'doc-2786' that
  has 'We are concerned with reliably harvesting timebounded, time-sensitive time-series data collected in a mobile ad hoc network (MANET) environment used in military operations. The data describe the state of the network and hosted applications. Harvesting is used in time-series analyses requiring a global view of distributed monitoring data. The MANET environment challenges data harvesting, due to inherently unstable and unpredictable connectivity and the resource limitations of wireless devices. We present an epidemic, delay tolerant method to improve the availability of time-series monitoring data in the presence of network instabilities, asymmetries, and partitions. The method establishes a network-wide synchronization overlay to incrementally and efficiently move data to intermediate nodes. We have implemented the algorithm in Java EE and evaluated it in the CORE and EMANE MANET emulation environments.' as abstract.

there is a document named 'doc-2787' that
  has 'Complexities in current military operations increase the cognitive burden placed on intelligence analysts, as they need to process a vast amount of information. In this paper, we propose different means to support analysts in assessing the credibility of information to identify plausible hypotheses. We present a method for processing structured information about events observed at the network edge. A probabilistic model for truth discovery is proposed to assess the credibility of data in the presence of unreliable participants. However, when information is derived from less structured reports, analysts must reason about the provenance of information. Thus, we propose a model of argument schemes for exploring provenance in order to improve the information assessment. Argument schemes, as patterns of reasoning, present essential provenance elements to reason about the credibility of information according to its timeliness, reliability, and trustworthiness. These methods are aimed at a faster identification of credible information within CISpaces, a space for intelligence analysis that enables collaboration among analysts within a coalition. We believe that the integration of these techniques in CISpaces will facilitate the delivery of more robust intelligence products to military decision-makers.' as abstract.

there is a document named 'doc-2788' that
  has 'Trust and reputation are significant components in open dynamic systems for making informed and reliable decisions. State-of-the-art information fusion models that exploit these mechanisms generally rely on reports from as many sources as possible. Situations exist, however, where seeking evidence from all possible sources is unrealistic. This is particularly the case in resource-constrained environments, where querying information sources is costly in terms of energy, bandwidth, and delay overheads. We present an approach that exploits diversity among information sources to stratify the population into homogeneous subgroups to mitigate the effect of certain biases. We show empirically that this approach is robust in contexts of variable trust in information sources, and to a degree of deception.' as abstract.

there is a document named 'doc-2792' that
  has 'Argumentation is an important mechanism for nonmonotonic reasoning. Extending argumentation to permit reasoning about uncertainty would result in reasoners capable of operating in a wide variety of realistic domains. This paper describes one such formalism, which associates uncertainty with abstract argument, thereby allowing us to determine the level of uncertainty with respect to argument justification. We describe an application of this approach within CISpaces, a virtual space for collaborative intelligence analysis among coalitions. Uncertainty is present in the incoming information at the basis of the hypotheses identified during the analysis. By adopting our approach, CISpaces provides support to analysts in identifying how likely the hypotheses are justified.' as abstract.

there is a document named 'doc-2794' that
  has 'In a military coalition, the interests of the members are generally aligned, at least as regards their joint missions, but with members representing different nation-states, their strategic interests are complex, and this alignment of interests is inevitably imperfect. Members derive utility from the success of coalition missions, where success is contingent upon cooperation between members. However, each member might also wish to protect their strategic assets. The risk incurred due to sharing too much information can discourage cooperation or incentivize the deliberate obfuscation of information prior to sharing, which could jeopardize the mission. In this paper, we assume rational players and analyze the information-flow dynamics within a coalition using a game-theoretic model where each member tries to maximize its utility (due to joint mission success) while minimizing the risk it incurs by contributing its own intelligence to the mission- i.e., a collective action problem. As each member\'s utility and the risk functions (which together determine the payoff) are private and not known to the other coalition members, we frame this situation as a repeated (incomplete information) Bayesian game, where each round represent another coalition mission. In each game, the strategy of a member is to share obfuscated information at one of several possible discrete quality levels. We study the existence of Bayes-Nash equilibria of this game to derive the equilibrium strategies.' as abstract.

there is a document named 'doc-2796' that
  has 'Trends in artificial intelligence, machine learning, and the Internet of People and Things (IoPaT) point to increasing intelligence and autonomy of services in coalition operation environments. Exploiting these trends in support of unified mission objectives, while mitigating undesirable effects, will be a key challenge in the next few years. Highly complex, dynamically composable services with significant autonomy, cooperating across coalitions, will require new robust management methods. Autonomous systems and services will only be accepted if they are able to account for their actions in terms of explanation, audit trails, provenance, compliance with policies/norms, assumptions, and so on. Accountability will need to be designed into the increasingly rich human-machine and machine-machine interactions that will emerge, including both individual and collective systems, e.g. swarming behaviour based on large numbers of interoperating "things" in the IoPaT sense. In this context, accountability - of individual elements of such complex systems, and of the whole systems - needs itself to be a service. Achieving "accountability as a service" will require research in models, algorithms, and systems for effective, verifiable, and communicable system accountability. The human-computer collaborative aspects of this require an interdisciplinary approach, involving both computer science/engineering with social sciences. In this paper, we review elements of ITA technology - centered on approaches to human-in-the-loop crosscoalition service provision.' as abstract.

there is a document named 'doc-2798' that
  has 'Services management in large-scale service oriented sensor networks carries many challenges. Such networks consist of heterogeneous assets that host a variety of services and maintain perpetual information flow/fusion between hard (i.e. sensing devices, distributed databases) and soft (human) resources. Regulating access to assets and services using policies has proven to be effective in many ways especially when resources are affiliated with multiple parties in coalition formations. In this paper, we propose a two-layer novel service management mechanism: the first layer directly interfaces with humans and is based on controlled natural language (CNL) technologies; and the second interacts with the system level policies and are interpreted utilizing Restriction Set Theoretic Expressions (RSTE). The RSTE layer is responsible for the policy evaluation by limiting the resources available for service compositions pertaining to a user\'s requests, and facilitates the reporting to the upper-layer of over-restrictive policy constraints that may hinder the service compositions process. Finally, we describe a policy relaxation mechanism between coalition collaborating parties, when services are not configurable given the strict policies currently enforced. The relaxation mechanism exploits the ability of RSTE to flow information from service composition to upper layer, which interacts with human-in-the-loop in a transparent way.' as abstract.

there is a document named 'doc-2804' that
  has 'This paper outlines the formulation of a methodology to research aspects of a Mobile Micro Cloud in a military context. The goal is ensure that research in this area is targeted toward both interesting and challenging problems, and also toward aspects of the problem that are specific and unique to future military operational needs, whilst exploiting techniques that are prevalent in the commercial arena.' as abstract.

there is a document named 'doc-2837' that
  has 'We investigate the problem of localizing node failures in a communication network from end-to-end path measurements, under the assumption that a path behaves normally if and only if it does not contain any failed nodes. To uniquely localize node failures, the measurement paths must show different symptoms under different failure events, i.e., for any two distinct sets of failed nodes, there must be a measurement path traversing one and only one of them. This condition is, however, impractical to test for large networks due to the combinatorial numbers of paths and failure sets. Our first contribution is a characterization of this condition in terms of easily verifiable conditions on the network topology with given monitor placements under three families of probing mechanisms, which differ in whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cyclefree, or (iii) uncontrollable (i.e., determined by the default routing protocol). Our second contribution is a characterization of the maximum identifiability of node failures, measured by the maximum number of simultaneous failures that can always be uniquely localized. Specifically, we bound the maximal identifi- ability from both the upper and the lower bounds which differ by at most one, and show that these bounds can be evaluated in polynomial time. Finally, we quantify the impact of the probing mechanism on the capability of node failure localization by numerically comparing the maximum identifiability under different probing mechanisms on both random and real network topologies. We observe that despite a higher implementation cost, probing along controllable paths can significantly improve a network\'s capability to localize simultaneous node failures.' as abstract.

there is a document named 'doc-2838' that
  has 'We study experiment design to infer link loss rates from end-to-end losses on selected paths using network tomography. Since the inverse Fisher information matrix (FIM) establishes a lower bound on the error of any unbiased estimator, we formulate the problem as the design of probabilities in selecting probing paths to minimize an objective function based on the FIM. We consider two widely-adopted objective functions: the determinant of the inverse FIM (D-optimality) and the trace of the inverse FIM (A-optimality), where the former characterizes the volume of error ellipsoid and the latter characterizes the sum mean square error (MSE). Using a special property of the FIM, we obtain closed-form expressions for both objective functions, which lead to closed-form solutions for the optimal path selection probabilities. In particular, we show that the D-optimal design is uniform probing, i.e., probing each path with an equal probability. We verify through simulations that the A-optimal design can reduce the MSE compared with uniform probing.' as abstract.

there is a document named 'doc-2839' that
  has 'We investigate the problem of placing a given number of monitors in a hybrid communication network to identify the maximum number of link metrics from end-toend measurements between monitors, assuming that link metrics are additive, and measurement paths cannot contain cycles. Motivated by our previous result that complete identification of all link metrics can require a large number of monitors, we focus on partial identification using a limited number of monitors. The basis to our solution is an efficient algorithm for determining all identifiable links for a given monitor placement. Based on this algorithm, we develop a polynomial-time greedy algorithm to incrementally place monitors such that each newly placed monitor maximizes the number of additional identifiable links. We prove that the proposed algorithm is optimal for 2-vertexconnected networks, and demonstrate that it is near-optimal for several real ISP topologies that are not 2-vertex-connected. Our solution provides a quantifiable tradeoff between level of identifiability and available monitor resources.' as abstract.

there is a document named 'doc-2842' that
  has 'Collaborative query processing, in which the querier outsources some query processing tasks to collaborating parties, reduces the cost of answering queries in distributed database networks. To prevent data leakage, authorizations between multiple parties are enforced during query planning and execution. However, other types of sensitive information, such as authorization information, could be leaked through certain inference channels during query execution. In this work, we first formulate the authorization information leakage problem and analyze possible inference channels. Then we propose a new policy specification to control authorization information leakage, and a new query planning algorithm to enforce the policy and further reduce the risk of information disclosure. Our evaluation shows that the algorithm can reduce 71.1% leakage with 7.5% increase of execution cost on average.' as abstract.

there is a document named 'doc-2844' that
  has 'Multi-Path TCP (MPTCP) is a new transport protocol that enables systems to exploit available paths through multiple network interfaces. MPTCP is particularly useful for mobile devices, which usually have multiple wireless interfaces. However, these devices have limited power capacity and thus judicious use of these interfaces is required. In this work, we develop a model for MPTCP energy consumption derived from experimental measurements using MPTCP on a mobile device with both cellular and WiFi interfaces. Using our energy model, we identify an operating region where there is scope to improve power efficiency compared to both standard TCP and MPTCP. We design and implement an improved energy-efficient MPTCP, called eMPTCP. We evaluate eMPTCP on a mobile device across several scenarios, including varying bandwidth, background traffic, and user mobility. Our results show that eMPTCP can reduce the power consumption by up to 15% compared with MPTCP, while preserving the availability and robustness benefits of MPTCP. Furthermore, we show that when compared with TCP over WiFi, which is more energy efficient than TCP over LTE, eMPTCP obtains significantly better performance with relatively little additional energy overhead.' as abstract.

there is a document named 'doc-2845' that
  has 'As a rule, a communication network does not expose its complete structure to end systems. Consequently, end systems must rely on a single path provided by the network. The recent development of multipath data transport protocols (e.g., multipath TCP), has changed this by allowing end systems to explore and more fully use path diversity that often exists within networks. However, this relies on end-systems being multi-homed, which is often not the case. This paper explores the benefits that traffic-splitting functionality at intermediate data forwarding nodes can provide single-homed end systems with for exploring and utilizing available path diversity when using multipath data transport. This paper aims to study and quantify multipath transport control in conjunction with traffic splitting and the effectiveness of different traffic splitting architectures.' as abstract.

there is a document named 'doc-2846' that
  has 'Video streaming through mobile devices has become extremely important in the battlefield for the purpose of surveillance and content delivery. Soldiers in a coalition alliance normally have access to different networks of different affiliations to stream desired video content. Popular video content will be replicated at different sites within a US/UK coalition network, and users can stream videos from close-by locations with low latency. Users constantly seek to stream high quality videos for better experience under the constraint of limited bandwidth in the military scenario. As most mobile devices in a coalition exercise are equipped with multiple wireless interfaces connecting to different authorized networks, aggregating bandwidth for high definition video streaming has become feasible. We propose a client-based video streaming solution, called MSPlayer, that takes advantage of multiple video sources in the coalition network as well as network paths through different interfaces. MSPlayer reduces start-up latency, and aims to provide high quality video streaming and robust data transport in mobile scenarios. We experimentally demonstrate our solution through the largest commercial video service.' as abstract.

there is a document named 'doc-2849b' that
  has 'Recent advances in natural language question- answering systems and context-aware mobile apps create op- portunities for improved sensemaking in a tactical setting. Users equipped with mobile devices act as both sensors (able to acquire information) and effectors (able to act in situ), operating alone or in collectives. The currently-dominant technical approaches follow either a pull model (e.g. Apple\'s Siri or IBM\'s Watson which respond to users\' natural language queries) or a push model (e.g. Google\'s Now which sends notifications to a user based on their context). There is growing recognition that users need more flexible styles of conversational interaction, where they are able to freely ask or tell, be asked or told, seek explanations and clarifications. Ideally such conversations should involve a mix of human and machine agents, able to collaborate in collective sensemaking activities with as few barriers as possible. Desirable capabilities include adding new knowledge, collaboratively building models, invoking specific services, and drawing inferences. As a step towards this goal, we collect evidence from a number of recent pilot studies including natural experiments (e.g. situation awareness in the context of organised protests) and synthetic experiments (e.g. human and machine agents collaborating in information seeking and spot reporting). We identify some principles and areas of future research for "conversational sensemaking".' as abstract.

there is a document named 'doc-2853' that
  has 'The overarching goal of ITA Project 1, Task 3, is to construct, analyze and evaluate the performance of a multicast communication architecture for a hybrid network that is robust under node mobility. In this short paper, we describe our research program for this task, and outline new results from four inter- related threads aimed at addressing distinct dimensions of the underlying problem. The setup is a mobile adhoc network in a spatial region equipped with a cellular base-station infrastruc- ture. We show that when a small fraction of the (broadcast) MANET nodes is equipped with (point-to-point) cellular wireless (i.e., 4G) capability, spanning connectivity in the network may emerge when the MANET itself is not fully connected. We show that the cost of broadcast, and multicast communication to a small set of terminal nodes continue to diminish as this hybrid network is driven deeper into the supercritical-connected regime. We analyze uniform-mobility-driven dynamics in a network- both for spatial as well as for random networks with arbitrary degree distributions-and find exact results on the number of mobile walkers that, in a given topology, would result in a giant connected cluster that remains stable in time. We also perform a temporal stability analysis of this spanning cluster as the number of mobile nodes crosses into the percolating region. We then propose two distinct algorithms-adapting two well-known Steiner-tree algorithms-to construct a multicast tree for the hybrid network, and do a comparative evaluation of their runtime-vs.-cost performance. Finally, we describe our initial results on bringing together the analyses on connectivity- vs.-mobility, and multicast-cost-vs.-connectivity, along with an extension of the aforesaid algorithms, which permit introducing a controllable amount of link-redundancy in the multicast- subgraph construction. Our goal is to evaluate the tradeoff between the cost overhead associated with communicating over a multicast subgraph in the hybrid network with a tunable amount of redundancy built in to account for link failures, versus the robustness of multicast under node dynamics, where robustness is defined as the time horizon for re-computation of the multicast subgraph.' as abstract.

there is a document named 'doc-2870' that
  has 'We investigate the problem of delay-optimal stream routing and content caching in a cellular/MANET hybrid coalition network (HCN) which processes data streams of HCN user requests for content and supports in-MANET content caching. The users can always access content at a back-end server via the cellular infrastructure; alternatively, they can access content via cache-equipped MANET nodes. To access content, users must thus decide whether to route to in-MANET caches or to the back-end server via the cellular infrastructure; the in-MANET cache nodes must additionally decide which content to cache. We model the cellular path as either i) a congestion-insensitive fixed delay path or ii) a congestion-sensitive path modeled as an M/M/1 queue. We show that the problem is NP-complete in both cases. We prove that under the congestion-insensitive model the problem can be solved optimally in polynomial time if each piece of content is requested by only one user, or when there are at most two caches in the network. For the congestion-sensitive model, we prove that the problem remains NP-complete even if there is only one cache in the network and each content is requested by only one user. We show that approximate solutions can be found for both models within a (1 _ 1/e) factor of the optimal solution, and demonstrate a greedy algorithm that is found to be within 1% of optimal for small problem sizes. Through tracedriven simulations we evaluate the performance of our greedy algorithms, which show up to a 50% reduction in average delay over solutions based on LRU content caching.' as abstract.

there is a document named 'doc-2871' that
  has 'We consider the problem of adaptive stream query planning in hybrid coalition networks (HCNs) formed from mobile ad-hoc networks and cellular infrastructure. Previous approaches to adaptive query planning are targeted to datacenter environments where network variability is low and centralized control is available, and so adapt only to changes in data characteristics. The distributed, bandwidth-constrained and highly dynamic nature of HCNs render such adaptation insufficient - while a query plan executes, changes in the network mean that any fixed query plan will eventually become outdated. We introduce a network-aware framework for adaptive query planning in HCNs that supports both stateless and multi-input stateful operations for a fault-tolerance model with strong reliability guarantees of exactly-once delivery. We demonstrate our framework\'s performance benefits in a simulated mobile ad-hoc network scenario.' as abstract.

there is a document named 'doc-2998' that
  has 'Large organizations by design tend to suffer from a bureaucratic buffer that obstructs efficiency and stalls progress. The military, being one of these organizations, has increasingly acknowledged the need to optimize interactions at all levels of the force, especially in tactical environments that have a strategic importance. Recently, a potential solution that has emerged with the rise of technology is the usage of Controlled Natural Language in developing a platform that can facilitate an expansion of crowd-sourced intelligence gathering and a more secure and effective information dissemination. This software seeks to concurrently improve the capabilities of soldiers on the ground while providing a higher level of understanding and perspective to the strategic leaders of the military. In order to kick start our understanding of the practical usage of such technology, we developed a simulation with a simple storyline and implanted characters, scenes, and other objects. Using this simulation, our goal is to see if users are able to properly receive the information we feed into the scenes and to communicate that information to the machine agents to refine a "bigger picture" perspective on the entirety of our storyline. From here, a higher level of understanding is reached and the human agents can be involved to decide the key intelligence that needs to be transmitted back to the agents at the tactical edge. This paper outlines our conceptual understanding of the status quo of the military and how Controlled Natural Language software could fill the gaps created by a bureaucratic delay in intelligence gathering and the subsequent dissemination.' as abstract.

there is a document named 'doc-828' that
  has 'Overlay networks have been studied extensively in recent years as a flexible means to improving the reliability, resiliency, and performance of many networking applications. In this paper we present a novel use of overlay networks and distributed mechanisms to construct them for handling information assurance issues in networking systems. The problem is explored in the context of constructing an overlay that satisfies a given access control policies in decentralized information sharing systems. We formulate a new graph-theoretic optimization problem of constructing a minimum policy-compatible graph, which is NP-complete. We provide efficient centralized and fullydistributed heuristics, and prove the convergence property of the distributed process. Our simulation study with synthetic and empirical data set shows that our methods result in the performance (in terms of total number of links) very close to the optimal case (within 3%) for small input, and that they can reduce the number by up to 30% compared to a method based on minimum spanning tree algorithm for larger data set.' as abstract.

there is a document named 'doc-830' that
  has 'In this paper, we return to the drawing board to rethink the basic approach to multi-hop forwarding for highly dynamic wireless networks. The result is Listen First, Broadcast Later (LFBL), a surprisingly minimalist forwarding protocol. LFBL is topology-agnostic, that is, it has no knowledge of neighbors, routes, or next hops. LFBL receivers, not senders, make the forwarding decisions, and they only keep a small, fixed amount of state per active communication endpoint in order to do so. As a result, there is little state to go stale, and no predetermined paths to be broken. Frequent topology changes do not adversely impact performance. LFBL uses exclusively broadcast communication for all packets, making it a more natural fit for a wireless medium and allowing for more flexibility in the selection of MAC layer protocols. In addition to physical mobility of nodes, LFBL also supports logical mobility of application-level data. Under simulation, LFBL significantly outperforms AODV.' as abstract.

there is a document named 'doc-831' that
  has 'Inter-domain networking in MANETs is an important capability for various coalition operations in real life. Current inter-domain networking solutions typically rely on gateways for protocol translation, inter-domain route update, and policy enforcement. Previous work assumed that the gateway functionalities are statically assigned to a subset of nodes. While this approach will work well in a static scenario, it may not be effective in MANET due to node mobility. In this paper, we develop distributed mechanisms to elect minimal number of gateways while ensuring all eligible network partitions are connected. To this aim, we formulate a novel graph optimization problem, called Minimal Gateway Assignment Problem, and formally prove its hardness. We then design efficient algorithms to solve this problem with varying degree of complexity and coordination. By analysis, we show that our centralized algorithm has a tight approximation bound. By simulation, we show that our centralized and distributed algorithms perform well in average case. We also report an interesting result that cooperation is the key factor to get good results: a simple algorithm with tight cooperation among network partitions results in much better performance than a smart algorithm with loose cooperation.' as abstract.

there is a document named 'doc-833' that
  has 'Self adapting protocols are attractive theoretically because they converge towards optimal operation, and practically because they eliminate classes of misconfiguration error. In order to achieve self adaption, a protocol needs to gather state information upon which to base decisions. This paper begins to address these issues by investigating the trade-off surrounding the rate at which information needs to be gathered. If information is gathered too frequently, the overhead outweighs the benefit; but gathering too infrequently means that the state information is inaccurate and sub-optimal. This trade-off is investigated analytically on some simplified abstract problems. The results reveal how there are large and distinct regions where the routing strategy is degenerate (i.e. maximal or minimal state gathering). Moreover there are sharp transitions between these regions, providing useful insight to the next stage of the research. This will look at more realistic network models, a wider class of routing algorithms, and reduced assumptions about wireless characteristics.' as abstract.

there is a document named 'doc-836' that
  has 'When evaluating systems or protocol performance in a military MANET context, it is desirable to perform a sensitivity analysis across a broad range of realistic mobility patterns. However, there is currently no way to easily select a set of mobility models that continuously vary across a particular property (e.g. inter contact time); or identify different "classes" of mobility for which the system is suited. This is strongly related to the difficulty of taking actual military trace data, and generalising it (for sensitivity or simulation) or abstracting it (for declassification). The previously proposed universal mobility modelling framework (UMMF) gives a common syntax for mobility model definitions, but its configuration is based upon abstract behaviours, rather than familiar and intuitive network properties. This paper therefore outlines a system that allows a researcher to specify a set of desired network properties as input, and obtain a matching mobility model as output. By extension this allows a specific military trace to have its properties calculated (in preprocessing), and then the generation of an abstract mobility model with the same (or related) properties. The proposed tool is an extension to the UMMF which uses artificial intelligence techniques to search the UMMF parameter space. Not only does it provide direct search benefits, but it also continually adds to a database of network property to mobility parameter mappings. Given the vast dimensionality of the UMMF parameters, and the debate surrounding the most crucial network properties, the database can be used to enable a principal component analysis to begin to make the problem more tractable. The database also provides a valuable information repository for researchers interested in more fundamental dynamic graph theory and network classification.' as abstract.

there is a document named 'doc-848' that
  has 'Decision makers (humans or a software agents alike) are faced with the challenge of examining large volumes of information originating from heterogeneous sources with the goal of ascertaining trust in various pieces of information. While previous work have traditionally focused on simple models for review and rating systems, we introduce a new trust model for rich, complex and uncertain information encoded using Bayesian Description Logics. We present the challenges raised by the new model, and the results of an evaluation of the first prototype implementation under a variety of scenarios.' as abstract.

there is a document named 'doc-851' that
  has 'Partially Observable Markov Decision Process (POMDP) is a popular framework for planning under uncertainty in partially observable domains. Yet, the POMDP model is risk-neutral in that it assumes that the agent is maximizing the expected reward of its actions. In contrast, in domains like financial planning, it is often required that the agent decisions are risk-sensitive (maximize the utility of agent actions, for nonlinear utility functions). Unfortunately, existing POMDP solvers cannot solve such planning problems exactly. By considering piecewise linear approximations of utility functions, this paper addresses this shortcoming in three contributions: (i) It defines the Risk-Sensitive POMDP model; (ii) It derives the fundamental properties of the underlying value functions and provides a functional value iteration technique to compute them exactly and (c) It proposes an efficient procedure to determine the dominated value functions, to speed up the algorithm. Our experiments show that the proposed approach is feasible and applicable to realistic financial planning domains.' as abstract.

there is a document named 'doc-858' that
  has 'Recent work has applied game-theoretic models to real-world security problems at the Los Angeles International Airport (LAX) and Federal Air Marshals Service (FAMS). The analysis of these domains is based on input from domain experts intended to capture the best available intelligence information about potential terrorist activities and possible security countermeasures. Nevertheless, these models are subject to significant uncertaintyespecially in security domains where intelligence about adversary capabilities and preferences is very difficult to gather. This uncertainty presents significant challenges for applying game-theoretic analysis in these domains. Our experimental results show that standard solution methods based on perfect information assumptions are very sensitive to payoff uncertainty, resulting in low payoffs for the defender. We describe a model of Bayesian Stackelberg games that allows for general distributional uncertainty over the attackers payoffs. We conduct an experimental analysis of two algorithms for approximating equilibria of these games, and show that the resulting solutions give much better results than the standard approach when there is payoff uncertainty.' as abstract.

there is a document named 'doc-864' that
  has 'Markov Decision Processes (MDPs) or Partially Observable MDPs (POMDPs) assume that the agent\'s observations are available immediately. However, in domains such as controlling mars rovers from earth, observations arrive with a delay while the agents have to act continuously. Towards finding quality guaranteed policies for agents acting in such domains this paper makes three key contributions: (i) It introduces Delayed Observation POMDPs (D-POMDPs), a novel framework for rich modeling of delayed observations, (ii) It provides a suite of approximate algorithms for solving D-POMDPs with a desired accuracy and (iii) It proposes a policy execution technique that continuously adjusts the agent policy to improve the agent performance. We demonstrate the impact of our contributions in standard POMDP benchmark problems where explicit modeling of delayed observations leads to solutions of superior quality.' as abstract.

there is a document named 'doc-876' that
  has 'Timely dissemination of information to mobile users is vital in many applications. In a critical situation, no network infrastructure may be available for use in dissemination, over and above the on-board storage capability of the mobile users themselves. We consider the following specialized content distribution application: a group of users equipped with wireless devices build an ad hoc network in order to cooperatively retrieve information from certain regions (the mission sites). Each user requires access to some set of information items originating from sources lying within a region. Each user desires low-latency access to its desired data items, upon request (i.e., when pulled). In order to minimize average response time, we allow users to pull data either directly from sources or, when possible, from other nearby users who have already pulled, and continue to carry, the desired data items. That is, we allow for data to be pushed to one user and then pulled by one or more additional users. The total latency experienced by a user vis-a-vis a certain data item is then in general a combination of the push delay and the pull delay. We assume each delay time is a function of the hop distance between the pair of points in question. Our goal in this paper is to assign data to mobile users, in order to minimize the total cost and the average latency experienced by all the users. In a static setting, we solve this problem using two models, one of which is easy to solve but wasteful, the other of which is less easy to solve but also is less wasteful. Then in a dynamic setting, we adapt our static setting algorithm and also develop a new algorithm responding to users\' gradual arrival. Finally, we show that a trade-off can be made between minimizing cost and latency.' as abstract.

there is a document named 'doc-878' that
  has 'In various domains, including public safety, firstresponder, and security applications, an important task is monitoring a public space for events of interest, using sensors of various types, including e.g. networks of cameras installed on the ground or unmanned aerial vehicles (UAVs), which may be either autonomous or not. In the settings we consider here, cameras are characterized by a family of parameters, some fixed, some settable, including location, viewing range, and so on. The task is to deploy the sensors, i.e., set the applicable parameters, in order to optimize an objective function capturing the quality with which we observe a set of targets. In a dynamic scenario, we may wish to observe a large set of targets (or a continuous region) with observation quality passing some low threshold, sufficient for surveillance; upon request (i.e., when events are detected), we may then be obliged to observe a small number of distinguished targets to a higher level of quality, sufficient for identification and localization. An example objective function might maximize the total observation quality of all targets, conditioned on the hard constraint that each target is observed to the appropriate minimum quality threshold.' as abstract.

there is a document named 'doc-881' that
  has 'In the sensor scheduling problem we have m sensors (cameras, radars, PIRs, etc.) that need to observe n > m distinct locations. The objective is to schedule the sensors to observe the sites so as to reduce potential information loss due to limited or delayed observations. As an application, a camera surveillance may be used to monitor intrusions along a border, in which case there may be many distinct unprotected places along the border to observe. If it is too costly to dedicate a single camera to each site, then one or more cameras may be scheduled to observe and alternate between observation of different locations.' as abstract.

there is a document named 'doc-882' that
  has 'A variety of environmental conditions may affect sensor coverage, such as terrain, weather, and vegetation. In many applications of sensor networks, these conditions and, consequently, the sensing areas vary throughout the field of deployment. For example, an initial deployment of sensors to detect chemical plumes in an urban environment revealed irregularities in sensing ranges due to variations in terrain and meteorological conditions [1]. In order to maximize the coverage of a diverse environment, sensors must be distributed according to the coverage they provide at each location. Most deployment strategies for diverse environments are computationally complex and offer few guarantees on the resulting quality of coverage [1]-[3], [3]-[5]. These techniques can be simplified by realistically assuming that the field can be divided into zones of similar conditions such that the sensing area of a sensor is fairly uniform within each zone. This way, the sensors can first be allocated to each zone and then deployed within each zone according to well studied strategies for homogeneous fields.' as abstract.

there is a document named 'doc-883' that
  has 'We consider variations of a problem in which data must be delivered to mobile clients en-route, as they travel toward their destinations. The data can only be delivered to the mobile clients as they pass within range of wireless base stations. Example scenarios include the delivery of building maps to firefighters responding to multiple alarms. We cast this scenario as a parallel-machine scheduling problem with the littlestudied property that jobs may have different release times and deadlines when assigned to different machines. We present new algorithms and also adapt existing algorithms, for both online and offline settings. We evaluate these algorithms on a variety of problem instance types, using both synthetic and real-world data, including several geographical scenarios, and show that our algorithms produce schedules achieving near-optimal throughput.' as abstract.

there is a document named 'doc-886' that
  has 'In wireless sensor networks (WSNs), sensor nodes collect data of interesting events and send them back to stationary data access points (DAP), awaiting the end users to collect the information on demand. However, how to efficiently disseminate the information from DAPs further to end users in a WSN, which are also mobile sensors, has seldom been studied in the literature. Especially, if the end user is moving and the wanted data needs to be wirelessly downloaded from a DAP only when the user passed by, the collection of data is subject to constrained contact windows in time. Furthermore, when there are more than one end users in the network to collect data from the given set of DAPs, they compete for the limited DAPs and constrained contact windows. In this case, how to assign the multiple DAPs to the mobile end users in time forms a job-machine scheduling problem.' as abstract.

there is a document named 'doc-899a' that
  has 'Trust and reputation are vital concepts in multiagent systems where diverse, self-interested agents interact in pursuit of their own goals. While much work to date has focussed on enabling agents to form beliefs about the trustworthiness of potential delegation partners, the problem of deciding to trust on the basis of such beliefs has received relatively little attention. Without a model of trust decision, agents cannot take into account the risks involved in delegation. For example, different candidates may incur different costs, and tasks may offer varying payoffs in case of success or failure. In this paper, we present a decision theoretic model of trust decision which employs probabilistic trust beliefs within a Principal-Agent framework. We show how both agents in a two-party delegation relationship can make use of trust and reputation, as well as implicit reputational incentives, to make decisions about who (and indeed whether) to trust.' as abstract.

there is a document named 'doc-899b' that
  has 'Trust and reputation are vital concepts in multiagent systems where diverse, self-interested agents interact in pursuit of their own goals. While much work to date has focussed on enabling agents to form beliefs about the trustworthiness of potential delegation partners, the problem of deciding to trust on the basis of such beliefs has received relatively little attention. Without a model of trust decision, agents cannot take into account the risks involved in delegation. For example, different candidates may incur different costs, and tasks may offer varying payoffs in case of success or failure. In this paper, we present a decision theoretic model of trust decision which employs probabilistic trust beliefs within a Principal-Agent framework. We show how both agents in a two-party delegation relationship can make use of trust and reputation, as well as implicit reputational incentives, to make decisions about who (and indeed whether) to trust.' as abstract.

there is a document named 'doc-927a' that
  has 'This paper introduces and explores the new concept of Time-Specific Encryption (TSE). In (Plain) TSE, a Time Server broadcasts a key at the beginning of each time unit, a Time Instant Key (TIK). The sender of a message can specify any time interval during the encryption process; the receiver can decrypt to recover the message only if it has a TIK that corresponds to a time in that interval. We extend Plain TSE to the public-key and identity-based settings, where receivers are additionally equipped with private keys and either public keys or identities, and where decryption now requires the use of the private key as well as an appropriate TIK. We introduce security models for the plain, public-key and identitybased settings. We also provide constructions for schemes in the different settings, showing how to obtain Plain TSE using identity-based techniques, how to combine Plain TSE with public-key and identity-based encryption schemes, and how to build schemes that are chosen-ciphertext secure from schemes that are chosen-plaintext secure. Finally, we suggest applications for our new primitive, and discuss its relationships with existing primitives, such as Timed-Release Encryption and Broadcast Encryption.' as abstract.

there is a document named 'doc-932a' that
  has 'In a masquerade attack, an adversary who has stolen a legitimate user\'s credentials attempts to impersonate him to carry out malicious actions. Automatic detection of such attacks is often undertaken constructing models of normal behaviour of each user and then measuring significant departures from them. One potential vulnerability of this approach is that anomaly detection algorithms are generally susceptible of being deceived. In this paper, we first investigate how a resourceful masquerader can successfully evade detection while still accomplishing his goals. We then propose an algorithm based on the Kullback-Leibler divergence which attempts to identify if a sufficiently anomalous attack is present within an apparently normal request. Our experimental results indicate that the proposed scheme achieves considerably better detection quality than adversarial-unaware approaches.' as abstract.

there is a document named 'doc-954b' that
  has 'The Diffie-Hellman protocol (DHP) is one of the most studied protocols in cryptography. Much work has been dedicated to armor the original protocol against active attacks while incurring a minimal performance overhead relative to the basic (unauthenticated) DHP. This line of work has resulted in some remarkable protocols, e.g., MQV, where the protocol\'s communication cost is identical to that of the basic DHP and the computation overhead is small. Unfortunately, MQV and similar 2-message "implicitly authenticated" protocols do not achieve full security against active attacks since they cannot provide forward secrecy (PFS), a major security goal of DHP, against active attackers. In this paper we investigate the question of whether one can push the limits of authenticated DHPs even further, namely, to achieve communication complexity as in the original DHP (two messages with a single group element per message), maintain low computational overhead, and yet achieve full PFS against active attackers in a provable way. We answer this question in the affirmative by resorting to an old and elegant key agreement protocol: the Okamoto-Tanaka protocol [32]. We present a variant of the protocol (denoted mOT) which achieves the above minimal communication, incurs a computational overhead relative to the basic DHP that is practically negligible, and yet achieves full provable key agreement security, including PFS, against active attackers. Moreover, due to the identity-based properties of mOT, even the sending of certificates (typical for authenticated DHPs) can be avoided in the protocol. As additional contributions, we apply our analysis to prove the security of a recent multi-domain extension of the Okamoto-Tanaka protocol by Schridde et al. and show how to adapt mOT to the (non id-based) certificate-based setting.' as abstract.

there is a document named 'doc-955a' that
  has 'We explore the idea of applying machine learning techniques to automatically infer risk-adaptive policies whose purpose is to reconfigure a network\'s security architecture when the context in which it operates changes. To illustrate this approach, we consider the case of a MANET where certain nodes carrying sensitive services (e.g., web servers, key repositories, firewalls, etc.) should consider relocating themselves into a different node to guarantee a proper functioning. We use simulation to derive properties from a candidate policy, and then apply Genetic Programming and Multi-Objective Optimisation techniques to search for optimal candidates. The inferred policies take the form of risk-aware service relocation algorithms that autonomously dictate when and how to relocate services with the aim of keeping risk to a minimum. Since security policies often have implications in dimensions other than security, we force the learning process to consider also the consequences (performance, usability) of a given policy.' as abstract.

there is a document named 'doc-975b' that
  has 'Verifiable Computation enables a computationally weak client to "outsource" the computation of a function F on various inputs x1,...,xk to one or more workers. The workers return the result of the function evaluation, e.g., yi = F(xi), as well as a proof that the computation of F was carried out correctly on the given value xi . The verification of the proof should require substantially less computational effort than computing F(xi) from scratch. We present a protocol that allows the worker to return a computationally-sound, non-interactive proof that can be verified in O(m) time, where m is the bit-length of the output of F. The protocol requires a one-time pre-processing stage by the client which takes O(' as abstract and
  has 'C' as abstract and
  has ') time, where C is the smallest Boolean circuit computing F. Our scheme also provides input and output privacy for the client, meaning that the workers do not learn any information about the xi or yi values.' as abstract.

there is a document named 'doc-986b' that
  has 'Network coding has received significant attention in the networking community for its potential to increase throughput and improve robustness without any centralized control. Unfortunately, network coding is highly susceptible to "pollution attacks" in which malicious nodes modify packets in a way that prevents the reconstruction of information at recipients; such attacks cannot be prevented using standard end-to-end cryptographic authentication because network coding requires that intermediate nodes modify data packets in transit. Specialized solutions to the problem have been developed in recent years based on homomorphic hashing and homomorphic signatures. The latter are more bandwidth-efficient but require more computation; in particular, the only known construction uses bilinear maps. We contribute to this area in several ways. We present the first homomorphic signature scheme based solely on the RSA assumption (in the random oracle model), and present a homomorphic hashing scheme based on composite moduli that is computationally more efficient than existing schemes (and which leads to secure network coding signatures based solely on the hardness of factoring in the standard model). Both schemes use shorter public keys than previous schemes. In addition, we show variants of existing schemes that reduce the communication overhead significantly for moderate-size networks, and which improve computational efficiency in some cases quite dramatically (e.g., we achieve a 20-fold speedup in the computation of intermediate nodes). At the core of our techniques is a modified approach to network coding where instead of working in a vector space over a field, we work directly over the integers (with small coefficients).' as abstract.

there is a document named 'doc-998' that
  has 'We study how physical layer cooperation impacts the requirements for asymptotic connectivity of large wireless networks. Under a model where receivers in a connected cluster cooperate, as well as the assumed cooperation between transmitters, we present a complete characterization of the conditions under which the network is fully connected in terms of the path loss exponent and node density. We also present results on whether percolation occurs under various conditions. Finally, we have considered the case where receivers do not collaborate, where recent results of other researchers for such based on fluid approximations are erroneous. All of these results illustrate the significant positive impact that cooperation can have on the connectivity of large wireless networks.' as abstract.

there is a document named 'doc-1110a' that
  has 'Despite several research studies, the effective analysis of policy based systems remains a significant challenge. Policy analysis should at least (i) be expressive (ii) take account of obligations and authorizations, (iii) include a dynamic system model, and (iv) give useful diagnostic information. We present a logic-based policy analysis framework which satisfies these requirements, showing how many significant policy-related properties can be analysed, and we give details of a prototype implementation.' as abstract.

there is a document named 'doc-1111' that
  has 'Network coding offers increased throughput and improved robustness to random faults in completely decentralized networks. Since it does not require centralized control, network coding has been suggested for routing packets in ad-hoc networks, for content distribution in P2P file systems, and for improving the efficiency of large-scale data dissemination over the Internet. In contrast to traditional routing schemes, however, network coding requires intermediate nodes to process and modify data packets en route. For this reason, standard signature schemes are inapplicable and it is therefore a challenge to provide resilience to tampering by malicious nodes in the network. Here, we propose a novel homomorphic signature scheme that can be used in conjunction with network coding to prevent malicious modification of data. The overhead of our scheme is small and independent of the file or packet size: both public keys and signatures in our scheme consist of only a single group element.' as abstract.

there is a document named 'doc-1224b' that
  has 'Connectivity and capacity are two measures for the performance of mobile ad hoc networks that have been studied extensively under standard point-to-point physical layer assumptions. However, extensive recent research at the physical layer has demonstrated the improvement in performance possible when multiple radios concurrently transmit in the same radio channel. In this paper, we consider how such physical layer cooperation improves the connectivity in wireless ad hoc networks. In particular, with noncoherent cooperation at the physical layer, we consider conditions on the node density _ (or, equivalently, the transmit power) for full connectivity and percolation for large networks in various dimensions and with various path loss exponents _. For one-dimensional (1-D) extended networks, in sharp contrast to noncooperative networks, we demonstrate that full connectivity can be realized under certain conditions. In particular, for any node density with _ < 1, or for node density _ > 2 when _ = 1, full connectivity occurs with probability one. Conversely, we demonstrate that, under noncoherent cooperation, there is no full connectivity with probability one when _ > 1. In two-dimensional (2-D) extended networks with noncoherent cooperation, for any node density with _ < 2, or for node density _ > 5 when _ = 2, full connectivity is achieved. Conversely, there is no full connectivity with probability one when _ > 2, but we prove that, for _ 6 4, the percolation threshold of the noncoherent cooperative network is strictly less than that of the noncooperative network. Analogous results are presented for dense networks. Hence, the main conclusion is that even relatively simple physical layer cooperation in the form of noncoherent power summing can substantially improve the connectivity of large ad hoc networks.' as abstract.

there is a document named 'doc-1279c' that
  has 'In this paper we study the capacity of wireless ad hoc networks with infrastructure support of an overlay of wired base stations. Such a network architecture is often referred to as hybrid wireless network or multihop cellular network. Previous studies on this topic are all focused on the twodimensional disk model proposed by Gupta and Kumar in their original work on the capacity of wireless ad hoc networks. We further consider a one-dimensional network model and a two-dimensional strip model to investigate the impact of network dimensionality and geometry on the capacity of such networks. Our results show that different network dimensions lead to significantly different capacity scaling laws. Specifically, for a one-dimensional network of n nodes and b base stations, even with a small number of base stations, the gain in capacity is substantial, increasing linearly with the number of base stations as long as b log b _ n. However, a two-dimensional square (or disk) network requires a large number of base stations b = _(_ n) before we see such a capacity increase. For a 2-dimensional strip network, if the width of the strip is at least on the order of the logarithmic of its length, the capacity follows the same scaling law as in the 2-dimensional square case. Otherwise the capacity exhibits the same scaling behavior as in the 1-dimensional network. We find that the different capacity scaling behaviors are attributed to the percolation properties of the respective network models.' as abstract.

there is a document named 'doc-1334a' that
  has 'Numerous sensor-based applications depend upon the detection of certain events. The proper operation of these applications depend on the quality of the information (QoI) that they receive from their sensor-based event detectors. In this paper, we establish relationships that tie the QoI attributes of timeliness and confidence to the operational characteristics of a sensor system and the events they detect. By building upon the Neyman-Pearson hypothesis testing procedure, we study the dependence of these characteristics and attributes on each other and establishing their theoretical performance boundaries.' as abstract.

there is a document named 'doc-1334d' that
  has 'Numerous sensor-based applications depend upon the detection of certain events. The proper operation of these applications depend on the quality of the information (QoI) that they receive from their sensor-based event detectors. In this paper, we establish relationships that tie the QoI attributes of timeliness and confidence to the operational characteristics of a sensor system and the events they detect. By building upon the Neyman-Pearson hypothesis testing procedure, we study the dependence of these characteristics and attributes on each other and establishing their theoretical performance boundaries.' as abstract.

there is a document named 'doc-1371' that
  has 'The advent of miniaturised sensors that can be carried on the body or embedded in the environment, together with ubiquitous \'smart-phones\' with various sensors means ubiquitous computing systems already pervade our lives. However, for them to \'disappear\' in the background, they need to be adaptive, autonomous and self-managing. We present an architectural model based on policy-based Self Managed Cells for engineering ubiquitous computing systems and discuss issues of security and fault management. We indicate the need for learning adaptive behaviour from users and the importance of formal methods within the engineering design process.' as abstract.

there is a document named 'doc-1406' that
  has 'The OWL file named "simple_artypro_0_3A.owl" was generated as a simple example of a brigade plan, to be used in the ACITA demonstrator. Here we use this file as an example of how the rationale is constructed,  The definition of the rationale structure is contained in:  _   The Representation of Logic in the CPM, https://www.usukitacs.com/?q=node/4995 _   The representation of logic in semantic web languages, https://www.usukitacs.com/?q=node/4986  However minor changes have been made to the structure subsequent to these documents.' as abstract.

there is a document named 'doc-1407' that
  has 'To describe an example of an English statement that appears simple but whose logical meaning can be difficult to grasp, and some linguistic factors that might affect its comprehension. This is part of the P12 Task 2 work on human graspability of logic, exploring how difficulties may occur in a military context in understanding the formal logical meaning of sentences and how these might be reduced.' as abstract.

there is a document named 'doc-1410' that
  has 'In this paper, we review some security concerns of network utility maximization (NUM) based networks. Nodes in NUM based networks rely on the exchange of status messages to converge on optimal parameters for the network. We address concerns unique to NUM based networks, specifically focusing on denial of service (DoS) attacks. Through use of simulation, we demonstrate how these networks can be manipulated. For each vulnerability, we present simulation data where appropriate. We then briefly discuss possible countermeasures and solutions that address these vulnerabilities. We show that where unauthenticated status messages are allowed, a malicious user can manipulate the network into a variety of undesired states.' as abstract.

there is a document named 'doc-1411a' that
  has 'Controlled English (CE) is a type of Controlled Natural Language (CNL), designed to be readable by a native English speaker whilst representing information in a structured and unambiguous form. It is structured by following a simple but fully defined syntax, which may be parsed by a computer system. It is unambiguous by using only words that are defined by the analyst as part of a conceptual model. CE therefore serves as a language that is simultaneously understandable by human and computer system, thus facilitating the communication between them.' as abstract.

there is a document named 'doc-1412' that
  has 'CE defines statements in a Controlled Natural Language, but the current version of ITA CE does not define how these statements may be structured into sections, and documents. A full use of CE in human-readable documents will require such structuring.  In the following, the term "CE Document" (or CED) will refer to a structured document containing CE statements.' as abstract.

there is a document named 'doc-1417a' that
  has 'This paper reviews the coverage of formal Relational Algebra as it applies to distributed, federated databases in varying network topologies. The review shows that a number of Relational Algebra extensions allow distributed relations and federation of heterogeneous database schema. More concrete physical Relational Algebra extensions support access plans for multi-database query processing but lack cost functions dealing with specific network topologies such as scale-free networks, hyper-cubes and Kautz graphs. Statistic gathering techniques are highlighted which allow efficient distribution of database metrics, with the aim of providing optimized query processing.' as abstract.

there is a document named 'doc-1418' that
  has 'Commercial games environments are developed to provide enjoyable and interesting challenges for leisure activities. Some of the aspects of these environments make them ill suited for qualitative and quantitative research. We describe collaboratively developed additions to commercial software that were combined to address these issues and create a research environment. We also describe early results obtained using this environment to study mission context and coordination within military teams. A semantic wiki provided a structured representation of team context.' as abstract.

there is a document named 'doc-1420' that
  has 'To define the extensions and modifications that have been made to the CPM, leading to CPM v3.0  This work is a deliverable of the ITA programme, project 12 task 3 "Shared Understanding and Collaborative Planning", for Q2 of the BPP09.' as abstract.

there is a document named 'doc-1425' that
  has 'To define how logical statements from specialist logics may be explicitly represented in ontologies expressed in semantic web technologies. This document builds upon the representation of basic logical statements [BASLOG]  This work is a deliverable of the ITA programme, project 12 task 2 "Collective Intelligence and the Web", for Q2 of the BPP09.' as abstract.

there is a document named 'doc-1427' that
  has 'This paper reports on our investigation of the latest advances for the Social Web, Web 2.0, Linked Data Web and the emerging area of Web Science. We discuss these advances mainly in terms of the latest capabilities and functions that are available (or being made available) on the web at the time of writing this paper, or as outlined in related research, and we refer to this as the "state-of-the-web". We propose that the capabilities we discuss can be of significant benefit to teams comprised of multinational team members who are geographically dispersed, especially those comprised of coalition members in a military context such as disaster relief or humanitarian aid, working closely with non-government organisations and non-military teams to achieve their aims as quickly and efficiently as possible. This diversity of background and motivation amongst the team members, coupled with a lack of dedicated private network infrastructure provides an interesting case for the potential use of existing open web-based capabilities to help support such teams both in terms of their collaborative activities and their access to and sharing of information sources.' as abstract.

there is a document named 'doc-1428' that
  has 'To define logical mappings between the G-PAL and CPM ontologies.  This work is a deliverable of the ITA programme, project 12 task 3 "Shared Understanding and Collaborative Planning", for Q2 of the BPP09.' as abstract.

there is a document named 'doc-1431a' that
  has 'This paper introduces the Global Interlinked Data Store (GIDS), a technique to support the easy creation and retrieval of interlinked semantic data within a web-scale distributed network environment such as the World Wide Web (WWW). By using the GIDS a web application developer can treat the network as a data store without worrying about files, databases or other traditional data storage concerns. The data that is created on the network can be subsequently accessed and navigated by end users and software agents alike. The GIDS proposes a novel three-stage data storage process which enables the data to be stored in up to three contextually relevant locations to enhance subsequent retrieval opportunities. We propose that the GIDS can provide a highly-scalable distributed capability to store and retrieve data directly on a network. We believe that the capability offered by the GIDS will be of significant use to rapidly formed diverse coalitions who wish to communicate and exchange semantic data in a large network environment such as the WWW. Based on commonly used Web standards, we have implemented the GIDS in a prototype which can be invoked via simple web service requests to read and write data as prescribed by the GIDS.' as abstract.

there is a document named 'doc-1432' that
  has 'To define how logical statements may be explicitly represented in ontologies expressed in semantic web technologies. This document concentrates on basic logical statements; a subsequent piece of work will define the representation of statements in more complex logics.  This work is a deliverable of the ITA programme, project 12 task 2 "Collective Intelligence and the Web", for Q1 of the BPP09.' as abstract.

there is a document named 'doc-1433' that
  has 'To define how the CPM may be augmented by a greater level of logical detail, by using a more expressive logical language, defined in a companion paper [REPLOG].  This work is a deliverable of the ITA programme, project 12 task 3 "Shared Understanding and Collaborative Planning", for Q1 of the BPP09.' as abstract.

there is a document named 'doc-1434' that
  has 'This paper looks at the issues around creating and populating an ontology model from a textual description of an application domain.' as abstract.

there is a document named 'doc-1438' that
  has 'This brief report presents the architecture and functionality of LigNite, a hybrid search engine for the semantic web. LigNite provides a web platform for querying semantic web ontologies, using both a controlled as well as a free-text search functionality. Ontologies may pre-exist in the system, or may be provided by the user via LigNite\'s import mechanism. A user searching an ontology using the controlled search engine will be guided to construct ontology triples, which are matched against the ontology\'s contents to retrieve results meeting the user\'s criteria. A user searching an ontology using the free-text search engine will not be restricted to a triple pattern, but may choose freely the structure of his query (hence the name "free-text"). The free-text search engine greatly relies on producing results that are ranked according to their correlation to the user\'s query.' as abstract.

there is a document named 'doc-1441' that
  has 'This report documents the results of an evaluation of the Collaborative Planning Model (CPM) held in September 2008.  Military planning is essentially a human activity that is distributed across a hierarchal team. This evaluation is built upon the premise that military planning, especially across hierarchical boundaries, depends on the ability to communicate a common understanding of commander\'s intent, objectives, resources, and constraints. In addition, decisions made at any level of the planning can be better communicated if the justification for planning options chosen or alternatives rejected is communicated (Allen et al, 2008).' as abstract.

there is a document named 'doc-1443a' that
  has 'Kinematic state refers to a vehicle\'s position, velocity, and attitude. The kinematic state of a vehicle can be defined by specifying the relative position, velocity, and orientation of two reference frames. The two reference frames typically used are a vehicle fixed body frame and a navigation frame with known orientation. In this application, we consider the vehicle\'s position and velocity as the kinematic state and we select a North-East-Down (NED) frame as the navigation frame. A tracking system is used to estimate the kinematic state of the vehicle or, more specifically, to estimate the position vector and velocity vector of the body frame relative to the navigation frame.' as abstract.

there is a document named 'doc-1521' that
  has 'With their increasing capabilities, research efforts in sensor networks have been spreading on a variety of aspects. In this paper, we present a system-oriented, layered approach for evaluating their application-related performance. Focusing on sensor-enabled detection system, an application planners point of view of the system is considered and a hypothesis-testing-based analysis framework for evaluating the quality of information (QoI) supported by a sensor network deployment is explored. The QoI properties of centralized, distributed and hybrid decision topologies are investigated and trade-offs explored at the sensor, cluster, and system-level. In the process, the computationally powerful concept of a QoI equivalent sensor is presented and applied in the aforementioned explorations. Finite six networks are considered and limiting behavior and dominance properties are also investigated. Finally, extensions of the analysis framework to faulty sensor and the impact of calibration are also investigated.' as abstract.

there is a document named 'doc-1526' that
  has 'Besides safe navigation (e.g., warning of approaching vehicles), car to car communications will enable a host of new applications, ranging from office-on-the-wheel support to entertainment, advertisement and urban sensing. Content distribution among drivers is one of the most promising applications and is well positioned to deliver new opportunities to all facets of the industry: e.g., navigation map data distribution and proximity advertisement (advertisement video clips). Content distribution in vehicular networks is a challenge due to network dynamics and high mobility, however network coding was shown to efficiently handle such dynamics and to considerably enhance performance. This paper provides an in depth analysis of implementation issues of network coding in vehicular networks. To this end, we consider general resource constraints (e.g., CPU, disk, memory) besides bandwidth, that are likely to impact the encoding and storage management operations required by network coding. We develop an abstract model of the network coding procedures and implement it in the wireless network simulator to evaluate the impact of limited resources. We then propose schemes that considerably improve the use of such resources. Our model and extensive simulation results show that network coding parameters must be carefully configured by taking resource constraints into account.' as abstract.

there is a document named 'doc-1527' that
  has 'This paper addresses the problem of specifying and establishing secure collaborations between autonomous entities that need to interact and depend on each other in order to accomplish their goals. We call such collaborations missionoriented dynamic communities. We propose an abstract model for policy-based collaboration that relies on a set of task-oriented roles. Nodes are discovered dynamically and assigned to one or more roles, and then start enforcing the policies associated with these roles according to the description of the community. In this paper we describe a basic set of management roles that are needed to provide management and security functions for dynamic communities. Policies are used to specify management protocols as these can be easily modified to reflect different adaptive strategies. We focus on collaboration between nodes in the context of mobile ad-hoc networks. Our implementation is based on a surveillance scenario using unmanned autonomous vehicles (UAVs). However our approach to dynamic communities could be applied to emergency services at a disaster site, search and rescue applications or many military scenarios.' as abstract.

there is a document named 'doc-1530' that
  has 'In this paper we present a framework for a sensor system which enables timely information dissemination to support operational coalition forces. Information is an important asset which enables informed decision making allowing the strategic distribution of resources thus minimizing risk and threats. Information should be delivered taking in account time and network constraints, and information overload and relevance, and forecasting issues. Here we outline the system network requirements and information fusion needs to delivery accurate and meaningful information during defence and security missions.' as abstract.

there is a document named 'doc-1534' that
  has 'We present an approach for evaluating risk using risk-contributing factors. This approach could be applied recursively to a hierarchy of risk-contributing factors. We also use a MANET scenario to demonstrate how the approach may be applied. We believe that the scenario covers some of the most important risk factors regarding access control for information in a MANET. The set of risk factors discussed in this paper is by no means complete. Besides the usual technical considerations for information system security, other factors such as human psychology, social network and warfare should also be taken into consideration to evaluate risk in a MANET and much more research is needed.' as abstract.

there is a document named 'doc-1535' that
  has 'An initial investigation is reported into a unified approach to safety and security in the development and operation of MANETs. It draws on the general literature on safety and security, and links it to other work in ITA especially that on architectures (Project 6) and policy (Project 4).' as abstract.

there is a document named 'doc-1536' that
  has 'This report addresses the \'system-level\' management of trust and risk for MANETs. Trust and risk management (TRM) is discussed as both an aspect of security development and as a function implemented in the MANET system and its operations processes. The hypothesized TRM function enables adaptive management at the level of the integrated MANET system so that security requirements are met under changing operational and threat conditions. Various aspects of the TRM function are investigated: its purpose; relationships with the MANET security system; how it fits within a MANET security architecture; interfaces with security mechanisms and services. Also considered is how such a function can be systematically developed. Trust and risk management is most likely to be conducted \'quasi-independently\' in the various organizational and technical layers involved so that requirements for a system-level TRM function arise from considerations of coordination between layers and services. The support of security policies, the aggregation of risk across MANET layers and the relationship between trust and risk are discussed. An approach based on the management of commitments to risks and mitigations is proposed leading to a TRM function that provides system-level resilience to changes in risks and threats. Recommendations for further development are made.' as abstract.

there is a document named 'doc-1559a' that
  has 'In the literature of ad-hoc sensory networks, a wellstudied problem is that of achieving full sensory coverage within a region according to the boolean model of coverage. We introduce a new technique to achieve full coverage that anticipates bounded random deviations of resultant positions of some or all sensors from their designated positions. Various scenarios of placement for arbitrary arrangements of sensors are presented, and illustrated using two widely used grid arrangements. Quantitative results linking radius of coverage, grid granularity and tolerance for inexact placement are provided for triangular and square grid arrangements. Finally, we elaborate on various practical applications cutting across the aforementioned scenarios and arrangements.' as abstract.

there is a document named 'doc-1585' that
  has 'This paper studies and analyzes instant messaging (IM) behavior . Users are exploring various mediums of communications. Interaction between two users is not just restricted through phone or e-mail. It is being observed that more and more users are moving to other non-obtrusive forms of communication like IM and text messaging. This paper analyzes IM-based communication of a user with other users to study how often he chats with them, amount of time he spends on instant messaging with a contact, frequency of messaging on a per contact basis, average duration of IM conversation and distribution of total chat to chat per conversation. We analyzed the chat logs generated by IBM Sametime 7.5 IM application. The logs were collected from IBM Sametime IM users who volunteered to share their chat data by running a chat-analyzer program on their chat transcripts. Analysis of user IM usage pattern can be used for server side capacity planning based on IM usage trends, automatically adjusting the presence subscription rate for each of the contacts based on amount of inter-user IM activity, providing personalized services to different users based on usage pattern profiling.' as abstract.

there is a document named 'doc-1587' that
  has 'Firewalls are a effective means of protecting a local system or network of systems from network-based security threats. In this paper, we propose a policy algebra framework for security policy enforcement in hybrid firewalls, ones that exist both in the network and on end systems. To preserve the security semantics, the policy algebras provide a formalism to compute addition, conjunction, subtraction, and summation on rule sets; it also defines the cost and risk functions associated with policy enforcement. Policy outsourcing triggers global cost minimization. We show that our framework can easily be extended to support packet filter firewall policies. Finally, we discuss special challenges and requirements for applying the policy algebra framework to MANETs.' as abstract.

there is a document named 'doc-1590' that
  has 'Sensors play a central role in providing the intelligence, surveillance, and reconnaissance (ISR) capabilities required by security decision-makers and military commanders to plan and execute missions. In network-centric operations, ISR devices (sensors and platforms equipped with sensors) are expected to be highly valuable but scarce resources that have to be allocated to a number of possibly competing missions. Hence, algorithms to efficiently assign sensors to missions are necessary.' as abstract.

there is a document named 'doc-1668a' that
  has 'This paper explores the idea of knowledge-based security policies, which are used to decide whether to answer queries over secret data based on an estimation of the querier\'s (possibly increased) knowledge given the results. Limiting knowledge is the goal of existing information release policies that employ mechanisms such as noising, anonymization, and redaction. Knowledge-based policies are more general: they increase flexibility by not fixing the means to restrict information flow. We enforce a knowledge-based policy by explicitly tracking a model of a querier\'s belief about secret data, represented as a probability distribution, and denying any query that could increase knowledge above a given threshold. We implement query analysis and belief tracking via abstract interpretation using a novel probabilistic polyhedral domain, whose design permits trading off precision with performance while ensuring estimates of a querier\'s knowledge are sound. Experiments with our implementation show that several useful queries can be handled efficiently, and performance scales far better than would more standard implementations of probabilistic computation based on sampling.' as abstract.

there is a document named 'doc-1676' that
  has 'This document reports on the construction of a simple "Information Extraction" scenario that was worked through at the Project 4 Task 2 (P4T2) workshop at Boeing in August 2011. The purpose of the exercise was to take a sample sentence from the SYNCOIN dataset and show how that could be expressed in Controlled English (CE) and then work through a basic scenario to demonstrate how a very simple anaphor resolver agent could query the CeStore for sentences in order to try to resolve certain anaphoric references. The purpose of the workshop exercise was to take all members through an end-to-end worked example using only CE as the input and output language, in order to achieve a greater in-depth understanding of the approach and the possibilities.' as abstract.

there is a document named 'doc-1774' that
  has 'In order to press maximal cognitive benefit from their social, technological and informational environments, military coalitions need to understand how best to exploit available information assets as well as how best to organize their socially-distributed information processing activities. The International Technology Alliance (ITA) program is beginning to address some of the challenges associated with improved cognition in military coalitions by integrating a variety of research and development efforts. In particular, research in one component of the ITA (\'Project 4: Shared Understanding and Information Exploitation\') is seeking to develop capabilities that enable military coalitions to better exploit and distribute networked information assets in the service of collective cognitive outcomes (e.g. improved decision-making) (see Smart et al., 2011).' as abstract.

there is a document named 'doc-1864' that
  has 'Sharing information is a critical requirement of effective decision making during coalition operations. Such information is intended to improve the knowledge and situation awareness of military commanders to ac- complish the tactical tasks at hand. The network information systems available to modern militaries, as well as the vast array of sensors now employed in intelligence gathering allow an unprecedented amount of information to be collected and disseminated to the all decision makers. This vast amount of information creates many opportunities, but also puts the decision maker in danger of being provided too much infor- mation, so raw data is seldom directly presented to decision makers, but rather, processed, summarized and aggregated to allow a decision maker to create a mental big-picture. It widely is understood that any kind of information source (be it human or signals-based) suffers from a degree of unreliability or uncertainty, as no sensor is perfect and human sources may suffer from various biases. As information from multiple sources is gathered and processed for the benefit of the decision maker, they accumulate and possibly introduce uncertainty. Thus, in order to improve the quality of the decisions made based on such information, it is critical to understand, process and characterize the uncertainties while presenting the resulting information.' as abstract.

there is a document named 'doc-1870' that
  has 'In the business world, processes matter. Processes can be used to validate experimental results, reproduce scientific experiments, and check compliance against regulations or to audit applications. Computers are good at producing results quickly but they are bad at explaining and documenting past actions. Academic work into information provenance has addressed this and is proposing a standardised way of addressing the problem.' as abstract.

there is a document named 'doc-1896' that
  has 'As the semantic web expands, ontological data becomes distributed over a large network of data sources on the Web. Consequently, evaluating queries that aim to tap into this distributed semantic database necessitates the ability to consult multiple data sources efficiently. In this paper, we propose methods and heuristics to effi- ciently query distributed ontological data based on a series of properties of summarized data. In our approach, each source summarizes its data as another RDF graph, and relevant section of these summaries are merged and analyzed at query evaluation time. We show how the analysis of these summaries enables more efficient source selection, query pruning and transformation of expensive distributed joins into local joins.' as abstract.

there is a document named 'doc-1907' that
  has 'The current \'expresses\' relation assumes a 1-1 association between word and concept. However this is inadequate when a word has multiple senses, such as "tank" which may mean a military vehicle or a physical container. As an aside it may be noted that the military vehicle sense borrows some of its meaning from the container sense and may, for example, be "filled" "punctured" "sealed" etc. Thus it may be that a military tank is really a sub concept of physical container.' as abstract.

there is a document named 'doc-1909' that
  has 'An "Analyst\'s Helper" is being developed in Prolog to assist the analyst in constructing linguistic mappings between words and concepts in the ACM. Other resources are being used to provide linguistic knowledge, currently this is Wordnet [WN1,WN2,WN3]. As a result of using the analysts helper, further CE sentences are constructed to represent the linguistic mappings, which is then used to guide other NL and CNL processing.  At present this facility is limited in ways that are noted in a later section, and it is planned to extend the functionality in the future.' as abstract.

there is a document named 'doc-1931' that
  has 'The ultimate goal of our project is to allow various members of the coalition to access necessary information that has been created by other people and teams, often from other divisions, other branches of the military, even other countries. One aspect of this is creating a common query language that can bridge different conceptualizations of the domain, whether due to different cultures, different dialects, or different areas of expertise. However, a query language is not enough; the information must be entered into the system to be accessible. Much of this information already exists in the form of free text memos, reports, and other documents and it must be re-coded in order to be entered into a system in a form that is broadly accessible. This is where information extraction comes in. Various aspects of the information in those free text documents must be extracted, including various types of entities and their attributes and relations to each other, various types of events along with their participants, location, time, and how certain we can be of the report. However, although a certain amount of information extraction can be done with a fairly general system, much of it requires knowledge that only those familiar with the domain can provide, including knowledge of the types of people, objects, locations, events, and situations that are relevant and the language used to describe them. So a fundamental goal of this project is to provide a system and a language for these domain experts to express this knowledge in. In pursuit of this goal, we are developing a system for performing ontology-based information extraction using a controlled natural language. Having an ontology-based system will simplify the extraction by allowing us (or rather the domain experts) to describe constraints on the kinds of information that is expected to be found (or at least to be useful), thus aiding the process of interpreting the text. The controlled natural language, a subset of ordinary language (English in our case), is ITA-CE (hereafter just CE). Because it is ordinary English it will be relatively easy for these domain experts to use yet sufficiently precise and unambiguous that a computer can interpret the input of the domain experts and use it to automatically extract the necessary knowledge from free text.' as abstract.

there is a document named 'doc-1933' that
  has 'The task of ontology clustering is to group similar or related concepts into clusters. Sometimes it also involves annotating and presenting the new identified clusters in a concise way. The input to an ontology clustering system can be natural language texts, structured data, ontology knowledge bases, or their combinations. The output of an ontology clustering system is either a new ontology or an updated ontology with new concepts, new taxonomies, and new relations. This report surveys the ontology clustering methods that are potentially useful for the task of presenting answers to queries that are posted to ontology knowledge bases within the ITA project. The answer presentation can be viewed as a process which takes an ontology knowledge base and queries as input, and present the answers in the right level of concepts so that human users can easily understand the answers with as few cognitive efforts as possible. This might involve choosing the right ontology, creating and evolving existent ontology to present the answers. Ontology clustering methods can be used to identify the appropriate generalization of the concepts so as to reduce the cognitive load of human users to understand the answers. Other surveys on ontology learning can be found in [Perez and Mancho, 2003; Zhou, 2007; Zouaq and Nkambou, 2010; Cimiano et al., 2009].' as abstract.

there is a document named 'doc-1990' that
  has 'The concept of lambda variables and associated lambda expressions is important for allowing the composition of smaller semantic units into larger ones. It is desirable that we be able to represent lambda expressions in CE as part of the goal of a more accessible linguistic model. The current semantic frames do use the concept of lambda variable and compositionality, but the current syntax ("is predicated on") is ugly, and there is a need for a better representation of such expressions in CE. However this CE representation should be capable of handling all forms of lambda expression (including embedded lambda expressions and the notion of lambda conversion). It would also be desirable if it also handled quantifiers such as every and forall, allowing the composition of NL phrases that contain these.' as abstract.

there is a document named 'doc-2005' that
  has 'Facilitating effective coalition decision-making at brigade-level and below requires support for the rapid formation of ad hoc teams. These teams are typically assembled from multiple coalition partners, operating at the edges of the information network. The research programme described in this paper addresses a set of challenges associated with supporting decision-making by such teams, aiming to give them: the agility to adapt to dynamically-changing situations; provision of a resilient information infrastructure across hybrid networks (military and commercial); transparency in terms of quality of information and services; and a means of handling differing levels of mutual trust. The overall goal is to create an integrated platform to connect data - of which there are increasing volumes - to the context and intent of coalition decisions. The platform covers network service management, infrastructure to handle trust, policy, and information summarisation, and a human-machine environment for shared understanding and information exploitation.' as abstract.

there is a document named 'doc-2048' that
  has 'This report describes the current status of work in the experimental exploitation of obfuscation techniques in a distributed, shared, middleware infrastructure. The aim of the work is to develop a framework, based upon the ITA Information Fabric, for information sharing in coalition operations with data obfuscation and variable trust. The work carried out to-date meets the following needs: demonstration of the obfuscation of information flowing across and between message/service buses; visualization of the effect of information obfuscation with a representative data set; and the incorporation of information obfuscation techniques in the Information Fabric. This obfuscation capability provides a basis from which to evaluate practically the broader research aims of the ITA obfuscation research task - Project 6, Task 1.' as abstract.

there is a document named 'doc-2056' that
  has 'Minimize path cost taking into account link quality and security-related penalties.' as abstract.

there is a document named 'doc-2066' that
  has 'he Linguistic Knowledge Base system, from Prof Ann Copestake, contains much useful information, and provides a parsing system different to that of the Stanford parser. It is therefore of benefit to consider how the LKB information can be represented in CE, in order that we could both use the LKB parser and contribute to the development of the DELPH-In Project.  The LKB has as its basic representation, the Typed Feature Structure, or TFS, and the first step is to represent TFSs in CE.  This note is based upon the specifications in "Implementing Typed Feature Structure Grammars" by Ann Copestake., CSLI Publications, 2002. However the example grammars have been taken from the LKB distribution files.' as abstract.

there is a document named 'doc-2089' that
  has 'Advances in cellular technology have increased the demand of accessing the Internet dramatically. Cellular technology not only enables mobility, but also allows redundant connectivity using multiple wireless paths to improve availability, reliability, and performance. Multiple paths enable the potential to shift traffic from broken or congested paths to higher-quality ones as traffic characteristics dynamically change, particularly during movement. However, little work has been done to date studying cellular networks or their suitability. This paper characterizes the behavior of cellular networks, examining 3G, 4G, and Wi-Fi networks for both single-path and multi-path data transport. Our contribution is two-fold: First, we perform measurements of single path transport using TCP over major US cellular wireless networks (both 4G and 3G), and characterize them in terms of throughput, packet loss, and round-trip time. Second, we measure and evaluate transport using multi-path TCP in cellular environments and show that leveraging path diversity under changing environments is a promising solution for more reliable and efficient TCP transfer. We also identify potential issues in using multi-path TCP which can limit performance.' as abstract.

there is a document named 'doc-2120' that
  has 'Facts written in any language must refer to individuals in a way that uniquely identifies them, in order that the reader of the facts can determine which individuals are involved in the propositions and can relate these propositions to previous facts about the individuals. The simplest way is to use unique identifiers such as "person123" and this is the approach currently used by Controlled English (CE).This allows a simple approach to working out identity but it has several significant problems: _   it may not be easy to read especially if the identifier is complex (such as ce_store123456), such complexity often being required to ensure uniqueness _   it may be impossible to actually guarantee uniqueness across facts written by different systems unless some form of global naming service is used.' as abstract.

there is a document named 'doc-2121' that
  has 'As part of the "replay conversation" demonstration, all of the SYNCOIN reports that refer to communications being monitored have been tested against the NL processing, with the aim that further improvement can be made over time. This document records the current state of the processing in terms of the information extracted.' as abstract.

there is a document named 'doc-2123' that
  has 'This document records issues and decisions in respect of the linguistic processing (and preprocessing) for the ACITA demonstration 2012. The outline of the demonstration is given in "ACITA Fact Extraction Demonstration, https://www.usukitacs.com/node/2122".' as abstract.

there is a document named 'doc-2127' that
  has 'This document describes the work done in Q5 of the ITA programme for Project6 task 2 on the use of Controlled English (CE) to configure fact extraction from unstructured Natural Language text. Most of the work is reported in separate documents, and the purpose of this document is to summarise and aggregate these more detailed documents.' as abstract.

there is a document named 'doc-2235' that
  has 'This report is a deliverable in the ITA Programme BPP11 Project 4 task 2, q6. It addresses the use of assumptions in supporting alternative hypotheses as part of the fact extraction process. The use of assumptions has been previously explored in the ITA UK Pathfinder transition project, when it was applied to alternative hypotheses about the specific domain of  IED events, and some initial mechanisms were built into the Prolog-based CE rule interpreter.   An assumption may be defined as a "label" on a proposition indicating that the proposition is not necessarily known to be true but, for the purpose of argument, is considered to be true. Thus the CE sentence "the person John is located in the city London" represents a proposition (that John is in London) and we may make the assumption that this is true by stating the CE sentence "it is assumed that the person John is located in the city London".  The meaning of his (and other assumptions) is that one can conceive of a world in which that is actually true, and we can use any form of reasoning (including CE rules) to infer new information in that world. If we subsequently discover that this assumption is incorrect (eg by inferring an inconsistency from rules that use this assumption as a premise) then the world considered to be impossible, and any inferred information contained in this world is not longer considered to be true (unless there are other valid inferences not derived from incorrect assumptions that conclude this information).' as abstract.

there is a document named 'doc-2271' that
  has 'The P4T2 "fact extraction using CE" project has constructed a lexical model (v1) to support NL processing and the construction of demonstrations. However there are issues with the v1 version, especially with regard to the representation of words and lexical items, and this note defines a new version of the lexical model that is more comprehensive and more in keeping with scientific linguistic practice. This model is also aimed to be closer to the LKB models, with a view to integration in the future.' as abstract.

there is a document named 'doc-2273' that
  has 'We propose to extend current P4T2 research on using a Controlled Natural Language, specifically Controlled English (CE) [1,2,3], for fact extraction from unstructured text and reasoning from these facts to provide high-value information [4,5]. By focusing on the challenging problem of Natural Language (NL) processing and reasoning we address key issues to improve our CE capabilities and facilitate greater adoption of CE, as recommended in the peer review. Improving fact extraction is also a requirement in the whitepapers call (TA6-3a), and improving reasoning capabilities will enhance information gathering for decision making.' as abstract.

there is a document named 'doc-2274' that
  has 'This document describes ways to extend Controlled English [1] with additional syntax and associated semantics, thus allowing greater expressibility of facts and logical rules. For example it is desirable to use (predicative) adjectives such as "the person John is red", and to have the parser turn this into standard CE such as "the person John is a red thing". It is also desirable to provide facilities for the user to define such extensions in a declarative way, for example continuing the idea that CE can be used to configure the NL processing.  The document is aimed at the original P4.2 deliverable relating to "the use of linguistic transformation rules to support different fidelities of extracted information". However more recent work has suggested that the original idea of transformation rules [2] should be replaced by that of linguistic frames [3]. Therefore this document describes both possibilities, but comes down in favour of linguistic frames, as being more powerful.' as abstract.

there is a document named 'doc-2322' that
  has 'This working paper introduces the model produced to determine the "expansion" of a logical network with respect to its underlying physical network.  This is a metric which gives an indication of how efficient transmission of messages in a logical network is.' as abstract.

there is a document named 'doc-2329' that
  has 'In this paper, we study the selection of end-toend measurements in network tomography to optimize several important aspects of network management. We formulate an optimization problem with a modular or submodular objective function under the constraint of a matroid. We propose a general framework, called OptiMeas, to solve this optimization problem using a greedy algorithm. OptiMeas provides an optimal solution in the case of modular functions and achieves a 1/2- approximation for submodular functions. We apply OptiMeas to two separate optimization problems: the minimization of monitoring cost and the maximization of robustness against link failures, and also show that our framework allows us to optimize these two objective functions at the same time. We run a wide range of simulations based on realistic network topologies and link failure models to evaluate our solution against a baseline algorithm. We provide results that show significant improvements in both the monitoring cost and the robustness against failures for applying standard network tomography techniques.' as abstract.

there is a document named 'doc-2333' that
  has 'A decision procedure implemented over a computational trust mechanism aims to allow for decisions to be made regarding whether some entity or information should be trusted. As recognised in the literature, trust is contextual, and we describe how such a context often translates into a confidence level which should be used to modify an underlying trust value. Josang\'s Subjective Logic has long been used in the trust domain, and we show that its operators are insufficient to address this problem. We therefore provide a decision-making approach about trust which also considers the notion of confidence (based on context) through the introduction of a new operator. In particular, we introduce general requirements that must be respected when combining trustworthiness and confidence degree, and demonstrate the soundness of our new operator with respect to these properties.' as abstract.

there is a document named 'doc-2341' that
  has 'Some unusual activity was noted in regard of communications in Baghdad between known and unknown agents. A new agent named Amir Mahallati was detected. See "Reviewing Communications". The new approach to recognising "replay conversations" was analysed, and it was noted that some of these back-to-back conversations appeared to be in code, see "Reviewing Replay Conversations".  Therefore initial investigation has started in the construction of a modelling and analysis capability for representing codes, with the potential of identifying organisations that share use of these codes. Such modelling is being undertaken by the analyst team rather than requesting IT development. Details of this work are given in "Modelling Codes". It is recommended that further effort be expended to take this work further. It may be noted that the relevant facts shown in the tables are automatically extracted from the SYNCOIN reports and placed in this report via queries to the Controlled English system, these queries being entirely designed by the author of this report and contained in the body of this document. No further IT support was required.' as abstract.

there is a document named 'doc-2342' that
  has 'This document describes the processing flow as at the end of the BPP11 project 4 task 2, based upon the analysis of SYNCOIN [1] sentences. It is a CE embedded document [2], and the CE facts are based upon the "newsyncoin" directory. This uses the new lexical model, [3] and some (but not all) of the ACITA12 [4] demonstration rules. Note that this document is in draft form, and requires further work.' as abstract.

there is a document named 'doc-2346' that
  has 'The socially-distributed nature of cognitive processing in a variety of organizational settings means that there is increasing scientific interest in the factors that affect collective cognition. In military coalitions, for example, there is a need to understand how factors such as communication network topology, trust, cultural differences and the potential for miscommunication affects the ability of distributed teams to generate high quality plans, to formulate effective decisions and to develop shared situation awareness. The current paper presents a computational model and associated simulation capability for performing in silico experimental analyses of collective sensemaking. This model can be used in combination with the results of human experimental studies in order to improve our understanding of the factors that influence collective sensemaking processes.' as abstract.

there is a document named 'doc-2390' that
  has 'This document briefly describes how the ERG, the PET parser and the Prolog-based "erg service" can be installed and run on Debian Linux.   It should be noted that some minor changes have been made to the PET source code, and these changes are part of the delivery.  This document is a working document and will be updated as necessary.' as abstract.

there is a document named 'doc-2495' that
  has 'The networked information systems available to modern militaries, as well as the vast array of sensors now employed in intelligence gathering allow an unprecedented amount of information to be collected and disseminated to decision makers. Information must be processed, linked and aggregated in suitable ways to allow a decision maker to make high quality decisions. It is critical to develop techniques that help the decision maker understand the characteristics and the implications of the information so as to factor it into his/her decisions. In the area of intelligence analysis, a technique, called linchpin analysis, is used by analysts to identify underlying assumptions explicitly, evaluate and challenge these assumptions so as to explore alternative hypotheses and analyze them thoroughly. Linchpin analysis is a proven technique used by analysts to reach high quality intelligence assessments. However, linchpin analysis has not been computationally formalized. On the other hand, the computational theory of argumentation has its root in interdisciplinary research across philosophy, linguistics, psychology, communication studies, and artificial intelligence. Echoing linchpin analysis, argumentation schemes are patterns of reasoning, questioning, and defeating that enable one to identify and evaluate argumentation in everyday discourse investigating facts, assumptions, evidence, rules, information sources, and so on. While linchpin analysis has its root in real world practice, the theory of argumentation schemes has developed solid theoretical models of reasoning, arguments, defeats, and the acceptability of arguments. To the authors\' best knowledge there is no study in the literature which connects argumentation schemes to linchpin analysis. This report surveys the literature of these two areas seeking opportunities for a role of argumentation schemes in linchpin analysis, and identifying the enhancements needed in the theory of argumentation schemes and their computational models to support linchpin analysis based on vast amount of information available in networked information systems.' as abstract.

there is a document named 'doc-2509' that
  has 'This report is a deliverable for Project 4 task 2, Quarter 1, originally named "Initial principles for mapping external resources into CE framework". It is an extension of the long paper submitted to the Fall Meeting 2013 [22] Fact extraction from unstructured sources is a key component in the supply of information to human users such as analysts, but the extraction of the complete set of facts is a complex and challenging task. In addition it is necessary to express these facts using a conceptual model of the domain understood by the users and to present the rationale for their extraction in order that they can use and assess the facts in their analysis tasks.  In the BPP11, [1,2] we demonstrated an approach of using Controlled English (CE) [3,4] for the facts extracted by Natural Language (NL) processing and the configuration of the NL processing itself, in order that extracted facts may be used for inference of high value information, and that linguistic processing is made more accessible to the analyst user. Two key aspects were the development of a common linguistic model and the mapping of the linguistic structures into the domain semantics of the user. However linguistic capabilities of the BPP11 parsing system was limited, and we proposed to address this in the BPP13 research by seeking to integrate more sophisticated linguistic systems developed by the DELPH-IN consortium [6]. This report describes initial work into the use of the English Resource Grammar (ERG) [11] from the DELPH-IN community, capable of generating high quality and detailed representations of the syntax and semantics of English sentences, and outlines how transformations might be made between knowledge in the ERG and CE-based representations, so that the semantic output can be extracted into higher quality CE facts, that domain semantics contained in a CE model can be applied to assist parsing and that linguistic reasoning can be made more available to the non-technical human user when configuring the extraction process to a particular domain, in support of information access and knowledge sharing in coalition operations.' as abstract.

there is a document named 'doc-2512' that
  has 'This report is a deliverable for Project 4 task 2, Quarter 1, named "Motivating Scenario". It is a working document as discussion is still continuing with the partners and SMEs in this area.' as abstract.

there is a document named 'doc-2547' that
  has 'Cognitive social simulations, enabled by cognitive architectures (such as ACT-R), can be used to advance our understanding of socially-distributed and socially-situated cognition. As a result, multi-agent simulations featuring the use of ACT-R agents may be valuable in terms of exploring the factors that influence collective sensemaking. While previous studies have demonstrated the feasibility of using ACT-R to model collective cognition, as well as sensemaking processes at the individual level, the development of an ACT-R model of collective sensemaking in a coalition environment presents a range of relatively novel methodological, technological and modeling challenges. Such challenges include the need to equip ACTR agents with communication capabilities, the need to deal with highly dynamic information environments, the need to support intelligent information retrieval capabilities, and the need to represent inter-agent cognitive differences. These challenges shape the nature of research and development efforts to create a multi-agent simulation capability that can be used to explore the impact of different sociotechnical interventions on collective sensemaking processes. In this paper, we undertake a selective review of the literature relating to sensemaking, cognitive architectures and cognitive social simulation. We also discuss the motivations for using ACT-R to model collective sensemaking processes within the context of the International Technology Alliance (ITA) research program. Finally, we outline a number of research focus areas that are relevant to the development of a computational socio-cognitive model of collective sensemaking.' as abstract.

there is a document named 'doc-2564' that
  has 'This document describes an investigation into a specific parsing problem when analysing sentences from the SYNCOIN dataset. This problem was taken up as an example of the need to modify the ERG, and hence as an example for studying the workings of the ERG in terms of the representations employed and the algorithms used to parse sentences.    The specific problem involves the processing of the word "male" by the ERG. In the sentence "male called a woman", the ERG (1111) fails to create a parse. This has implications for the SYNCOIN sentences, where a sentence like "call from unidentified male to a male" also fails to parse.' as abstract.

there is a document named 'doc-2568' that
  has 'This note describes the processing that occurred in the SYNCOIN3000 demonstration to move from the linguistic semantics defined in MRS to the domain specific semantics for the SYNCOIN scenario.   General models for the ERG output, including the MRS are defined in Model_ERG.doc, which also describes how these are to be compiled and used. Domain semantic models are defined elsewhere, according to the domain, for example in Model_SyncoinSituations.doc.' as abstract.

there is a document named 'doc-2573' that
  has 'This document briefly describes how the ERG, the PET parser and the Prolog-based "erg service" can be installed and run on Debian Linux.   It should be noted that some minor changes have been made to the PET source code, and these changes are part of the delivery.  This document is a working document and will be updated as necessary.' as abstract.

there is a document named 'doc-2580' that
  has 'In cloud computing environments, one of the basic problems is how to efficiently assign physical resources to satisfy user-application\'s processing and communication demands. In its full generality, this application placement problem is notoriously hard, and therefore, heuristic algorithms are employed in common practice, which may unknowingly suffer from poor performance as compared to the optimal solution. In this paper, we focus on developing practical application placement algorithms with provable performance bounds. The crux of our approach is to restrict the placement to "cycle-free" path assignment which enables us to obtain an exact optimal algorithm for linear application graph placement and online approximation algorithms for tree application graph placement. In our formulation, we jointly consider node and link assignments, and also incorporate multiple types of computational resources at nodes, as well as practical domain and conflict constraints related to security and access-control policies.' as abstract.

there is a document named 'doc-2603' that
  has 'This note explores how the ELICIT factoids may be interpreted into a more formal representation, such as Controlled English. The factoids are a good corpus of simple English sentences, but they also provide a "logic puzzle" that could demonstrate cognitive reasoning either with a single agent or as a collaboration of agents. There is a need in Project 4 Task 1 to turn these sentences into CE for the purpose of communication between ACT-R agents, and there is also a possibility that these sentences could be used to demonstrate the capabilities of CE rules to actually solve the puzzle itself, or to act as a query capability for other agents. Finally, the ELICIT factoids can act as another corpus for exploring the ERG system, and the issues of mapping linguistic semantics (and expressions of uncertainty) to domain semantics.' as abstract.

there is a document named 'doc-2604' that
  has 'This note suggests a simple formulation of the ELICIT sentences, including rules and facts. These may be run to generate a possible solution, as described in ELICITIdentification.doc.' as abstract.

there is a document named 'doc-2605' that
  has 'This document records the output of the ERG system when run against the ELICIT sentences, modified as per ELICITSentences.doc.  The output is shown here as a table, generated by the Prolog component from the CE version of the MRS. In the table, for convenience, the elementary propositions are shown as rows, each having a name constructed from the identifier in the CE and the mrs relation of which it is an instance.' as abstract.

there is a document named 'doc-2610' that
  has 'This document describes a simple example of uncertainty handling in CE, based upon simple relations between people, and contains an example of negative support. This may be used as a simple test for the uncertainty handling mechanisms in the CE interpreter. The logic of assumptions, as employed in the interpreter is described in "Ambiguity in Natural language processing".' as abstract.

there is a document named 'doc-2611' that
  has 'This document describes a simple example of uncertainty handling in CE used to analyse an ambiguity in NL text. This uses the domain model in SampleDomain.doc. Some of the key transformations from MRS to CE are described in Model_MRSToCE_1.doc.' as abstract.

there is a document named 'doc-2612' that
  has 'This report is a deliverable for Project 4 Task 2, Quarter 3, named "CE-based mechanisms for handling ambiguity in Natural Language". The document first describes some aspects of Natural Language that lead to ambiguities, as examples that may be handled by CE-based mechanisms. It should be noted that a more comprehensive document is being produced by the Boeing team on ambiguity in Natural Language, as another deliverable in this quarter [1]. In this current document we are focusing on several examples that demonstrate the approach.' as abstract.

there is a document named 'doc-2633' that
  has 'This report presents architecture options for the realization of an inference management firewall (IMF) that employs context-aware information masking techniques for systematic management of risk-vs-value trade-off of sensor data. Previously we have demonstrated an initial implementation of the IMF running as messaging services on the ITA Information Fabric. Furthermore, we have presented an additional asset, recently implemented on a commercially-available mobile device running the Android operating system, which is intended to operate as an information source and first-line inference management capability at the edge of the network. In this report, we show how similar inference management techniques can proficiently encompass richer knowledge representation and reasoning models. In particular, we focus on knowledge representation based on Description Logic (DL), a widely recognized machinery able to provide computer-readable descriptions of an application domain. Finally, we present a technological case study, showing how the Fabric architecture can encompass policies for inference management of ontological-enriched knowledge.' as abstract.

there is a document named 'doc-2636' that
  has 'Linchpin analysis is a well accepted approach in human intelligence analysis. The tasks of linchpin analysis are 1) to identify the key drivers (evidence, premises, factors or variables) that will determine outcomes, and 2) to address any indicators or singposts that would render the premises or outcomes unreliable or reverse the outcomes dramatically. To facilitate linchpin analysis, we have formalized a computational model of Markov Argumentation Random Field (MARF). In the reality, linchpin analysis will need to depend on knowledge and received information (e.g. sensor readings, human reports and etc.) that is argumentative (i.e. defeasible because of inconsistency and uncertainty). MARF compiles the argumentative knowledge and received information into a mathematically rigid Markov Random Field (MRF). The resulting MARF is able to track both supporting links and conflicting links (argumentative defeats) among the outcomes, the applied knowledge and the received information. It can compute the most probable argumentation for the outcomes and identify the pieces of knowledge or received information that would render the premises or outcomes unreliable or reverse the outcome dramatically. In addition, MARF provides a computational framework to learn probabilistic evaluation functions of the premises and outcomes from expert supervised data. This work will be a key building block for our task 6.2: In-time Assembly and Presentation of Relevant Information in Support of Decision Making.' as abstract.

there is a document named 'doc-2638a' that
  has 'CDN servers play an important role to provide internet users with content where data has been moved to the edge for better access performance. However, the performance of CDN servers assigned to cellular users is not fully studied yet. Our results show that the improper CDN servers assignment do exit in cellular networks. They affect 20% of the traffic is affected by improper assignment of CDNs, which can result in 71x increased response latencies in the worst case.' as abstract.

there is a document named 'doc-2643' that
  has 'Smartphones with built-in and externally connected sensors of different modalities are increasingly being used to instrument our personal spaces for unobtrusive and continuous collection of sensor data. The personal data thus collected embeds in it minute details of our daily life and activities. On one hand, the user has variety of incentives to share the personal data with third-party apps and their developers which include receiving targeted advertisements, behavior tailored app services, and delegation of computation on their behalf - obtaining utility. On the other hand, the same data can be misused by the developer (acting as an adversary) to derive inferences sensitive to the user resulting in loss of privacy. Traditional approaches to tackling privacy have focused on disassociating the user identity from the shared data, thus preventing an adversary from tracing a sensitive inference back to the user. In this dissertation, we provide a novel perspective to the privacy problem and consider a complimentary problem setting where user identity is an inalienable part of the shared data. Thus, instead of identity we focus on protecting a set of sensitive inferences that a user wants to keep private. The contributions of this dissertation are multifold. First, using informationtheoretic notions we define utility and privacy parameters and formulate the privacy problem in terms of a whitelist of utility providing allowed inferences, and a ii blacklist of sensitive inferences. We derive bounds on the feasible region spanned by these parameters, and also provide constructive schemes for achieving certain boundary points. Second, using insights from the theoretical exploration, we design and implement ipShield, a privacy-enforcing system by modifying the Android OS. In addition to better monitoring of resources and privacy rule recommendations, using ipShield we also raise the level of privacy abstraction from sensors to inferences to better express the risks of data sharing. Finally, to reduce the user burden of configuring privacy rules, and minimize their overall interaction with a privacy-enforcing system we present iDeceit. This is a framework that implements model-based plausible falsification of sensor data. It uses a Markov chain to model the temporal transitions between inferences, of which some are marked as sensitive by the user. Using suitable utility and privacy metrics, iDeceit combines prediction with past user behavior to output plausible, falsified and synthetic sensor data while maintaining perfect privacy and maximizing utility.' as abstract.

there is a document named 'doc-2644' that
  has 'Providing analysts and decision makers with a means of assessing how certain a fact is and what the sources of uncertainty are is an important part of the provenance of a piece of intelligence. A major source of uncertainty derives from the various text reports that analysts and decision makers rely on. One of the major tasks of Project 4.2 is to use CE to extract facts from such reports, infer important information, and provide this to end-users. This paper further investigates the sources of uncertainty that derive from language and its use and how this might be represented to analysts.' as abstract.

there is a document named 'doc-2676' that
  has 'This paper describes our research into ITA Controlled English (CE) to represent knowledge and reasoning in different domains. A basic core of restricted English allowing expression of concepts, entities, and relationships has been extended to express negation, assumptions, uncertainty values, and meta-level reasoning about concepts. We describe a reasoning capability based on CE and provide examples of its application to cognitive tasks such as intelligence analysis and fact extraction from Natural Language text, with provision of rationale to users for conclusions derived.' as abstract.

there is a document named 'doc-2690' that
  has 'There is a need to extend CE to include phrases expressing the time and location of the events represented in a CE sentence, for example "the person John loves the person Mary in the place Paris" or "there is an attack situation at 1100 on Tuesday". Grammatically, such expressions are called "adjunct" since there are not mandatory (could be omitted) and are general to all verbs.   Such extensions would be useful to support temporal and spatial reasoning and would make it more elegant to state these contextual conditions attached to basic CE sentences. Since they are not specific to particular verbs, it is hoped that a generic mechanism for implementing them will be easier to define (than say preposition phrases that are specific to particular verbs). The experience in handling these extensions will facilitate the construction of prepositional extensions that do depend upon the verbs involved.' as abstract.

there is a document named 'doc-2695' that
  has 'The purpose of this Module Design Specification for the ACT-R Multi-agent CE Integration is to document the high level design of the Controlled English integration. It also ensures that all aspects of the design have been thought through and are in a form suitable for communication and review amongst the ITA Task.' as abstract.

there is a document named 'doc-2701' that
  has 'Computational trust mechanisms aim to produce trust ratings from both direct and indirect information about agents\' behaviour. In this document, we summarise the current state-of-the-art of the procedures for maintaining privacy while sharing information. In particular, we identify how we can reuse current approach for a specific new task, viz. privacy of trust/reputation information.' as abstract.

there is a document named 'doc-2705' that
  has 'This report provides a short and informal guide to configuring and using the ITA Experimentation Framework and Information Fabric. It is intended to be useful as a technical resource to aid with development and experimentation activities. It is being maintained by Stephen Pipes, for research currently underway in Project 6, Task 1.' as abstract.

there is a document named 'doc-2752' that
  has 'This document analyses the "Analysis Game" developed by Prof D Shemanski, of Penn State University. The Game is a logical puzzle that is used as part of a training exercise in analytic skills. We set ourselves the task of solving the problem using the Controlled English (CE) knowledge modeling and reasoning system, in order to research into how CE could be used in a more complex reasoning task. This document summaries the approach and the solution, and provides a conceptual model of the domain and a formulation of the puzzle sentences in terms of CE facts.' as abstract.

there is a document named 'doc-2759' that
  has 'CE is used for different purposes and at different levels of extraction, as exemplified on two sentences:  _   The Rose group may be involved The Brown group is recruiting locals' as abstract.

there is a document named 'doc-2773' that
  has 'The heterogeneous nature of tactical coalition networks poses several challenges to common network management tasks due to the lack of complete information shared among coalitions. In this paper we consider the problem of deploying services in mobile tactical networks. We propose iTop, an algorithm for inferring the network topology when only partial information is available. iTop initially constructs a virtual topology that overestimates the number of network components, and then repeatedly merges links in this topology to resolve it towards the structure of the true network. We perform extensive simulations and show that iTop enables an efficient deployment of services over the network despite the limitation of partial information.' as abstract.

there is a document named 'doc-2778' that
  has 'This paper summarises recent progress towards the initial realisation of a multitiered inference management architecture. This architecture enables systematic control to be exercised over the risk and utility trade-off when sharing information in a coalition network. Our previous work proposed a set of requirements for, and challenges facing, the development of an end-to-end inference management capability which has guided the latest design and implementation activities. Newly introduced features include the ability to distribute the command and control of the inference management process within the network. This facilitates the separation of phases in the policy specification and enforcement process thereby enabling diverse scenarios involving multiple parties where authority is distributed. We have also developed a messaging schema for expressing inference management goals which we are experimentally evaluating in a military coalition scenario.' as abstract.

there is a document named 'doc-2833' that
  has 'This document analyses some more complex sentences interpretations in the ELICIT sentences. The aim is both to avoid the simplification of the sentences, and to explore the use of domain knowledge to help understand the sentence. Two sentences have been considered, both involving the concept of an organisation and the factors that involve the operational status of a group.' as abstract.

there is a document named 'doc-2834' that
  has 'This document analyses the "Three Ties" problem created by Martin Gardner. This is a relatively simple problem, and we set ourselves the task of solving the problem using the Controlled English (CE) knowledge modeling and reasoning system, in order to research into how CE could be used in a more complex reasoning task. This document summaries the approach and the solution, and provides a conceptual model of the domain and a formulation of the puzzle sentences in terms of CE facts.' as abstract.

there is a document named 'doc-2836' that
  has 'This report is a deliverable of the ITA programme, in Quarter 6 of the BPP13, from Task 4.2. It provides some thoughts on the provision of a richer explanation of CE-based reasoning. At this stage it is an initial draft, and further work is required in this area.' as abstract.

there is a document named 'doc-2851' that
  has 'In this report, we formulated a model of argumentation-based linchpin sensitivity analysis which quantitatively analyzes the sensitivity of linchpins - the assumptions and reasons that leads to decision making. The linchpin sensitivity is established based on Markov Argumentation Random Field (MARF). Given a set of facts represented in a structural format, such as predicate logic, MARF constructs a probabilistic graphical model using argumentation theory to identify the support and defeat links. These links are then used to construct justifications to support or reject conclusions. These links, as well as the derived supporting and rejecting justifications, are parameterized numerically with training data to simulating human experts reasoning. Upon the established probabilistic graphical model, how the linchpin- the facts, assumptions, support links and defeat links - affect the conclusions is quantified. MARF provides a rigorous model with formal semantics of quantitative probabilistic reasoning on inconsistency thus facilitating linchpin sensitivity analysis and making probabilistic reasoning more understandable to human users. Integrated with the argumentation presentation system that we previously developed, the most important facts and assumptions as well as the derived structure over the information, e.g. support and defeat links, are incrementally presented to the human decision makers so as to highlight for the decision makers the critical information and the critical structure over the information while minimizing the cognitive workload of the decision makers.' as abstract.

there is a document named 'doc-2856' that
  has 'Growing use of location-aware apps that require sharing of user\'s location necessitate finding methods to protect users\' location privacy. Users often visit locations that are sensitive to them and need to be kept private. Existing mitigation techniques include the addition of random noise to the data, which is inadequate because location data often exhibit known temporal and spatial correlation patterns that can be exploited by an adversary to perform denoising. The technique of selective suppression also does not work well as the very act of suppression reveals sensitive information and often apps are not designed to handle data suppression. Finally, manually configuring fine-grained privacy rules may help protect location privacy, but it is cumbersome and hard to remember in every sensitive situation. We present a framework that implements substitution of sensitive data segments with synthetic data to protect sensitive locations while ensuring the plausibility of the entire location trace. To ensure plausibility while preserving utility, the framework employs a user-behavior model to find candidate segments for substitution that captures the temporal correlation between the different locations visited by the user over time. We show formally that our model-based substitution ensures zero-additional leakage to privacy.' as abstract.

there is a document named 'doc-2892' that
  has 'The combination of software-as-a-service and the increasing use of mobile devices gives rise to a considerable difference in computational power between servers and clients. Thus, there is a desire for clients to outsource the evaluation of complex functions to an external server. Servers providing such a service may be rewarded per computation, and as such have an incentive to cheat by returning garbage rather than devoting resources and time to compute a valid result. In this work, we introduce the notion of Revocable Publicly Verifiable Computation (RPVC), where a cheating server is revoked and may not perform future computations (thus incurring a financial penalty). We introduce a Key Distribution Center (KDC) to efficiently handle the generation and distribution of the keys required to support RPVC. The KDC is an authority over entities in the system and enables revocation. We also introduce a notion of blind verification such that results are verifiable (and hence servers can be rewarded or punished) without learning the value. We present a rigorous definitional framework, define a number of new security models and present a construction of such a scheme built upon Key-Policy Attribute-based Encryption.' as abstract.

there is a document named 'doc-2894' that
  has 'This document outlines some principles on how simple and informal experimentation on the use of CE for reasoning might be undertaken within Task 4.2.' as abstract.

there is a document named 'doc-2902' that
  has 'This paper presents a summary of the latest results of a research asset, called Inference Management Firewall (IMF), which is being actively developed as a platform for experimentation and validation within Task 6.1. The components required for achieving an inference management capability are reviewed, an architecture is presented and details of the current implementation are described. The paper also discusses a possible exploitation path for our work in the domain of intelligence analysis.' as abstract.

there is a document named 'doc-2908' that
  has 'Event monitoring is an important application of sensor networks. Multiple parties, with different surveillance targets, can share the same network, with limited sensing resources, to monitor their events of interest simultaneously. Such a system achieves profit by allocating sensing resources to missions to collect event related information (e.g., videos, photos, electromagnetic signals). We address the problem of dynamically assigning resources to missions so as to achieve maximum profit with uncertainty in event occurrence. We consider time- varying resource demands and profits, and multiple concurrent surveillance missions. We model each mission as a sequence of monitoring attempts, each being allocated with a certain amount of resources, on a specific set of events that occurs as a Markov process. We propose a Self-Adaptive Resource Allocation algorithm (SARA) to adaptively and efficiently allocate resources according to the results of previous observations. By means of simulations we compare SARA to previous solutions and show SARA\'s potential in finding higher profit in both static and dynamic scenarios.' as abstract.

there is a document named 'doc-1424' that
  has 'Several key concepts in the Network Enabled Cognition task (BPP09 section 5.3) are "Cognitive Extension" where cognitive mechanisms are extended into the environment and "Network Extension" where these extensions are spread over a network of heterogeneous collaborative elements. An interesting hypothesis underlying these concepts is that the use of the environment and networks in this way actually enhances cognitive processing, thus the extension is of benefit.' as abstract.

there is a document named 'doc-2502b' that
  has 'An authenticated data structure (ADS) is a data structure whose operations can be carried out by an untrusted prover, the results of which a verifier can efficiently check as authentic. This is done by having the prover produce a compact proof that the verifier can check along with each query result. ADSs thus support outsourcing data maintenance and processing tasks to untrusted servers without loss of integrity. Past work on ADSs has focused on particular data structures (or limited classes of data structures), one at a time, often with support only for particular operations. This paper presents a generic method, using a simple extension to a MLlike functional programming language we call __ (lambda-auth), with which one can program authenticated operations over any data structure constructed from standard type constructors, including recursive types, sums, and products. The programmer writes the data structure largely as usual; it can then be compiled to code to be run by the prover and verifier. Using a formalization of __ we prove that all well-typed __ programs result in code that is secure under the standard cryptographic assumption of collision-resistant hash functions. We have implemented our approach as an extension to the OCaml compiler, and have used it to produce authenticated versions of many interesting data structures including binary search trees, red-black trees, skip lists, and more. Performance experiments show that our approach is efficient, giving up little compared to the hand-optimized data structures developed previously.' as abstract.

there is a document named 'doc-2565b' that
  has 'In a Secure Multiparty Computation (SMC), mutually distrusting parties use cryptographic techniques to cooperatively compute over their private data; in the process each party learns only explicitly revealed outputs. In this paper, we present WYSTERIA, a high-level programming language for writing SMCs. As with past languages, like Fairplay, WYSTERIA compiles secure computations to circuits that are executed by an underlying engine. Unlike past work, WYSTERIA provides support for mixed-mode programs, which combine local, private computations with synchronous SMCs. WYSTERIA complements a standard feature set with built-in support for secret shares and with wire bundles, a new abstraction that supports generic n-party computations. We have formalized WYSTERIA, its re- finement type system, and its operational semantics. We show that WYSTERIA programs have an easy-to-understand singlethreaded interpretation and prove that this view corresponds to the actual multi-threaded semantics. We also prove type soundness, a property we show has security ramifications, namely that information about one party\'s data can only be revealed to another via (agreed upon) secure computations. We have implemented WYSTERIA, and used it to program a variety of interesting SMC protocols from the literature, as well as several new ones. We find that WYSTERIA\'s performance is competitive with hand-rolled approaches while making programming far easier, and more trustworthy.' as abstract.

there is a document named 'doc-2565d' that
  has 'In a Secure Multiparty Computation (SMC), mutually distrusting parties use cryptographic techniques to cooperatively compute over their private data; in the process each party learns only explicitly revealed outputs. In this paper, we present WYSTERIA, a high-level programming language for writing SMCs. As with past languages, like Fairplay, WYSTERIA compiles secure computations to circuits that are executed by an underlying engine. Unlike past work, WYSTERIA provides support for mixed-mode programs, which combine local, private computations with synchronous SMCs. WYSTERIA complements a standard feature set with built-in support for secret shares and with wire bundles, a new abstraction that supports generic n-party computations. We have formalized WYSTERIA, its re- finement type system, and its operational semantics. We show that WYSTERIA programs have an easy-to-understand singlethreaded interpretation and prove that this view corresponds to the actual multi-threaded semantics. We also prove type soundness, a property we show has security ramifications, namely that information about one party\'s data can only be revealed to another via (agreed upon) secure computations. We have implemented WYSTERIA, and used it to program a variety of interesting SMC protocols from the literature, as well as several new ones. We find that WYSTERIA\'s performance is competitive with hand-rolled approaches while making programming far easier, and more trustworthy.' as abstract.

there is a document named 'doc-813' that
  has 'Utility is a measure of an asset that quantifies the benefit it provides, or the satisfaction it creates for a stakeholder. The asset could be a sensor, a vehicle, an allocation of communications bandwidth, an information product or stream, or a consumable such as time or battery energy. Cardinal utility is intended to explain some absolute measure that will be comparable across assets and stakeholders. Ordinal utility captures differences in benefit, perhaps as rankings over options, or as a derivative with respect to some change in asset parameters.' as abstract.

there is a document named 'doc-1372' that
  has 'This chapter discusses the problem of agent aiding of ad-hoc, decentralized human teams so as to improve team performance on time-stressed group tasks. To see how human teams rise to the challenge, the authors analyze the communication patterns of teams performing a collaborative search task that recreates some of the cognitive difficulties faced by teams during search and rescue operations. The experiments show that the communication patterns of successful decentralized ad-hoc teams performing a version of the task that requires tight coordination differ both from the teams that are less successful at task completion and from teams performing a loosely coupled version of the same task. The authors conclude by discussing (1) what lessons can be derived, from observing humans, to facilitate the development of agents to support ad-hoc, decentralized teams, and (2) where can intelligent agents be inserted into human teams to improve the humans\' performance.' as abstract.

there is a document named 'doc-1373' that
  has 'Intelligent software personal assistants are an active research area with the potential to revolutionize the way that human organizations operate, but there has been little research quantifying how they will impact organizational performance or how organizations will or should adapt in response. In this chapter, the authors develop a computational model of the organization to evaluate the impact different proposed assistant abilities have on the behavior and performance of the organization. By varying the organizational structures under consideration, they can identify which abilities are most beneficial, as well as explore how organizations may adapt to best leverage the new technology. The results indicate that the most beneficial abilities for hierarchical organizations are those that improve load balancing through task allocation and failure recovery, while for horizontal organizations the most beneficial abilities are those that improve communication. The results also suggest that software personal assistant technology will facilitate more horizontal organizations.' as abstract.

there is a document named 'doc-1393a' that
  has 'Effective deployment and utilisation of limited and constrained intelligence, surveillance and reconnaissance (ISR) resources is seen as a key issue in modern network-centric joint-forces operations. In this chapter, we examine the application of semantic matchmaking and argumentation technologies to the management of ISR resources in the context of coalition operations. We show how ontologies and reasoning can be used to assign sensors and sources to meet the needs of missions, and we show how argumentation can support the process of gathering and reasoning about uncertain evidence obtained from various sources.' as abstract.

there is a document named 'doc-1921' that
  has 'In this paper we overview a language to write distributed applications. We provide an operational semantics of a single computational node based on Datalog. We then introduce a framework that can capture the semantics of a network of computational nodes working together. The framework can capture several communication models (e.g. synchronous vs. asynchronous) and can be used to check many properties of the distributed computation under the different communication models. The framework is developed using Answer Set Programs.' as abstract.

there is a document named 'doc-2286' that
  has 'Wireless networks provide flexible ubiquitous user communications but, at the same time, provide the same access to an attacker or eavesdropper trying to intercept private messages. Hence, security has become a primary concern in wireless networks, and there has been a resurgence of interest in physical layer forms of secrecy based on Shannon\'s original information-theoretic formulation and Wyner\'s consideration of the wiretap formulation of such. In this chapter, we venture beyond the prototypical three-node Alice-Bob-Eve security scenario to consider physical layer forms of secrecy in wireless networks, where the need to protect the message over multiple hops opens up system vulnerabilities, but the addition of system nodes beyond the communicating parties also offers opportunities. In particular, the scaling of secrecy in (asymptotically) large multi-hop networks will be considered, and we will focus, in particular, on how the flexibility offered by the large number of system nodes in the network can facilitate secure communication in the face of a large number of eavesdroppers, even if the location of those eavesdroppers is unknown. The transmission techniques that are employed in the conclusive results for secrecy scaling in large networks with non-collaborating eavesdroppers also have clear analogs that should prove effective in finite networks, but there are challenges to be addressed in such an adaptation.' as abstract.

there is a document named 'doc-2911' that
  has 'Cognitive architectures are computational frameworks that support the development of computational models of human cognitive processes. They have typically been used to advance our understanding of human cognition in specific task environments; however, they have also been used to support the development of a variety of intelligent systems and agents (e.g., cognitive robots). Although a variety of cognitive architectures are available, such as SOAR (Laird, 2012; Laird, Newell, & Rosenbloom, 1987), ACT-R (Anderson, 2007; Anderson et al., 2004) and CLARION (Sun, 2006a; Sun, 2007), the focus of the current chapter is on ACT-R. ACT-R is a rule-based system that has been widely used by cognitive scientists to model aspects of human cognitive performance. Of all the cognitive architectures that are currently available, the ACT-R architecture is probably the one that is best grounded in the cognitive psychology literature. In addition, it is one of the few cognitive architectures that attempts to make explicit predictions about the recruitment of different brain regions in cognitive processes: the structural elements of the core ACT-R architecture (i.e., its modules and buffers) map onto different regions of the human brain (Anderson, 2007), and this enables cognitive modellers to make precise predictions about the activity of brain regions at specific junctures in a cognitive process (see Anderson, Qin, Jung, & Carter, 2007).' as abstract.

there is a document named 'doc-x0025' that
  has 'Coalition operations greatly benefit from the exchange of information collected from a plethora of wirelessly communicating sensors deployed in the theater of operation. However, to make judicious use of this information and act effectively based on this information knowledge of its quality, and a common means of expressing and communicating it are necessary. This chapter considers quality of information (QoI) for sensor networks starting with building a definition of it from first principles and also exploiting industry efforts to define quality in other domains, notably quality of service. A layered definition of QoI is presented layering the value of information for an application on top of the information\'s quality attributes. It then discusses information as a service and the various QoI viewpoints that associate sensor-originated information providers with sensor-enabled applications in dynamic coalition environment. Then, information processors and operators are introduced, which are functional modules of end-to-end systems that process information based on QoI attributes. Finally, a data model for QoI metadata, describing the QoI attributes, is presented. The data model provides common means to describe and communicate QoI attributes among information processors. It enables the indexing and searching of the most pertinent information, of desired quality, and information sources and the on-demand binding of applications to sources that is necessary to support operations in multiparty coalitions.' as abstract.

there is a document named 'doc-x0030' that
  has 'Whereas the traditional view in cognitive science has been to view mind and cognition as something that is the result of essentially inner, neural processes, the extended cognition perspective claims that at least some human mental states and processes stem from complex webs of causal influence involving extra-neural resources, most notably the resources of our social and technological environments. In this chapter, we explore the possibility that contemporary and near-future network systems are poised to extend and perhaps transform our human cognitive potential. We also examine the extent to which the information and network sciences are relevant to our understanding of various forms of cognitive extension, particularly with respect to the formation, maintenance and functioning of extended cognitive systems in network-enabled environments. Our claim is that the information and network sciences are relevant on two counts: firstly, they support an understanding of the mechanisms underpinning socially- and technologically-mediated forms of cognitive extension; secondly, they serve to guide and inform engineering efforts that strive to enhance and expand our cognitive capabilities. We discuss the relevance and applicability of these conclusions to current and future research exploring the contribution of network technologies to military coalition operations.' as abstract.

there is a document named 'doc-1758a' that
  has 'In many settings, agents (whether human or artificial) engage in problem solving activities, which require them to share resources, act on each others\' behalf, communicate and coordinate individual acts, and so on. If autonomous agents are to effectively interact (or support interaction among humans) in situations such as deciding whom and how to approach the provision of a resource or the performance of an action, there are a number of important questions to address. Who do I choose to delegate a task to? What do I need to say to convince him/her to do something? Were similar requests granted from similar agents in similar circumstances? What arguments were most persuasive? What are the costs involved in putting certain arguments forward? Research in argumentation strategies has received significant attention in recent years, and a number of approaches has been proposed to enable agents to reason about arguments to put forward in order to persuade another. However, current approaches do not adequately address situations where agents may be operating under social constraints (which we call policies) that regulate behaviour in a society. Furthermore, existing approaches are largely theoretical, lacking rigorous empirical evaluation. In this thesis, we propose a novel combination of techniques that takes into consideration the policies that others may be operating with. First, we present an approach where evidence derived from argumentation-based dialogue is utilised to learn the policies that others may be operating under. We show that this approach enables agents to build more accurate and stable models of others more rapidly. In addition, we demonstrate how background knowledge can be utilised to further refine such models. Secondly, we present an agent decision-making mechanism where models of other agents are used to guide future argumentation strategy. This approach takes into account the learned policy constraints of others, the cost of revealing information, and anticipated resource availability in deciding whom to approach for a resource or for an action to be done. Furthermore, we present a number of strategies that an agent can employ during such interactions. We empirically evaluate our approach within a simulated multi-agent framework, and demonstrate that through the use of such informed strategies agents can both significantly improve the cumulative utility of dialogical outcomes, and reduce communication overhead.' as abstract.

there is a document named 'doc-1892' that
  has 'Anticipating human subjects\' intentions and information needs is considered one of the ultimate goals of Artificial Intelligence. Activity and plan recognition contribute to this goal by studying how low-level observations about subjects and the environment in which they act can be linked to a high-level plan representation. This task is challenging in a dynamic and uncertain environment; the environment may change while the subjects are reasoning about it, and the effects of the subjects\' interactions cannot be predicted with certainty. Humans generally struggle to enact plans and maintain situation awareness in such circumstances, even when they work in teams towards a common objective. Intelligent software assistants can support human teams by monitoring their activities and plan progress, thus relieving them from some of the cognitive burden they experience. The assistants\' design needs to keep into account that teams can form and disband quickly in response to environmental changes, and that the course of action may change during plan execution. It is also crucial to efficiently and incrementally process a stream of observations in order to enable online prediction of those intentions and information needs. In this thesis we propose an incremental approach for team composition and activity recognition based on probabilistic graphical models. We show that this model can successfully learn team formations and behaviours in highly dynamic domains, and that classification can be performed in polynomial time. We evaluate our model within a simulated scenario provided by an open-source computer game. In addition, we discuss an incremental approach to plan recognition that exploits the results yielded by activity recognition to assess a team\'s course of action. We show how this model can account for incomplete or inconsistent knowledge about recognised activities, and how it can be integrated into an existing mechanism for plan recognition.' as abstract.

there is a document named 'doc-2319a' that
  has 'In dynamic multiagent systems, self-motivated agents pursuing individual goals may interfere with each other\'s plans. Agents must, therefore, coordinate their plans to resolve dependencies among them. This drives the need for agents to engage in dialogue to decide what to do in collaboration. Agreeing what to do is a complex activity, however, when agents come to an encounter with different objectives and norm expectations (i.e. societal norms that constrain acceptable behaviour). Argumentation-based models of dialogue support agents in deciding what to do analysing pros/cons for decisions, and enable conflict resolution by revealing structured background information that facilitates the identification of acceptable solutions. Existing models of deliberative dialogue, however, commonly assume that agents have a shared goal, and to date their effectiveness has been shown only through the use of extended examples. In this research, we propose a novel model of argumentation schemes to be integrated in a dialogue for the identification of plan, goal and norm conflicts when agents have individual but interdependent objectives. We empirically evaluate our model within a dynamic system to establish how the information shared with argumentation schemes influence dialogue outcomes. We show that by employing our model of arguments in dialogue, agents achieve more successful agreements. The resolution of conflicts and identification of more feasible interdependent plans is achieved through the sharing of focussed information driven by argumentation schemes. Agents may also consider more important conflicts, or conflicts that cause higher loss of utility if unresolved. We explore the use of strategies for agents to select arguments that are more likely to solve important conflicts. We show through an empirical evaluation that the most effective strategy combines the drive for agents to search for additional conflicts and the need to solve the most important ones already identified.' as abstract.

there is a document named 'doc-2829' that
  has 'Social computing initiatives that mark a shift from personal computing towards computations involving collective action, are driving a dramatic evolution in modern decision-making. Decisionmakers or stakeholders can now tap into the power of tremendous numbers and varieties of information sources (crowds), capable of providing information for decisions that could impact individual or collective well-being. More information sources does not necessarily translate to better information quality, however. Social influence in online environments, for example, may bias collective opinions. In addition, querying information sources may be costly, in terms of energy, bandwidth, delay overheads, etc., in real-world applications. In this research, we propose a general approach for truth discovery in resource constrained environments, where there is uncertainty regarding the trustworthiness of sources. First, we present a model of diversity, which allows a decision-maker to form groups, made up of sources likely to provide similar reports. We demonstrate that this mechanism is able to identify different forms of dependencies among information sources, and hence has the potential to mitigate the risk of double-counting evidence due to correlated biases among information sources. Secondly, we present a sampling decision-making model, which combines source diversification and reinforcement learning to drive sampling strategy. We demonstrate that this mechanism is effective in guiding sampling decisions given different task constraints or information needs. We evaluate our model by comparing it with algorithms representing classes of existing approaches reported in the literature.' as abstract.

there is a document named 'doc-1659' that
  has 'Effectively sharing potentially sensitive information to the relevant coalition partners in a risk optimal and timely manner has been recognized as a critical factor for the success of modern military coalition operations.  The main challenge to enable effective information exchange in a military coalition context is to address the inherently conflicting objectives between information consumers and producers.  On one hand, the information consumers seek the most complete, up-to-date and high quality information to make informed decisions. On the other hand, to share high quality sensitive information, the sender needs assurance from the recipient that the shared information will be appropriately protected from the threat of misuse; otherwise, he might be constrained (e.g., by security policies) to sharing only an obfuscated version of the information needed by the recipient.  Furthermore, the information provided by a sender might be the result of the aggregation/fusion of data originating from a variety of sources with varying degree of trust, accuracy and obfuscation level, all of which could influence the recipient\'s perceived value of the information received.' as abstract.

there is a document named 'doc-2190' that
  has 'The purpose of this workshop is to provide an introduction and detailed overview of ITA Controlled English (CE) that is a human-friendly structured information representation format that has been researched and developed during the previous ITA research. The motivations and benefits of CE will be covered, along with a brief summary of alternative approaches in this area and a perspective on why CE is different to these efforts. We will also review the specific CE syntax, with worked examples, showing how new conceptual models of a domain can be constructed, along with subsequent information being stated about that domain. The more advanced capabilities of CE will also be covered, including: rationale, truth values, uncertainty, statements of belief, assumption etc.' as abstract.

there is a document named 'doc-1670a' that
  has 'Please note that there is now a publically available version of the CE Store at http://github.com/ce-store' as abstract.

there is a document named 'doc-1124' that
  has 'Mobile ad hoc networks (MANETs) can enable effective communications in dynamic operation environments such as coalition military operations, emergency operations for disaster recovery, communication in vehicular area networks (VANETs). In these situations, multiple organizations in different administrative domains need to communicate and collaborate to achieve common goals. For example, in a disaster recovery scenario, the local police force may need to coordinate with fire fighters, military forces, and medical crews by sharing information and communicating with each other regardless of the particular networking technologies that are used by different parties. This kind of applications needs call for the development of a technology to enable end-to-end communications over heterogeneous MANET domains.' as abstract.

there is a document named 'doc-2187' that
  has 'The effective decision making relies on correlating, analyzing, and understanding the context of the large amounts of data that is being collected in a sensor network (or any other real-time system). In order to understand the context, data needs to be enriched through analysis and categorization based on their relationships to each other. This demo will illustrate a system that enables automated semantic enrichment of disparate data sources in support of effective decision making in coalition operating environments.' as abstract.

there is a document named 'doc-2188' that
  has 'In the context of intelligence, surveillance, and reconnaissance (ISR) operations, there are typically multiple ways to achieve a task using sensor-provided data. Given an ISR task and a set of sensing assets in a particular area of interest, there may be many options for resourcing that task. In a coalition context, the problem is more complex, because the assets may be \'owned\' by different partners. From the point of view of a user (for example, an ISR analyst) with a particular information need (for example, tracking high value targets in an area), the problem of identifying suitable ISR assets is difficult, without a great deal of knowledge about sensing capabilities and availability of coalition assets. We developed an approach founded on the Military Missions and Means Framework (MMF) [1]. We created ontologies of task and asset types - originally using the Web Ontology Language, OWL - and an automatic procedure for allocating assets to tasks [2].' as abstract.

there is a document named 'doc-2189' that
  has 'The demonstration will illustrate the use of Controlled English (CE) [1,2] for fact extraction from text-based documents. To provide automated assistance to an analyst in performing forensic analysis or answering queries, it is important to be able to convert text-based sources, such as reports, informal chat or data feeds into machine readable statements that are formally expressed in terms of the analysts conceptual model of the domain. We are using Controlled English to represent these machine readable statements about the facts extracted. It is also important to involve the analyst in the language processing that performs the fact extraction, as this may have to be tailored to the specific concepts, requirements and linguistic expressions for the domain of analysis.' as abstract.

there is a document named 'doc-2524' that
  has 'Mobile micro-cloud envisions a logical network composed of two components, the core (e.g., the command and control center) - with access to large quantities of static (and possibly stale) information and the edge (e.g., the forward operating base) - with access to smaller quantities of more real-time and dynamic data. The edge and core are separated by dynamic and performance constrained networks with a many-to-one relationship between the core and the edge. In this demonstration, we will illustrate a preliminary version of the mobile micro-cloud platform. This prototype is built on the IBM- ASPN platform, which provides a cloud-like virtualization environment for deployment into distributed resource-constrained environments. ASPN provides a software platform to deploy virtual machines, which have access to computing, storage, networks, and other resources. Applications run in distributed cloud-like containers, isolated from one another but able to communicate with each other (and the platform) in a secure manner. We will illustrate the advantages of moving applications to the edge using a demonstration scenario.' as abstract.

there is a document named 'doc-2544' that
  has 'This note summarises the results of processing the SYNCOIN sample sentences to date.' as abstract.

there is a document named 'doc-2776' that
  has 'Balancing the amount of information shared with a consumer such that on one hand it is sufficient for the consumer to perform computation and provide utility to the publisher and on the other hand the information is sanitised enough to prevent the leakage of sensitive detail is key to the operational efficiency of an information network. Previously, we have presented research towards a principled approach to managing the privacy versus utility trade-off during information sharing. One approach is based on the idea of treating potential inferences from shared data as primitives with which to reason about what a consumer can learn and we have demonstrated a policy-aware ITA In- formation Fabric that applies inference management techniques by obfuscating data as it flows across a network. We present a new integrated multitiered Inference Management Framework (IMF) which spans the information path from publisher to consumer and provides policy enforcements points (for masking of information) at various points in the path. This allows policy to be enforced both at source, allowing the information provider control over what is shared with the network, and in the network, allowing for policy enforcement to be delegated to peers.' as abstract.

there is a document named 'doc-2835' that
  has 'This demonstration shows the full running of the ELICIT sentences and rules. Note that the correct version of qkdtbswi must be running. It is repeating step 4.2 of the full running of the sentences.' as abstract.

there is a document named 'doc-1670b' that
  has 'The Prolog-based Controlled English reasoning system is a research prototype developed under the ITA programme. The user may express their knowledge about a problem domain as facts, rules and assumptions in ITA Controlled English (CE), a subset of English that is readable and writeable by human users. The system can be driven to infer new information by reasoning, and the making of assumptions, and the results of the reasoning can be displayed or used for further down-stream processing. The rationale for the inferred facts may also be graphed.  The reasoning system is similar to the Java-based CEstore, but provides some additional capabilities, such as assumption-based reasoning and the representation of CE facts as components of other objects. The system is integrated into Microsoft Word, allowing users to perform all of the representation and reasoning functions via Word macros and structures inside the documents; this provides a seamless means of interleaving the Controlled English with unstructured full Natural language, for the purpose of fuller documentation. The system is also integrated to the linguistic resources developed by the DELPH-IN community (http://www.delph-in.net/wiki/index.php/Home), and sentences can be sent to the PET parsing system from Word documents, and subsequently analysed against domain models provided by the user in CE.The following paragraphs provide a brief description of the files in this entry and how they may be installed. Several pieces of software need to be installed by the user as part of the description, and it is important that the user accepts the responsibility for these installations and the effects they may have on their system.The user will need to install, and accept the licence terms of, SWI-Prolog, (http://www.swi-prolog.org/) Microsoft Word (2013) and Graphviz (http://www.graphviz.org/). If the NL processing functionality is to be used, the user needs to compile and install a copy of the linguistic resources from DELPH-IN, specifically the PET parser, and the ERG (http://www.delph-in.net/wiki/index.php/Home).In order to use the Microsoft Word Document macros provided as part of the CE system, the macro files CEVBA.dot and CEVBA.exportedUI should be copied from "bin/toMSWordtemplates" (see below) into the user\'s Word template area (typically this will be "C:\\Users\\USERNAME\\AppData\\Roaming\\Microsoft\\Templates"). The "CEVBA.dot" file is a template that will be picked up by the sample documents described below. The "CEVBA.exportedUI" file is a customisation file for the Word "ribbon"; this may be imported as described in https://support.office.com/en-gb/article/Customize-the-ribbon-3c610b47-6f0f-4179-83d3-68a254a80ea6?ui=en-US&rs=en-GB&ad=GB&fromAR=1#__toc266259194The user should take note of the warning contained therein "Important:When you import a ribbon customization file, you lose all prior ribbon and Quick Access Toolbar customizations. If you think that you might want to revert to the customization you currently have, you should export them before importing any new customizations. The zip file provides the following subdirectories- "src" - the Prolog source code- "bin" - some examples of applications of the system to different domains, using only the binary version of the system generated from the source- "ergwrapper" - the Prolog source code that is needed to integrate to the DELPH-IN linguistic system, specifically the ERG and the PET parser.The "src" directory may be placed anywhere on the file store, and new "projects" may be established within the src/clce3/mods/ subdirectory, following the templates provided: "yourproject" for a simple project not involving the ERG NL processing or "yourergproject" for projects that involve the ERG. For a new project that requires changes to the source code, the user should copy one of these templates into the "mods" directory", make the changes and recompile by running the "qcompileme.pl" file, followed by typing "qcompileme." in the Prolog window. This generates a new "qkdtbswi.qlf" file which contains the binary of the source code. A new project may then be created in the binary directory, as described below, but using the new version of "qkdtbswi.qlf" in that project directory.The  "bin" directory may be placed anywhere on the file store, and is the suggested place where new projects are run and maintained. For projects that do NOT require a change to the code, a copy of the example templates "yourergproject" (for those involving ERG processing) or "yourproject" (for those not involving ERG processing) may be taken and placed in "yourrootdirectory". (These use the predefined version of the binary file "qkdtbswi.qlf").  Several other examples of such project files are included:- balda, involving a logic puzzle based upon work by Raymond Smullian- analysisgame1, involving a teaching game originated by Penn State University- nlpbasic, involving simple NL processingEach project directory includes some system files and a subdirectory called "documents". This is where the user should create and edit the Word documents that are to be used as the interface to the reasoning system; in the template projects the document is called "Model.docx". From these documents the user can: start the CE system, compile the conceptual model contained in the document, run the reasoning and see the rationale. Some minimal information is provided in these documents as to how this is done, more details are planned in the future.Copyright and LicensingThe IBM Prolog-based Controlled English store is written in SWI-Prolog, and Microsoft Visual Basic and has been developed under the ITA Programme, with the following licence:%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Licensed Materials - Property of IBM% (C) Copyright IBM Corporation, 2010, 2016% ALL RIGHTS RESERVED%% This research was sponsored by the U.S. Army Research Laboratory and% the U.K. Ministry of Defence and was accomplished under Agreement% Number W911NF-06-3-0001. The views and conclusions contained in this% document are those of the author(s) and should not be interpreted as% representing the official policies, either expressed or implied, of the% U.S. Army Research Laboratory, the U.S. Government, the U.K. Ministry% of Defence or the U.K. Government. The U.S. and U.K. Governments are% authorized to reproduce and distribute reprints for Government purposes% notwithstanding any copyright notation hereon.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%This software and associated documents comprise a research prototype, and is offered AS-IN with no guarantee of function, support or documentation. The user will need to install, and accept the licence terms of, SWI-Prolog, (http://www.swi-prolog.org/) Microsoft Word (2013) and Graphviz (http://www.graphviz.org/). If the NL processing functionality is to be used, the user must install a copy of the linguistic resources from DELPH-IN, specifically the PET parser, and the ERG (http://www.delph-in.net/wiki/index.php/Home).SWI-Prolog has a facility for checking application object code, to determine if the code requires access to third-party Prolog libraries, and (for information only) the following results were obtained when running the "eval_license" on the IBM Prolog application software, "qkdtbswi.qlf": % This program may be distributed under any license, provided all% conditions implied by the GNU Lesser General Public License% are satisfied.  In particular, this implies the source code% to any modification in SWI-Prolog or one of the used libraries% must be made available.% % See http://www.swi-prolog.org/license.html for details on% SWI-Prolog licensing policies supporting both free and non-free% Software.No modifications have been made to the source code of SWI-Prolog or any of the used libraries. This is for information only and does not offer any guarantee of licence conditions.Although no support is offered for this project, seriously interested parties might contact Davd Mott (mottd@uk.ibm,com) if they have queries about the system.' as abstract.

there is a document named 'doc-3165' that
  has 'This report is a final deliverable for the International Technology Alliance (ITA) [ITA] programme, under Project 4 Task 2. It defines the state of ITA Controlled English (CE) as used in that task at the end of the programme, it provides examples of its use, and suggests improvements and extensions. There are two parallel systems for experimenting with CE, one written in Java and open-sourced (called the CE Store), the other written in Prolog but not currently open-sourced (called Prolog CE). There are some differences between these systems, since Prolog CE is designed for experimentation on new techniques, whereas the Java CE Store is designed to be a more robustly engineered system, taking the ideas from the more experimental Prolog system. This document is based upon Prolog CE since this is the framework used in task 4.2.' as abstract.

there is a document named 'doc-3123' that
  has 'An island is inhabited by zombies and humans. Humans always tell the truth but zombies always lie.They understand English but only answer using the Native words Bal and Da.But you do not know which means yes and which means no.You find a native and ask him      Does Bal mean yes?He replies BalCan you tell whether the native is human or zombie?Can you tell if Bal means yes or no?' as abstract.
